The Unicode Cookbook for Linguists Managing orthographies in multilingual datasets using orthography profiles

Steven Moran \& Michael Cysouw

\textsc{Abstract}

In this paper, we define the notion of an orthography profile, which is a language-specific (or even resource-specific) description of the units and rules that are needed to adequately model a writing system. An orthography profile describes the Unicode code points, characters, graphemes and orthographic rules as used in a writing system. Our orthography profiles are used to describe writing systems and possibly even to transliterate them into another (for example IPA-like) transcriptions with the goal to harmonize orthographies across languages and resources on an ad-hoc basis. The central aspect of our approach is that we introduce an intermediate step of graphemic tokenization, which is closer to linguistic practice and easier to manage than transducer-based transliteration. We leverage orthography profiles to enable comparative analysis of languages with different writing systems and to perform quality control on datasets with highly specific orthographic systems. We also describe how our proposals relate to the current Unicode specification, and introduce some of the more delicate aspects of the Unicode Standard for a linguistic audience.

\textsc{Contents}

{[}{[}TOC{]}{]}

\section{1. Introduction}\label{introduction}

Writing systems arise and develop in a complex mixture of cultural, technological and practical pressures. They tend to be highly conservative, in that people who have learned to read and write in a specific way (however impractical or tedious) are mostly unwilling to change their habits, e.g.~they tend to resist spelling reforms. In all literate societies there exists a strong socio-political mainstream that tries to force unification of writing (for example by strongly enforcing `right' from `wrong' writing in schools). However, there is also a large community of users who take as many liberties in their writing as they can get away with.

For example, the writing of tone diacritics in Yoruba is often proclaimed to be the `right' way to write, although many users of Yoruba writing seem to be perfectly fine with leaving them out. As pointed out by the proponents of the `official' rules, there are some homographs when leaving out the tone diacritics (Olúmúyìw 2013: 44). However, writing systems (and the languages they represent) are normally full of homophones, which is normally not a problem at all for speakers of the language. More importantly, writing is not just a purely functional tool, but just as importantly it is a mechanism to signal social affiliation. By showing that you `know the rules' of expressing yourself in writing, others will more easily accept you as a worthy participant in their group. And that just as well holds for obeying to the `official' rules when writing a job application, as for obeying to the `informal' rules when writing an SMS to classmates in school. The case of Yoruba writing is an exemplary case, as even after more than a century of efforts to standardize the writing systems, there is still a wide range of variation in daily use (Olúmúyìw 2013).

The sometimes cumbersome and sometimes illogical structure, and the enormous variability of existing writing systems is a fact of life scholars have to accept and should try to adapt to as good as possible. Our plea here is a proposal for a formalization to do exactly that.

When considering the worldwide linguistic diversity, including all lesser-studied and endangered languages, there exist numerous different orthographies using symbols from the same scripts. For example, there are hundreds of orthographies using Latin-based alphabetic scripts. All of these orthographies use the same symbols, but these symbols differ in meaning and usage throughout the various orthographies. To be able to computationally use and compare different orthographies, we need a way to specify all orthographic idiosyncrasies in a computer-readable format (a process called `tailoring' in Unicode parlance). We call such specifications \textsc{orthography profiles}. Ideally, these specifications have to be integrated into so-called Unicode locale descriptions, though we will argue that in practice this is often not the most useful solution for the kind of problems arising in the daily practice of linguistics. Consequently, a central goal of this paper is to flesh out the linguistic challenges for locale descriptions, and work out suggestions to improve their structure for usage in a linguistic context. Conversely, we also aim to improve linguists' understanding and appreciation for the accomplishments of the Unicode Consortium in the development of the Unicode Standard.

The necessity to computationally use and compare different orthographies most forcefully arises in the context of language comparison. Concretely, in our current research our goal is to develop quantitative methods for language comparison and historical analysis in order to investigate worldwide linguistic variation and to model the historical and areal processes that underlie linguistic diversity (cf.~Steiner et al. 2010, List 2012a, 2012b, List \& Moran 2013, Moran \& Prokic 2013). In this work, it is crucial to be able to flexibly process across numerous resources with different orthographies. In many cases even different resources on the `same' language use different orthographic conventions. Another orthographic challenge that we encounter regularly in our linguistic practice is electronic resources on a particular language that claim to follow a specific orthographic convention (often a resource-specific convention), but on closer inspection such resources are almost always not consistently encoded. Thus a second goal of our orthography profiles is to allow for an easy specification of orthographic conventions, and use such profiles to check consistency and to report errors to be corrected.

A central step in our proposed solution to this problem is the tailored grapheme separation of strings of symbols, a process we call \textsc{grapheme tokenization}. Basically, given some strings of symbols (e.g.~morphemes, words, sentences) in a specific source, our first processing step is to specify how these strings have to be separated into graphemes, considering the specific orthographic conventions used in a particular source document. Our experience is that such a graphemic tokenization can be performed without extensive in-depth knowledge about the phonetic and phonological details of the language in question. For example, the specification that <ou> is a grapheme of English is a much easier task than to specify what exactly the phonetic values of this grapheme are in any specific occurrence in English words. Grapheme separation is a task that can be performed relatively reliably and with limited availability of time and resources (compare, for example, the task of creating a complete phonetic or phonological normalization).

Although grapheme tokenization is only one part of the solution, it is an important and highly fruitful processing step. Given a grapheme tokenization, various subsequent tasks become easier, like (a) temporarily reducing the orthography in a processing pipeline, e.g.~only distinguishing high versus low vowels; (b) normalizing orthographies across sources (often including temporary reduction of oppositions), e.g.~specifying an (approximate) mapping to the International Phonetic Alphabet; (c) using co-occurrence statistics across different languages (or different sources in the same language) to estimate the probability of grapheme matches, e.g.~with the goal to find regular sound changes between related languages or transliterations between different sources in the same language.

Before we deal with these proposals, in the first part of this paper (Sections 2 through 5) we give an extended introduction to the notion of encoding (Section 2) and writing systems, both from a linguistic perspective and from the perspective of the Unicode Consortium (Section 3). We consider the Unicode Standard to be a breakthrough (and ongoing) development that fundamentally changed the way we look at writing systems, and we aim to provide here a slightly more in-depth survey of the many techniques that are available in the standard. A good appreciation for the solutions that the Unicode Standard also allows for a thorough understanding of possible pitfalls that one might encounter when using it (Section 4). As an example of the current state-of-the-art, we discuss the rather problematic marriage of the International Phonetic Alphabet (IPA) with Unicode (Section 5).

The second part of the paper (Sections 6 and 7) describes our proposals for how to deal with the Unicode Standard in the daily practice of (comparative) linguists. First, we discuss the challenges of characterizing a writing system. To solve these problems, we propose the notions of orthography profiles, closely related to Unicode locale descriptions (Section 6). Finally, we discuss practical issues with actual examples (Section 7). We provide reference implementation of our proposals in R and in Python, available as open-source libraries.

The following conventions are followed in this paper. All phonemic and phonetic representations are given in the International Phonetic Alphabet (IPA), unless noted otherwise (International Phonetic Association, 2005). Standard conventions are used for distinguishing between graphemic <\textgreater{}, phonemic / / and phonetic {[} {]} representations. For character descriptions, we follow the notational conventions of the Unicode Standard (Unicode Consortium 2014). Character names are represented in small capital letters (e.g.~LATIN SMALL LETTER SCHWA) and code points are expressed as U\emph{+n} where \emph{n} is a four to six digit hexadecimal number, e.g.~\uni{0256}, which can be rendered as the glyph <ə>.

\section{2. Encoding}\label{encoding}

There are many in-depth histories of the origin and development of writing systems (e.g.~Robinson 1997, Powell 2012), a story that we will not repeat here. However, the history of turning writing into computer readable code is not so often told, so we decided to offer a short survey of the major developments here. This history turns out to be intimately related to the history of telegraphic communication.\footnote{Because of the recent history as summarised in this section, we have used mostly rather ephemeral internet sources, specifically http://www.unicode.org/history/ and various Wikipedia pages for the information presented here (when not references by traditional literature in the bibliography). A useful survey of the historical development of the physical hardware of telegraphy and telecommunication is Huurdeman (2003). Most books that discuss the development of encoding of telegraphic communication focus of cryptography (e.g.~Singh 1999) and forego the rather interesting story of `open' encoding that is related here.)

Writing systems have existed for roughly 6000 years, allowing people to exchange messages through time and space. Additionally, to bridge large geographic distances, \textsc{telegraphic systems} of communication (from Greek \emph{τῆλε γράφειν} ``distant writing'') have a long and widespread history since ancient times. The most widespread telegraphic systems worldwide are so-called whistled languages (Meyer 2015), but also drumming languages (Meyer et al. 2012) and smoke, fire, hydraulic or flag signals are forms of telegraphy. Telegraphy was reinvigorated in the end of the eighteenth century through the introduction of semaphoric systems by Claude Chapelle (since then using flags, flashing lights, or various specially designed contraptions) to convey messages over large distances. The `innovation' of those systems was that all characters of the written language were replaced one-to-one by visual signals. Since then, all telegraphic systems have taken this principle, namely that any language to be transmitted first has to be turned into some orthographic system, which subsequently is encoded for transmission by the sender, and then turned back into orthographic representation at the receiver side.\footnote{Sound and video-based telecommunication of course takes a different approach by ignoring the written version of language and directly encode sound waves or light patterns.) This of course implies that the usefulness of any such telegraphic encoding completely depends on the often rather haphazard structure of orthographic systems.

In the nineteenth century, \textsc{electric telegraphy} lead to a new approach in which written language characters were encoded by signals sent through a copper wire. Originally, `bisignal' codes were used, consisting of two different signals. For example, Carl Friedrich Gauss in 1833 used positive and negative current (Mania 2008: 282). More famous and influential, Samuel Morse in 1836 used long and short pulses. In those bisignal systems each character from the written language was encoded with a different number of signals (between one and five), so two different separators are needed: one between signals and one between characters (in Morse-code there is a short pause between signals and a long pause between characters).\footnote{Actually, Morse-code also includes an extra long pause between words. Interestingly, it took a long time to consider the written white-space as a bona-fide character that should simply be encoded with its own code point instead of with a different kind of pause. This happened only with the revision of the Baudot-code (see below) by Donald Murray in 1901, in which he finally introduced a white-space code. This principle has been followed ever since, and for example was included in the second version of the International Telegraph Alphabet (ITA2) in 1930.)

From those encodings, true \textsc{binary codes} developed with a fixed length of signals per character. In such systems only a single separator between signals is needed, because the separation between characters can be established by counting until a fixed number of signals.\footnote{Of course, no explicit separator is needed when the timing of the signals is known, which is the principle used in all modern telecommunication systems. An important modern consideration is also how to know where to start counting when you did not catch the start of a message, something that is known in Unicode as `self synchronizing'.) In the context of electric telegraphy, such a system was first established by Émile Baudot in 1870, using a fixed combination of five signals for each written character.\footnote{True binary codes have a longer history, going at least back to the Baconian cipher devised by Francis Bacon in 1605.) There are 25 = 32 possible combination when using five binary signals; an encoding today designated as `5-bit'. These codes are sufficient for all Latin letters, but of course they do not suffice for all written symbols, including punctuation and digits. As a solution, the Baudot code uses a so-called `shift' character, which signifies that from that point onwards (until shifted back) a different encoding is used, allowing for yet another set of 32 codes. In effect, this means that the Baudot code, and the \textsc{International Telegraph Alphabet (ITA)} derived from it, had an extra `bit' of information, so the encoding is actually 6-bit (with 26 = 64 different possible characters). For decades, this encoding was the standard for all telegraphy and it is still in limited use today.

To also allow for different uppercase and lowercase letters and a large variety of control characters, the American Standards Association decided to propose a new 7-bit encoding in 1963 (with 27 = 128 different possible characters), known as the \textsc{American Standard Code for Information Interchange(ASCII)}, geared towards the encoding of English orthography. With the ascent of other orthographies in computer usage, the wish to encode further variation of Latin letters (like German <ß> or various letters with diacritics like <è>) led the Digital Equipment Corporation to introduce an 8-bit \textsc{Multinational Character Set (MCS)} (with 28 = 256 different possible characters), first used with the introduction of the VT220 Terminal in 1983. Because 256 characters were clearly not enough for the many different characters needed in the world's writing systems, the ISO/IEC 8859 standard in 1987 extended the MCS to include 16 different 8-bit code pages. For example, part 5 was used for Cyrillic characters, part 6 for Arabic, and part 7 for Greek.\footnote{In effect, because 16 = 24, this means that ISO/IEC 8859 was actually a 8+4 = 12-bit encoding, though with very many duplicates by design (e.g.~all ASCII codes were repeated in each 8-bit code page). To be precise, ISO/IEC 8859 used the 7-bit ASCII as a basis for each code page, and defined 16 different 7-bit extensions, leading to (1+16) times 27 = 2,176 possible characters.)

This system almost immediately was understood to be insufficient and impractical, so various initiatives to extend and reorganize the encoding started in the 1980s. This led, for example, to various proprietary encodings from Microsoft (e.g.~Windows Latin 1) and Apple (e.g.~Mac OS Roman), which one still sometimes encounters today. More wide-ranging, various people in the 1980s started to develop true international code sets. In the United States, a group of computer scientists formed the Unicode Consortium, proposing a 16-bit encoding in 1991 (with 216 = 65,536 different possible characters). At the same time in Europe, the International Organization for Standardization (ISO) was working on ISO 10646 to supplant the ISO 8859 standard. Their first draft of the \textsc{Universal Character Set (UCS)} in 1990 was 31-bit (with theoretically 231 = 2,147,483,648 possible characters, but because of some technical restrictions only 679,477,248 were allowed). Since 1991, the Unicode Consortium and the International Organization for Standardization have jointly developed the \textsc{Unicode Standard}, or ISO/IEC 10646, leading to the current system including the original 16-bit Unicode proposal as the \textsc{Basic Multilingual Plane}, and 16 additional planes of 16-bit for further extensions (with in total 1+16 times 216 = 1,114,112 possible characters). The most recent version of the Unicode Standard (7.0) was published in June 2014 and it defines 112,956 different characters (Unicode Consortium 2014).

\section{3. Terminology}\label{terminology}

\subsection{The linguistic tradition}\label{the-linguistic-tradition}

In this section we provide a very brief overview of the linguistic terminology concerning writing systems before turning to the slightly different computational terminology in the next section on the Unicode Standard. Linguistically speaking, a \textsc{writing system} is a symbolic system that uses visible or tactile signs to represent language in a systematic way. The term `writing system' has two mutually exclusive meanings. First, it may refer to the way a particular language is written. In this sense the term refers to the writing system of a particular language, as, for example, in \emph{``the Serbian writing system uses two scripts: Latin and Cyrillic''}. Second, the term `writing system' may refer to a type of symbolic system as used among the world's languages to represent languages, as, for example, in \emph{``alphabetic writing system''}. In this latter sense the term refers to how scripts have been classified according to the way that they encode language, as in, for example, \emph{``the Latin and Cyrillic scripts are both alphabetic writing systems''}. To avoid confusion, this second notion of `writing system' would have been more aptly called \textsc{script system}. The term \textsc{script} refers to a collection of distinct symbols as employed by one or more orthographies. For example, both Serbian and Russian are written with subsets of the Cyrillic script. A single language, like Serbian or Japanese, can also be written using orthographies based on different scripts. Over the years linguists have typologized script systems in a variety of ways, with the tripartite classification of logographic, syllabic, and alphabetic remaining the most popular, even though there are at least half a dozen different types of script systems that can be distinguished (Daniels, 1990, 1996).

Only focussing here on the first sense, we distinguish two different kinds of writing systems as used to write a particular language, namely transcriptions and orthographies. First, \textsc{transcription} is a scientific procedure (and also the result of that procedure) for graphically representing the sounds of human speech at the phonetic level. It incorporates a set of unambiguous symbols to represent speech sounds, including conventions that specify how these symbols should be combined. A \textsc{transcription system} is a specific system of symbols and rules used for transcription of the sounds of a spoken language variety. In principle, a transcription system should be language-independent, in that it should be applicable to all spoken human languages. The International Phonetic Alphabet (IPA) is a commonly used transcription system that provides a medium for transcribing languages at the phonetic level. However, there is a long history of alternative kinds of transcription systems (see Kemp 1994) and today various alternatives are in widespread use (e.g.~X-SAMPA and Cyrillic-based phonetic transcription systems).\footnote{Most users of IPA do not follow the standard explicitly and many dialects based on it have emerged, e.g.~the Africanist and Americanist transcription systems. ) Note that IPA symbols are also often used to represent language on a phonemic level. However, it is important to realize that in this usage the IPA symbols are not a transcription system, but rather an orthography (though with strong links to the pronunciation). Further, a transcription system does not need to be highly detailed, but it can also be a system of broad sound classes. Although such an approximative transcription is not normally used in linguistics, it is widespread in technological approaches (e.g.~Soundex, Beider \& Morse 2010)\footnote{http://stevemorse.org/phonetics/bmpm.htm , http://stevemorse.org/phonetics/bmpm2.htm) and it is sometimes fruitfully used in automatic approaches to historical linguistics (Dolgopolsky 1986, ASJP\footnote{http://asjp.clld.org/)).

Second, an \textsc{orthography} specifies the symbols, punctuations, and the rules in which a specific language is written in a standardized way. Orthographies are often based on a phonemic analysis, but they almost always include idiosyncrasies because of historical developments (like sound changes or loans) and because of the widely-followed principle of lexical integrity (i.e.~the attempt to write the same lexical root in a consistent way, also when synchronic phonemic rules change the pronunciation, for example with final devoicing in many Germanic languages). All orthographies are language-specific (and often even resource-specific), although individual symbols or rules might be shared between languages. A \textsc{practical orthography} is a strongly phoneme-based writing system designed for practical use by speakers. The mapping relation between phonemes and graphemes in practical orthographies is purposely shallow, i.e.~there is mostly a systematic and faithful mapping from a phoneme to a grapheme. Practical orthographies are intended to jumpstart written materials development by correlating a writing system with the sound units of a language (cf.~Meinhof \& Jones 1928). Symbols from the IPA are often used by linguists in the development of practical orthographies for languages without writing systems, though this usage of IPA symbols should not be confused with transcription (as defined above). Further, a \textsc{transliteration} is a mapping between two different orthographies. It is the process of ``recording the graphic symbols of one writing system in terms of the corresponding graphic symbols of a second writing system'' (Kemp 2006:396). In straightforward cases, such a transliteration is simply a matter of replacing one symbol with another. However, there are widespread complications, like one-to-many or many-to-many mappings, which are not always easy, or even possible, to solve without listing all cases individually (cf.~Moran 2012, chp 2).

Breaking it down further, a script consists of \textsc{graphemes}, and graphemes consist of \textsc{characters}. In the linguistic terminology of writing systems, a \textsc{character} is a general term for any self-contained element in a writing system.\footnote{There is a second interpretation of the term `character', i.e.~a conventional term for a unit in the Chinese writing system (Daniels 1996). This interpretation will not be further explored in this paper.) Although in literate societies most people have a strong intuition about what the characters are in their particular orthography or orthographies, it turns out that the separation of an orthography into separate characters is far from trivial. The widespread intuitive notion of a character is strongly biased towards educational traditions, like the alphabet taught at schools, and technological possibilities, like the available type pieces in a printer's job case, the keys on a typewriter, or the symbols displayed in Microsoft Word's symbol browser. In practice, characters often consist of multiple building blocks, each of which could be considered a character in its own right. For example, although a Chinese character may be considered to be a single basic unanalyzable unit, at a more fine-grained level of analysis the internal structure of Chinese characters is often comprised of smaller semantic and phonetic units that should be considered characters (Sproat 2000). In alphabetic scripts, this problem is most forcefully exemplified by diacritics. A \textsc{diacritic} is a mark, or series of marks, that may be above, below, or through other characters (Gaultney 2002). Diacritics are sometimes used to distinguish homophonous words, but they are more often used to indicate a modified pronunciation (Daniels \& Bright 1996, xli). The central question is whether, for example, , , and should be considered four characters, or different combinations of three characters. In general, multiple characters together can form another character, and it is not always possible to decide on principled grounds what should be the basic building blocks of an orthography.

For that reason, it is better to analyse an orthography as a collection of graphemes. A \textsc{grapheme} is the basic, minimally distinctive symbol of a particular writing system, alike to the phoneme is an abstract representation of a distinct sound in a specific language. The term `grapheme' was modeled after the term `phoneme' and represents a contrastive graphical unit in a writing system (see Kohrt 1986 for a historical overview of the term grapheme). Most importantly, a single grapheme regularly consists of multiple characters, like <th>, <ou> and <gh> in English (note that each character in these graphemes is also a separate grapheme in English). Such complex graphemes are often used to represent single phonemes. So, a combination of characters is used to represent a single phoneme. Note that the opposite is also found in writing systems, in cases in which a single character represents a combination of two or more phonemes. For example, <x> in English orthography represents a combination of the phonemes /k/ and /s/. Further, conditioned or free variants of a grapheme are called \textsc{allographs}. For example, the distinctive forms of Greek sigma are conditioned, with <σ> being used word-internally and <ς> being used at the end of a word. In sum, there are many-to-many relationships between phonemes and graphemes as they are expressed in the myriad of language- and resource-specific orthographies.

This exposition of the linguistic terminology involved in describing writing systems has been purposely brief. We have highlighted some of the linguistic notions that are pertinent, yet sometimes confused with, the technological definitions developed for the computational processing of the world's writing systems, which we describe in the next section.

\subsection{The Unicode approach}\label{the-unicode-approach}

The conceptualisation and terminology of writing systems was rejuvenated through the development of the Unicode Standard, with major input from Mark Davis, co-founder and long-term president of the Unicode Consortium. For many years, `exotic' writing systems and phonetic transcription systems on personal computers were constrained by the American Standard Code for Information Interchange* *(ASCII) character encoding scheme (based on the Latin script), which only allowed for a strongly limited number of different symbols to be encoded. This implied that users could either use and adopt the (extended) Latin alphabet or they could assign new symbols to the small number of code points in the ASCII encoding scheme to be rendered by a specifically designed font (Bird \& Simons 2003). In this situation, it was necessary to specify the font together with each document to ensure the rightful display of its content. To alleviate this problem of assigning different symbols to the same code points, in the late 80's and early 90's the Unicode Consortium set itself the ambitious goal of developing a single universal character encoding to provide a unique number, a code point, for every character in the world's writing systems. Nowadays, the Unicode Standard is the default encoding of the technologies that support the World Wide Web and for all modern operating systems, software and programming languages.

The Unicode Standard represents a massive step forward because it aims to eradicate the distinction between `universal' (ASCII) versus `language/resource-particular' (Font) by adding as much as possible language-specific information into the universal standard. However, there are still language/resource-specific specifications necessary for the proper usage of Unicode, as will be discussed below. Within the Unicode structure many of these specifications can be captured by so-called `Locales' or `Locale Descriptions', so we are moving to a new distinction of `universal' (Unicode Standard) versus `language-particular' (Locale Description). The major gain is a much larger compatibility on the universal level (because Unicode standardizes a much larger portion of writing system diversity), and much better possibilities for automatic processing on the language-particular level (because Locale Descriptions are computer readable specifications).

Each version of the Unicode Standard (as of writing at version 7) consists of a set of specifications and guidelines that include (i) a core specification, (ii) code charts, (iii) standard annexes and (iv) a character database.\footnote{All documents of the Unicode Standard are available at http://www.unicode.org/versions/latest/. For a quick survey of the use of terminology inside the Unicode Standard, their glossary is particularly useful, available at http://www.unicode.org/glossary/. For a general introduction to the principles of Unicode, Chapter 2 of the core specification, called ``General Structure'', is particularly insightful. Different from many other documents of the Unicode Standard, this general introduction is relatively easy to read and illustrated with many interesting examples from various orthography traditions all over the world. ) The \textsc{core specification} is a book directed toward human readers that describes the formal standard for encoding multilingual text. The \textsc{code charts} provide a humanly readable online reference to the character contents of the Unicode Standard in the form of PDF files. The \textsc{Unicode Standard Annexes (UAX)} are a set of technical standards that describe the implementation of the Unicode Standard for software development, Web standards, programming languages, etc. The \textsc{Unicode Character Database (UCD)} is a set of computer-readable text files that describe the character properties, including a set of rich character and writing system semantics, for each character in the Unicode Standard. In this section, we introduce the basic Unicode concepts, but we will leave out many details. Please consult the above mentioned full documentation for a more detailed discussion. Further note that the Unicode Standard is exactly that, namely a standard. It normatively describes notions and rules to be followed. In the actual practice of applying this standard in a computational setting, a specific implementation is necessary. The most widely used implementation of the Unicode Standard is the \textsc{International Components for Unicode (ICU)}, which offers C/C++ and Java libraries trying to implement to Unicode Standard.\footnote{http://icu-project.org )

The Unicode Standard is a \textsc{character encoding system} which goal it is to support the interchange and processing of written characters and text in a computational setting. Underlyingly, the character encoding is represented by a range of numerical values called a \textsc{code space}, which is used to encode a set of characters. A \textsc{code point} is a unique non-negative integer within a code space (i.e.~within a certain numerical range). In the Unicode Standard character encoding system, an \textsc{abstract character}, for example a ``LATIN SMALL LETTER P'', is mapped to a particular code point, such as the decimal value 112, normally represented in hexadecimal and in Unicode parlance as (U+)0070.\footnote{Hexadecimal (`base-16') 0070 is equivalent to decimal (`base-10') 112, which can be calculated by considering 0·163 + 0·162 + 7·161 + 0·160 = 7·16 = 112. Underlyingly, computers will of course treat this code point binary (`base-2') as 11100000, i.e.~112 = 1·27 + 1·26 + 1·25 + 0·24 + 0·23 + 0·22 + 0·21 + 0·20 = 64 + 32 + 16.) That encoded abstract character is rendered on a computer screen (or printed page) as a \textsc{glyph}, e.g. <p>, depending on the \textsc{font} and the context in which that character appears.

In Unicode Standard terminology, an (abstract) \textsc{character} is the basic encoding unit. The term `character' can be quite confusing due to its alternative definitions across different scientific disciplines and because in general the word `character' means many different things to different people. It is therefore often preferable to refer to Unicode characters simply as \textsc{code points}, because there is a one-to-one mapping between Unicode characters and their numeric representation. The Unicode Consortium (2012) defines a character as ``the smallest component of written language that has semantic value. It refers to the abstract meaning and/or general shape, rather than a specific shape, though in code tables some form of visual representation is essential for the reader's understanding.'' Unicode defines characters as abstractions of orthographic symbols, and it does not define visualizations for these characters (although it does presents examples). In contrast, a \textsc{glyph} is a concrete graphical representation of a character as it appears when rendered (or rasterized) and displayed on an electronic device or on printed paper. For example, <reinstert Gs here> are different glyphs of the same character, i.e.~they may be rendered differently depending on the typography being used, but they all share the same code point. From the perspective of Unicode they are `the same thing'. In this approach, a \textsc{font} is then simply a collection of glyphs linked to code points. Allography is not specified in Unicode (expect for a few exceptional cases, due to legacy encoding issues), but can be specified in a font as a \textsc{contextual variant} (aka presentation form).

Each code point in the Unicode Standard is associated with a set of \textsc{character properties} as defined by the Unicode character property model.\footnote{The character property model is described in http://www.unicode.org/reports/tr23/, but the actual properties are described in http://www.unicode.org/reports/tr44/. A simplified overview of the properties is available at http://userguide.icu-project.org/strings/properties. The actual code tables listing all properties for all Unicode code points are available at http://www.unicode.org/Public/UCD/latest/ucd/. ) Basically, those properties are just a long list of values for each character. For example, code point \uni{0047} has the following properties (among many others):
\begin{itemize}
	\item Name: LATIN CAPITAL LETTER G 
	\item Alphabetic: YES 
	\item Uppercase: YES 
	\item Script: LATIN 
	\item Extender: NO 
	\item Simple\_Lowercase\_Mapping: 0067 
\end{itemize}

These properties contain the basic information of the Unicode Standard and they are necessary to define the correct behavior and conformance required for interoperability in and across different software implementations (as defined in the Unicode Standard Annexes). The character properties assigned to each code point is based on each character's behavior in the real-world writing traditions. For example, the corresponding lowercase character to \uni{0047} is \uni{0067} (though note that the relation between uppercase and lowercase is in many situations much more complex than this, and Unicode has further specifications for those cases). Another use of properties is to define the script of a character. The Unicode Standard defines the term \textsc{script} as, ``A collection of letters and other written signs used to represent textual information in one or more writing systems. For example, Russian is written with a subset of the Cyrillic script; Ukrainian is written with a different subset. The Japanese writing system uses several scripts.'' In practice, script is simply defined for each character as the explicit ``Script'' property in the Unicode Character Database.

One frequently references property is the \textsc{block} property, which is often used in software applications to impose some structure to the large number of Unicode characters. Each character in Unicode belongs to a specific block. These blocks are basically an organisational structure to alleviate the administrative burden of keeping Unicode up-to-date. Blocks consist of characters that in some way belong together, so that characters are easier to find. Some blocks are connected with a specific script, like the Hebrew block or the Gujarati block. However, blocks are predefined ranges of code points, and often there will come a point after which the range is completely filled. Any extra characters will have to be assigned somewhere else. There is, for example, a block `Arabic', which contains most Arabic symbols. However, there is also a block `Arabic Supplement', `Arabic Presentation Forms-A' and `Arabic Presentation Form B'. The situation with Latin symbols is even more extra. In general, the names for block should be taken as a definitional statement. For example, many IPA symbols are not located in the aptly-names block ``IPA extensions'', but in other blocks (see Section 5).

There are many cases in which a sequence of characters (i.e.~a sequence of more than one code point) represents what a user perceives as an individual unit in a particular orthographic writing system. For this reason the Unicode Standard differentiates between \textsc{abstract character} and \textsc{user-perceived character}. Sequences of multiple code points that correspond to a single user-perceived characters are called** grapheme clusters** in Unicode parlance. Grapheme clusters come in two flavors: (default) grapheme clusters and tailored grapheme clusters.

The (default) \textsc{grapheme clusters} are locale-independent graphemes, i.e.~they always apply when a particular combination of characters occurs independent of the writing system in which they are used. These character combinations are defined in the Unicode Standard as functioning as one \textsc{text element}.\footnote{The Unicode Standard defines text element as: ``A minimum unit of text in relation to a particular text process, in the context of a given writing system. In general, the mapping between text elements and code points is many-to-many.'') The simplest example of a grapheme cluster is a base character followed by a letter modifier character. For example, the sequence + <̃\textgreater{} (LATIN SMALL LETTER N at \uni{006E} followed by COMBINING TILDE at \uni{0303}) combines visually into , a user-percieved character in writing systems like that of Spanish. So, what the user perceives as a single character actually involves a multi-code-point sequence. Note that this specific sequence can also be represented with a single \textsc{precomposed} code point, LATIN SMALL LETTER N WITH TILDE at \uni{00F1}, but this is not the case for all multi-code point character sequences. The problem that there multiple encodings possible for the same text element has been acknowledged early on in the Unicode Standard (e.g.~for , the sequence \uni{006E} \uni{0303} should in all situations be treated identically to the precomposed \uni{00F1}), and a system of \textsc{canonical equivalence} is available for such situations. Basically, the Unicode Standard offers different kind of normalizations to either decompose all precomposed characters (called \textsc{NFD}, \textsc{Normalization Form Canonical Decomposition}), or compose as much as possible combinations (called \textsc{NFC}, \textsc{Normalization Form Canonical Composition}). In current practice of software development, NFC seems to be preferred in most situations and is widely proposed as the preferred canonical form.

More difficult for text processing, because less standardized, is what the Unicode Standard terms \textsc{tailored grapheme clusters}. Tailored grapheme clusters are locale-dependent graphemes, i.e.~such combination of characters do not function as text elements in all situations. For example, the sequence + for the Slovak digraph or the sequence in the Sisaala practical orthography (pronounced as IPA /tʃ/; Moran 2006). These grapheme clusters are ``tailored'' in the sense that they must be specified on a language-by-language or a writing system-by-writing system basis. The Unicode Standard provides technological specifications for creating locale specific data in so-called \textsc{Unicode Locale Descriptions}, i.e.~a set of specification that defines a set of language-specific elements (e.g.~tailored grapheme clusters, collation order, capitalization-equivalence), as well as other special information, like how to format numbers, dates, or currencies. Locale descriptions are saved in the \textsc{Common Locale Data Repository (CLDR)},\footnote{http://cldr.unicode.org/ ) a repository of language-specific definitions of writing system properties, each of which describes specific usages of characters. Each locale can be encoded in a document using the \textsc{Locale Data Markup Language (LDML)}. LDML is an XML format and vocabulary for the exchange of structured locale data.

Unicode Locale Descriptions allow users to define language- or resource-specific writing systems or orthographies. The Unicode Consortium defines \textsc{writing system} only very loosely, as it is not a central concept in the Unicode Standard. A writing system is, ``A set of rules for using one or more scripts to write a particular language. Examples include the American English writing system, the British English writing system, the French writing system, and the Japanese writing system.''. However, there are various drawbacks of locale descriptions for the daily practice of linguistic work in a multilingual setting (see Section 7).

\section{4. Unicode Pitfalls}\label{unicode-pitfalls}

\subsection{Wrong it is not}\label{wrong-it-is-not}

In this section, we describe some of the most common pitfalls that we have encountered when using the Unicode Standard in our own work, or in discussion with other linguists. This section is not meant as a criticism of the decisions made by the Unicode Consortium; on the contrary, we aim to highlight where the technological aspects of the Unicode Standard diverge from many users' intuitions. What have sometimes been referred to as problems or inconsistencies in the Unicode Standard are mostly due to legacy compatibility issues, which can lead to unexpected behavior by linguists using the standard. However, there are also some cases in which the Unicode Standard has made decisions that theoretically could have been taken differently, but for some reason or another (mostly very good reasons) were accepted as they are now. We call behavior that executes without error but does something different than the user expected---often unknowingly---a ``pitfall''.

In this context, it is important to realize that the Unicode Standard was not developed to solve linguistic problems per se, but to offer a consistent computational environment for written language. In those cases in which the Unicode Standard behaves differently as expected, we think it is important not to dismiss Unicode as `wrong' or `deficient', because our experience is that in almost all cases the behaviour of the Unicode Standard has been particularly well thought through. The Unicode Consortium considers important practical use-cases that from a linguistic point of view are normally not considered. Our general guideline for dealing with the Unicode Standard is to accept it as it is, and not to battle windmills. Alternatively, of course, it is possible to actively engage in the development of the standard itself, an effort that is highly appreciated by the Unicode Consortium.

\subsection{Pitfall: Blocks}\label{pitfall-blocks}

The Unicode code space is subdivided into blocks of contiguous code points. For example, the block called ``Cyrillic'' runs from \uni{0400} till \uni{04FF}. These blocks arose as an attempt at ordering the enormous amount of characters in Unicode, but the ideas of blocks very quickly ran into problems. First, the size of a block is fixed, so when a block is full, a new block will have to be instantiated somewhere further in the code space. For example, this led to the blocks ``Cyrillic Supplement'', ``Cyrillic Extended-A'' (which are also already full) and ``Cyrillic Extended-B''. Second, when a specific character already exists, then it is not duplicated in another block, although the name of the block might indicate that a specific symbol should be available there. In general, names of blocks are just an approximate indication of the kind of characters that will be in the block.

The problem with blocks arises because finding the right character among the thousands of Unicode characters is not easy. Many software applications present blocks as a primary search mechanism, because the block names suggest where to look for a particular character. However, when a user searches for an IPA character in the block ``IPA Extensions'' then many IPA characters will not be found there. For example, the velar nasal <ŋ> is not part of the block ``IPA Extensions'' because it was already included as LATIN SMALL LETTER ENG at \uni{014B} in the block ``Latin Extensions-A''.

In general, finding a specific character in the Unicode Standard is often not trivial. The names of the blocks can help, but it is not (and never was supposed to be) a foolproof structure. It is not the goal nor aim of the Unicode Consortium to provide a user interface to the Unicode Standard. If one often encounters the problem of needing to find a suitable character, there are various other useful services for end users available.\footnote{The Unicode website offers a basic interface to the code charts at http://www.unicode.org/charts/index.html. As a more flexible interface, we particularly like PopChar from Macility, available for both Macintosh and Windows. However, there are various free websites that offer search interfaces to the Unicode code tables, like http://unicode-search.net or http://unicode-search.net. A further useful approach for searching characters using shape matching is http://shapecatcher.com. )

\subsection{Pitfall: Missing glyphs}\label{pitfall-missing-glyphs}

The Unicode Standard is often praised (and deservedly so) for solving many of the perennial problems with the interchange and display of the world's writing systems. However, a common complaint from users is that, while the praise may be true, they mostly just see some boxes on their screen instead of those promised symbols. The problem of course is that users' computers do not have any glyphs installed matching the Unicode code points in the file they are trying to inspect. It is important to realize that internally in the computer everything still works as expected: any handling of Unicode code points works independently of how they are displayed on the screen. So, although a user might only see boxes being displayed, this user should be assured that everything is still in order.

The central problem behind the missing glyphs is that designing actual glyphs includes a lot of different considerations and it is a time-consuming process. Many traditional expectations of how specific characters should look like have to be taken into account when designing glyphs. Those expectations are often not well documented, and it is mostly up to the knowledge and experience of the font designer to try and conform to them as good as possible. Therefore, most designers produce fonts only including glyphs for certains parts of the Unicode Standard, namely for those characters they feel comfortable with. At the same time, the number of characters defined by the Unicode Standard is growing with each new version, so it is neigh impossible for any designer to produce glyphs for all characters. The result of this is that, almost necessarily, each font only includes glyphs for a subset of the characters in the Unicode Standard.

The simple solution to missing glyphs is thus to install additional fonts providing additional glyphs. For the more exotic characters there is often not much choice. There are a few particularly large fonts that might be considered. First, there is the \textsc{Everson Mono} font made by Michael Everson, which currently includes 9,756 different glyphs (not including Chinese) updated up to Unicode 7.0.\footnote{Everson Mono is available as shareware at http://www.evertype.com/emono/. ) Already a bit older is the \textsc{Titus Cyberbit Basic} font made by Jost Gippert and Carl-Martin Bunz, which includes 10,044 different glyphs (not including Chinese), but not including newer characters added after Unicode 4.0.\footnote{Titus Cyberbit Basic is available at http://titus.fkidg1.uni-frankfurt.de/unicode/tituut.asp. )

Further, we suggest to always install at least one so-called \textsc{fall-back font,} which provides glyphs that at least show the user some information about the underlyingly encoded character. Apple Macintoshes have such a font (which is invisible to the user), which is designed by Michael Everson and made available for other systems through the Unicode Consortium.\footnote{The Apple/Everson fallback font is available at http://www.unicode.org/policies/lastresortfont\_eula.html. ) Further, the \textsc{GNU Unifont} is a clever way to produce bitmaps approximating the intended glyph of each available character, updated to Unicode 7.0.\footnote{The GNU Unifont is available at http://unifoundry.com/unifont.html. ) Finally, the Summer Institute of Linguistics provides a \textsc{SIL Unicode BMP Fallback Font}, currently available up to Unicode version 6.1. This font does not even attempt to show a real glyph, but only shows the hexadecimal code inside a box for each character, so a user can at least see the Unicode codepoint of the character to be displayed.\footnote{The SIL Unicode BMP Fallback Font is available at http://scripts.sil.org/UnicodeBMPFallbackFont. )

\subsection{Pitfall: Faulty rendering}\label{pitfall-faulty-rendering}

A similar complaint to missing glyphs, discussed previously, is that while there might be a glyph being displayed, it does not `look right'. There are two reasons for unexpected visual display, namely automatic font substitution and faulty rendering. Like missing glyphs, any such problems are independent from the Unicode Standard. The Unicode Standard only includes very general information about characters and leaves the specific visual display to others to decide on. Any faulty display is thus not to be blamed on the Unicode Consortium, but on a complex interplay of different mechanisms happening in a computer to turn Unicode codepoints into visual symbols. We will only sketch a few aspects of this complex interplay here.

Most modern software applications (like Microsoft Word) offer some approach to \textsc{automatic font substitution}. This means that when a text is written in a specific font (e.g.~Times New Roman) and a inserted Unicode character does not have a glyph within this font, then the software application will automatically search for another font to display the glyph. The result will be that some specific glyph will look slightly different from the others. This mechanism works differently depending on the software application, and mostly only limited user influence is expected and little feedback is given, which might be rather frustrating to font-aware users.\footnote{For example, Apple Pages does not give any feedback that a font is being replaced, and the user does not seem to have any influence on the choice of replacement (except by manually marking all occurrences). In contrast, Microsoft Word does indicate the font replacement by showing the name of the font replacement, but it simply changes the font, so any text written after the replacement is written in a different font as before. Both behaviors leave much to be desired.)

The other problem with visual display is related to the so-called `font rendering'. \textsc{Font rendering }refers to the process of the actual positioning of Unicode characters on a page of written text. This positioning is actually a highly complex problem, and many things can go wrong in the process. Well-known rendering problems, like proportional glyph size or ligatures are reasonably well understood. However, even within the Latin script, the positioning of diacritics relative to a base character is still a widespread problem. Especially when more than one diacritic is supposed to be placed above (or below) each other, this often leads to unexpected effects in many modern software applications. The problems arising in Arabic and in many southeast Asian scripts (like Devanagari or Burmese) are even much more complex. There are basically three different approaches to font rendering. The most widespread is Adobe's and Microsoft's \emph{OpenType }system. This approach makes it relatively easy for font developers, as the font itself does not include all details about the precise placement of individual characters. For those details, additional script-descriptions are necessary. All of those systems can lead to unexpected behaviour.\footnote{For more details about OpenType, see http://www.adobe.com/products/type/opentype.html and http://www.microsoft.com/typography/otspec/. Additional systems for complex text layout are, among others, Microsoft's DirectWrite https://msdn.microsoft.com/library/dd368038.aspx and the open-source project HarfBuzz http://www.freedesktop.org/wiki/Software/HarfBuzz/. ) Alternative systems are Apple's \emph{Apple Advanced Typography} (AAT) and the open-source \emph{Graphite} system from the Summer Institute of Linguistics (SIL).\footnote{More information about AAT can be found at https://developer.apple.com/fonts/. SIL's Graphite is described in detail at http://scripts.sil.org/cms/scripts/page.php?site\_id=projects\&item\_id=graphite\_home. ) In both of these systems, a larger burden is placed on the description inside the font.

\subsection{Pitfall: Homoglyphs}\label{pitfall-homoglyphs}

Homoglyphs are visually indistinguishable glyphs (or highly similar glyphs) that have different code points in the Unicode Standard and thus different character semantics. As a principle, the Unicode Standard does not specify how a character appears visually on the page or the screen. So in most cases, a different appearance is caused by the specific design of a font, or by user-settings like size or boldface. Taking an example already discussed in the previous section, the following symbols <reinsert Gs here> are different glyphs of the same character, i.e.~they may be rendered differently depending on the typography being used, but they all share the same code point (viz. LATIN SMALL LETTER G at \uni{0067}). In contrast, the symbols <A А Α Ꭺ ᗅ ᴀ ꓮＡ𐊠 𐌀 𝖠 𝙰> are all different code points, although they look highly similar -- in some cases even sharing exactly the same glyph in some fonts. All these different A-like characters include the following code points in the Unicode Standard:
\begin{itemize}
	\item <A\textgreater{} LATIN CAPITAL LETTER A, at \uni{0041} 
	\item <А> CYRILLIC CAPITAL LETTER A, at \uni{0410} 
	\item <Α> GREEK CAPITAL LETTER ALPHA, at \uni{0391} 
	\item <Ꭺ> CHEROKEE LETTER GO, at \uni{13AA} 
	\item <ᗅ> CANADIAN SYLLABICS CARRIER GHO, at \uni{15C5} 
	\item <ᴀ> LATIN LETTER SMALL CAPITAL A, at \uni{1D00} 
	\item <ꓮ> LISU LETTER A, at \uni{A4EE} 
	\item <Ａ> FULLWIDTH LATIN CAPITAL LETTER A, at \uni{FF21} 
	\item <𐊠> CARIAN LETTER A, at \uni{102A}0 
	\item <𐌀> OLD ITALIC LETTER A, at \uni{1030}0 
	\item <𝖠> MATHEMATICAL SANS-SERIF CAPITAL A, \uni{1D5A}0 
	\item <𝙰> MATHEMATICAL MONOSPACE CAPITAL A, at \uni{1D67}0 
\end{itemize}

The existence of such homoglyphs is partly due to legacy compatibility, but for the most part these characters are simply different characters that happen to look similar.\footnote{A particularly nice interface to look for homoglyphs is http://shapecatcher.com, based on the principle of recognizing shapes (Belongie et al. 2002).) Yet, they are supposed to behave different from the perspective of a font designer. For example, when designing a Cyrillic font, the will have different aesthetics and different traditional expectation compared to a Latin .

Such homoglyphs are a widespread problem for consistent encoding. Although for most users it looks like the words and are identical, in actual fact they do not even share a single code point.\footnote{The first words consists completely of Latin characters, namely \uni{0076}, \uni{006F}, \uni{0063}, \uni{0065}, \uni{0073}, while the second is a mix of Cyrillic and Greek characters, namely \uni{03BD}, \uni{03BF}, \uni{0041}, \uni{0435}, \uni{0455}.) For computers these two words are completely different entities. Commonly, when users with Cyrillic or Greek keyboards have to type some Latin-based orthography, they mix similar looking Cyrillic or Greek characters into their text, because those characters are so much easier to type. Similarly, when users want to enter an unusual symbol, they normally search by visual impression in their favourite software application, and just pick something that looks reasonably alike to what they expect the glyph to look like.

\subsection{Pitfall: Canonical equivalence}\label{pitfall-canonical-equivalence}

For some characters, there is more than one possible encoding in the Unicode Standard. This is a possible pitfall, as this would mean that for the computer there exist multiple different entities that for a user are the same. This would, for example, lead to problems with searching, as the computer would search for specific encodings, and not find all expected characters. As a solution, the Unicode Standard includes a notion of \textsc{canonical equivalence}. Different encodings are explicitly declared as equivalent in the Unicode Standard code tables. Further, to harmonize all encodings in a specific piece of text, the Unicode Standard proposes a mechanism of \textsc{normalization}.

Consider for example the characters and following Unicode code points:
\begin{enumerate}
	\def\labelenumi{\arabic{enumi}.} 
	\item LATIN CAPITAL LETTER A WITH RING ABOVE (\uni{00C5}) 
	\item ANGSTROM SIGN (\uni{212B}) 
	\item LATIN CAPITAL LETTER A (\uni{0041}) + COMBINING RING ABOVE (\uni{030A}) 
\end{enumerate}

The character, represented here by glyph , is encoded in the Unicode Standard in the first two examples by a single-character sequence; each is assigned a different code point. In the third example, the glyph is encoded in a multiple-character sequence that is composed of two character code points. All three sequences are \textsc{canonically equivalent}, i.e.~they are strings that represent the same abstract character and because they are not distinguishable by the user, the Unicode Standard requires them to be treated the same in regards to their behavior and appearance. Nevertheless, they are encoded differently. For example, if one were to search an electronic text (with software that does not apply Unicode Standard normalization) for ANGSTROM SIGN (\uni{212B}), instances of LATIN CAPITAL LETTER A WITH RING ABOVE (\uni{00C5}) would not be found.

In other words, there are equivalent sequences of Unicode characters that should be normalized, i.e.~transformed into a unique Unicode-sanctioned representation of a character sequence called a \textsc{normalization form}. Unicode provides a \emph{Unicode Normalization Algorithm}, which essentially puts combining marks into a specific logical order and it defines decomposition and composition transformation rules to convert each string into one of four normalization forms. We will discuss here the two most relevant normalization forms: NFC and NFD.

The first of the three characters above is considered the \textsc{Normalization Form C (NFC)}, where ``C'' stands for composition. When the process of NFC normalization is applied to the character sequences in 2 and 3, both sequences are normalized into the `pre-composed' character sequence in 1. Thus all three canonical character sequences are standardized into one composition form in NFC. The other central Unicode normalization form is the \textsc{Normalization Form D (NFD)}, where ``D'' stands for decomposition. When NFD is applied to the three examples above, all three, including importantly the single-character sequences in 1 and 2, are normalized into the `decomposed' multiple-sequence of characters in 3. Again, all three are then logically equivalent and therefore comparable and syntactically interoperable.

As illustrated, some characters in the Unicode Standard have alternative representations (in fact, many do), but the Unicode Normalization Algorithm can be used to transform certain sequences of characters into canonical (or compatible) forms to test for equivalency. To determine equivalence, each character in the Unicode Standard is associated with a combining class, which is formally defined as a character property called ``Canonical Combining Class'' which is specified in the Unicode Character Database. The combining class assigned to each code point is a numeric value between 0 and 254 and is used by the Canonical Ordering Algorithm to determine which sequences of combining marks are canonically equivalent. Normalization forms, as very briefly described above, can be used to ensure character equivalence by ordering character sequences so that they can be faithfully compared.

It is very important to note that any software applications that is Unicode Standard compliant is free to change the character stream from one representation to another. This means that a software application may compose, decompose or reorder characters as its developers desire; as long as the resultant strings are canonically equivalent to the original. This might lead to unexpected behaviour for users. Various players, like the Unicode Consortium, the W3C, or the TEI recommend NFC in most user-directed situations, and some software applications that we tested indeed seem to automatically convert strings into NFC.\footnote{See the summary of various recommendation here: http://www.win.tue.nl/\textasciitilde{}aeb/linux/uc/nfc\_vs\_nfd.html. ) This means in practice that if a user, for example, enters <a> and <`>, i.e.~LATIN SMALL LETTER A at \uni{0061} and COMBINING GRAVE ACCENT at \uni{0300}, this might be converted into <à>, i.e.~LATIN SMALL LETTER A WITH GRAVE at \uni{00E0}.\footnote{The behaviour of software applications can be quite erratic in this respect. For example, Apple's TextEdit does not do any conversion on text entry. However, when you copy and paste some text inside the same document in ``rich text mode'' (i.e.~RTF-format), it will be transformed into NFC on paste. Saving a document does not do any conversion to the glyphs on screen, but it will save the characters in NFC.)

\subsection{Pitfall: Characters are not glyphs}\label{pitfall-characters-are-not-glyphs}

A central principle of Unicode is the distinction between character and glyph. A character is the abstract notion of a symbol in a writing system, while a glyph is the concrete drawing of such a symbol. In practice, there is a complex interaction between characters and glyphs. A single Unicode character may of course be rendered as a single glyph. However, a character may also be a `piece' of a glyph, or vice-versa. Actually, all possible relations between glyphs and characters are attested.

First, \textsc{a single character may have different contextually determined glyphs.} For example, characters in writing systems like Hebrew and Arabic have different glyphs depending on where they appear in a word. Some letters in Hebrew change their form at the end of the word, and in Arabic, primary letters have four contextually-sensitive variants (isolated, word initial, medial and final). Second, \textsc{a single character may be rendered as a sequence of multiple glyphs.} For example, in Tamil one Unicode character may result in a combination of a consonant and vowel, which are rendered as two adjacent glyphs by fonts that supports Tamil. Third, \textsc{a single glyph may be a combination of multiple characters. }For example, the ligature <ﬁ>, a single glyph, is the result of two characters, <f> and <i>, that have undergone glyph substitution by font rendering. Like contextually-determined glyphs, ligatures are (intended) artifacts of text processing instructions. Finally, \textsc{a single glyph may be a part of a character} (e.g.~diacritics).

Further, the rendering of a glyph is also always dependent on the font being used. For example, the Unicode character LATIN SMALL LETTER G appears as and in the Computer Modern and Courier fonts because their typefaces are designed differently. Furthermore, font face may change the visual appearance of a character, for example Times New Roman two-story <a> changes to a single-story a in italics <\emph{a}>. This becomes a real problem for some phonetic typesetting (see Section 5, Pitfall 2).

In sum, character-to-glyph mappings are complex technological issues that the Unicode Consortium has had to address in the development of the Unicode Standard, but for the lay user they can be utterly confusing because visual rendering does not indicate logical encoding, or, in other words, what you see isn't (necessarily) what you get.

\subsection{Pitfall: File formats}\label{pitfall-file-formats}

Unicode is a character encoding standard, but these characters of course actually appear inside some kind of file. The most basic Unicode-based file format is pure line-based text, i.e.~strings of Unicode-encoded characters separated by line breaks (note that these line breaks are what for most people intuitively corresponds to paragraph breaks). Unfortunately, even within this apparently basic setting there exist a multitude of variants. In general, these different possibilities are well-understood in the software industry, and nowadays they normally do not lead to any problems for the end user. However, there are some situations in which a user is suddenly confronted with cryptic questions in the user interface involving abbreviations like LF, CR, BE, LE or BOM. Most prominently this occurs with exporting or importing data in several software applications from Microsoft. Basically, there are two different issues involved. First, the encoding of** line breaks** and, second, the encoding of the Unicode characters into \textsc{code units} and the related issue of \textsc{endianness}.

The issue with \textsc{line breaks} originated with the instructions necessary to direct a printing head to a new line. This involves two movements, known as \textsc{carriage return (CR)} (returning the printing head to the start of the line on the page) and \textsc{line feed (LF)} (moving the printing head to the next line on the page). Physically, these are two different events, but conceptually together they form one action. In the history of computing, various encodings of line breaks have been used (e.g.~CR+LF, LF+CR, only LF, or only CR). Currently, all Unix and Unix-derived systems use only LF as code for a line break, while software from Microsoft still uses a combination of CR and LF. Today, most software applications recognize both options, and are able to deal with either encoding of line breaks (until rather recently this was not the case, and using the wrong line breaks could lead to unexpected errors). Our impression is that there is a strong tendency in software development to standardize on the simpler ``only LF'' encoding for line breaks, and we suggest that everybody use this encoding whenever possible.

A code unit is a sequence of bits used to encode character in a encoding. The issue with code units stems from the question how to separate a stream of binary ones and zero, i.e.~bits, into chunks representing Unicode characters. Depending on different use cases, the Unicode Standard offers three different approaches, called UTF-32, UTF-16 and UTF-8.\footnote{The letters UTF stand for either ``Unicode Transformation Format'' or ``Universal Coded Character Set Transformation Format'', but the notion of `transformation' is a legacy notion that does not have meaning anymore. Nevertheless, the designation ``UTF'' (in capitals) has become an official standard designation, but should probably best be read as simply ``Unicode Format''.) The details of this issue is extensively explained in section 2.5 of the Unicode core specification. Basically, \textsc{UTF-32} encodes each character in 32 bits (32 `binary units', i.e.~32 zeros or ones) and is the most disk-space-consuming variant of the three. However, it is the most efficient encoding processing-wise, because the computer simply has to separate each character after 32 bits. In contrast, \textsc{UTF-16} uses only 16 bits per character, which is sufficient for the large majority of Unicode characters, but not for all of them. A special system of `surrogates' is defined within the Unicode Standard to deal with these additional characters. The effect is a more disk-space efficient encoding (approximately half the size), while adding a limited computational overhead. Finally, \textsc{UTF-8} is a more complex system that dynamically encodes each character with the minimally necessary number of bits, choosing either 8, 16 or 32 bits depending on the character. This represents again a strong reduction in space (particularly due to the high frequency of data using ASCII characters that need only 8 bits) at the expense of even more computation necessary to process such strings. However, because of the ever growing computational power of modern machines, the processing overhead is in most practical situations a non-issue, while saving on space is still useful, particularly for sending texts over the Internet. As an effect, UTF-8 has become the dominant encoding on the World Wide Web. We suggest that everybody uses UTF-8 as their default encoding.

A related problem is a general issue about how to store information in computer memory, which is known as \textsc{endianness}. The details of this issue go beyond the scope of this paper. It suffices to realize that there is a difference between \textsc{big-endian} (BE) storage and \textsc{little-endian} (LE) storage. The Unicode Standard offers a possibility to explicitly indicate what kind of storage is used by starting a file with a so-called `Byte Order Mark' (BOM). However, the Unicode Standard does not require the usage of BOM, preferring other non-Unicode methods to signal to computers which kind of endianness is used. This issue only arises with UTF-32 and UTF-16 encodings, and when using the preferred UTF-8, using a BOM is theoretically possible, but strongly dispreferred according to the Unicode Standard. We suggest that everyone tries to prevent the inclusion of BOM in your data.

Together with the topic of normalisation (NFC vs.~NFD, see Pitfall 5 above), this leads to the following nicely cryptic file-encoding recommendation, namely to preferably use \textsc{UTF-8 in NFC without BOM with LF line breaks}, whenever possible.

\section{5. IPA meets Unicode}\label{ipa-meets-unicode}

\subsection{Introducing the International Phonetic Alphabet (IPA)}\label{introducing-the-international-phonetic-alphabet-ipa}

The International Phonetic Alphabet (IPA) is a common standard in linguistics to transcribe sounds of spoken language into some Latin-based characters (International Phonetic Association 1999). Although IPA is reasonably easily adhered to with pen and paper, it is not trivial to encode IPA characters electronically. Early work addressing the need for a universal computing environment for writing systems and their computational complexity is discussed in Simons 1989. For a long time, linguists (like all other computer users) were limited to ASCII-encoded 7-bit characters, which only includes Latin characters, numbers and some punctuation and symbols. Restricted to these standard character sets that lacked IPA support or other language-specific graphemes that they needed, linguists devised their own solutions (cf.~Bird and Simons 2003). For example, some chose to represent unavailable graphemes with substitutes, e.g.~the combination of to represent . Tech-savvy linguists redefined selected characters from a character encoding by mapping custom made fonts to those code points. However, one linguist's electronic text would not render properly on another linguist's computer without access to the same font. Further, if two character encodings defined two character sets differently, then data could not be reliably and correctly displayed. This is a common example of the non-interoperability of data and data formats.

To alleviate this problem, during the late 1980s, SAMPA (Speech Assessment Methods Phonetic Alphabet) was created to represent IPA symbols with 7-bit printable ASCII character sequences, e.g. for {[}ɸ{]}. Two problems with SAMPA are that (i) it is only a partial encoding of the IPA and (ii) it encodes different languages in separate data tables, instead of a universal alphabet, like IPA. SAMPA tables are derived from phonemes appearing in several European languages that were developed as part of a European Commission-funded project to address technical problems like electronic mail exchange (what is now simply called email). SAMPA is essentially a hack to work around displaying IPA characters, but it provided speech technology and other fields a basis that has been widely adopted and used in code. So, SAMPA was a collection of tables to be compared, instead of a large universal table representing all languages. An extended version of SAMPA, called X-SAMPA, set out to include every symbol in the IPA chart including all diacritics (Wells nd.). X-SAMPA was considered more universally applicable because it consisted of one table that encoded the set of characters that represented phones/segments in IPA across languages. SAMPA and X-SAMPA have been widely used for speech technology and as an encoding system in computational linguistics. Eventually, ASCII-encoding of the IPA became depreciated through the advent of the Unicode Standard. Note however that many popular software packages used for linguistic analyses still require ASCII input, e.g.~RuG/L04\footnote{http://www.let.rug.nl/kleiweg/L04/) and SplitsTree4.\footnote{http://www.splitstree.org/)

Still, there are a few pitfalls to be aware of when using the Unicode Standard to encode IPA. As we have said before, from a linguistic perspective it might look like the Unicode Consortium is making incomprehensible decisions, but it is important to realize that the consortium has tried and is continuing to try to be as consistent as possible across a wide range of use cases, and it does place linguistic traditions above other orthographic possibilities. In general, we strongly suggest linguists not to complain about any decisions in the Unicode Standard, but to try and understand the rationale of the Unicode Consortium (which in our experience is almost always well-conceived) and devise ways to work with any unexpected behaviour. Many of the current problems derive from the fact that the IPA is clearly historically based on the Latin script, but different enough from most other Latin-based writing systems to warrant special attention. This ambivalent status of the IPA glyphs (partly Latin, partly special) is unfortunately also attested in the treatment of IPA in the Unicode Standard. In retrospect, it might have been better to consider the IPA (and other transcription systems) to be a special or new kind of script within the Unicode Standard, and treat the obvious similarity to Latin glyphs as a historical relic. All IPA glyphs would then have their own code points, instead of the current situation in which some IPA glyphs have special code points, while others are treated as being identical to the `regular' Latin characters. Yet, the current situation, however unfortunate, is unlikely to change, so as linguists we will have to learn to deal with the specific pitfalls of IPA within the Unicode Standard. In this section, we will describe these pitfalls in some detail.

\subsection{Pitfall: There is no single complete Unicode code block for IPA}\label{pitfall-there-is-no-single-complete-unicode-code-block-for-ipa}

The ambivalent nature of IPA glyphs arises because, on the one hand, the IPA uses Latin-based glyphs like , or . From this perspective, the IPA seems to be just another orthographic tradition using Latin characters, all of which do not get a special treatment within the Unicode Standard (just like e.g.~the French, German, or Danish orthographic traditions do not have a special status). On the other hand, the IPA uses many special symbols (like turned , mirrored and/or extended Latin glyphs ) not found in any other Latin-based writing system. For this reason, and already in the first version of the Unicode Standard (Version 1.0 from 1991), a special block with code points, called ``IPA Extensions'' was included. As noted in Section 4, Pitfall 1, the Unicode Standard code space is subdivided into character blocks, which generally encode characters from a single script. However, as is illustrated by the IPA, characters that form a single writing system may be dispersed across several different character blocks. With its diverse collection of symbols from various scripts and diacritics, the IPA is spread across over 13 blocks in the Unicode Standard:\footnote{This number of blocks depends on whether only IPA-sanctioned symbols are counted or if the phonetic symbols commonly found in the literature are also included, see Moran 2012, Appendix C.)
\begin{itemize}
	\item Basic Latin (30 characters), e.g. <a, b, c, d, e> 
	\item Latin-1 Supplement (4 characters): <æ, ç, ð, ø\textgreater{} 
	\item Latin Extended-A (3 characters): <ħ, ŋ, œ> 
	\item Latin Extended-B (5 characters): <ǀ, ǁ, ǂ, ǃ, ȵ> 
	\item IPA Extensions (70 characters), e.g.: <ɐ, ɑ, ɔ> 
	\item Spacing Modifier Letters (20 characters), e.g.: <ʰ ʷ ˥> 
	\item Combining Diacritical Marks (33 characters), e.g.: <̝ ̥ ̪> 
	\item Greek and Coptic (3 characters): <β, θ, χ> 
	\item Phonetic Extensions (2 characters): <ᴅ, ᴴ> 
	\item Phonetic Extensions Supplement (3 characters): <ᶑ , ᶾ, ᶣ> 
	\item Superscripts and Subscripts (1 character): <ⁿ> 
	\item Arrows (2 characters): \textless{}↑, ↓\textgreater{} 
	\item Latin Extended-C (1 character): <ⱱ> 
\end{itemize}

\subsection{Pitfall: There are many IPA homoglyphs in Unicode}\label{pitfall-there-are-many-ipa-homoglyphs-in-unicode}

Another problem is the large number of homoglyphs, i.e.~different characters that have highly similar glyphs (or even completely identical, depending on the font rendering). For example, a speaker of Russian should ideally not use the CYRILLIC SMALL LETTER A at code point \uni{0430} for IPA transcriptions, but instead the LATIN SMALL LETTER A at code point \uni{0061}, although visually they are mostly indistinguishable, and the Cyrillic character is more easily typed on a Cyrillic keyboard. Another example we commonly encounter is the use of LATIN SMALL LETTER G at \uni{0067}, instead of the sanctioned Unicode Standard IPA character for the voiced velar stop LATIN SMALL LETTER SCRIPT G at \uni{0261}. One begins to question whether this issue is at all apparent to the working linguist, or if they simply use the \uni{0067} because it is easily keyboarded and thus saves time, whereas the latter must be cumbersomely inserted as a special symbol in most software.\footnote{This issue was recently addressed by the International Phonetic Association, which has taken the stance that both the keyboard LATIN SMALL LETTER G and the LATIN SMALL LETTER SCRIPT G are valid input characters for the voiced velar plosive. Unfortunately, this decision further introduces ambiguity for linguists trying to adhere to a strict Unicode Standard IPA encoding.)

Furthermore, on the one hand even linguists are unlikely to distinguish between the LATIN SMALL LETTER SCHWA at code point \uni{0259} and LATIN SMALL LETTER TURNED E at \uni{01DD}. On the other hand, non-linguists are unlikely to distinguish any semantic difference between an open back unrounded vowel , the LATIN SMALL LETTER ALPHA at \uni{0251}, and the open front unrounded vowel , LATIN SMALL LETTER A at \uni{0061}. But even among linguists this distinction leads to problems. For example, as pointed out by Mielke (2009), there is a problem stemming from the fact that about 75\% of languages are reported to have a five-vowel system (Maddieson 1984). Historically, linguistic descriptions tend not to include precise audio recording and measurements of formants, so this may lead one to ask if the many characters that are used in phonological description reflects a ``transcriptional bias''. The common use of in transcriptions could be in part due to the ease of typing the letter on an English keyboard (or for older descriptions, the typewriter). We found it to be exceedingly rare that a linguist uses for a low back unrounded vowel.\footnote{One example is Vidal 2001a:75, in which the author states: ``The definition of Pilagá /a/ as {[}+back{]} results from its behavior in certain phonological contexts. For instance, uvular and pharyngeal consonants only occur around /a/ and /o/. Hence, the characterization of /a/ and /o/ as a natural class of (i.e., {[}+back{]} vowels), as opposed to /i/ and /e/.'') They simply use as long as there is no opposition to .\footnote{See Thomason's Language Log post, ``Why I don't love the International Phonetic Alphabet'', at: http://itre.cis.upenn.edu/\textasciitilde{}myl/languagelog/archives/005287.html.) Making things even more problematic, there is an old typographic tradition that the double-story uses a single-story in italics. This leads to the unfortunate effect that in most well-designed fonts the italics of and use the same glyph. If this distinction has to be kept upright in italics, the only solution we can currently offer is to use `slanted' glyphs (i.e.~artificially italicised glyphs) instead of real italics (i.e.~special italics glyphs designed by a typographer).\footnote{For example, the widely used IPA font Doulos SIL (http://scripts.sil.org/cms/scripts/page.php?item\_id=DoulosSIL) does not have real italics. This leads some word-processing software, like Microsoft Word, to produce slanted glyphs instead. That particular combination of font and software application will thus lead to the desired effect. However, note that when the text is transferred to another font (i.e.~one that includes real italics) and/or to another software application (like Apple Pages, which does not perform slanting), then this visual appearance will be lost. In this case we are thus still in the pre-Unicode situation in which the choice of font and rendering software actually matters. The ideal solution from a linguistic point of view would be the introduction of a new IPA code point for a different kind of which explicitly specifies that it should still be rendered as a double-story character when italicized. After informal discussion with various Unicode players, our impression is that this highly restricted problem is not sufficiently urgent to introduce even more -like characters in Unicode (which already lead to much confusion, see Section 4, Pitfall 4). This is a clear situation in which the Unicode Consortium is not just thinking about linguists, but has a more wide-ranging practical view to consider.)

Some other homoglyphs related to encoding IPA in the Unicode Standard are:
\begin{itemize}
	\item The uses of the apostrophe has led to long discussions on the Unicode Standard email list. An English keyboard inputs \textless{}`\textgreater{} APOSTROPHE at \uni{0027}, although the `preferred' Unicode apostrophe is the \textless{}'\textgreater{} RIGHT SINGLE QUOTATION MARK at \uni{2019}. Yet the glottal stop/glottalization/ejective marker is another completely different character, the MODIFIER LETTER APOSTROPHE at \uni{02BC}, but unfortunately looks mostly highly similar to \uni{2019}. 
	\item There is ambiguous encoding of IPA segments within the Unicode Standard. An example is the \uni{02C1} MODIFIER LETTER REVERSED GLOTTAL STOP vs the \uni{02E4} MODIFIER LETTER SMALL REVERSED GLOTTAL STOP . Both are denoted in the Unicode Standard as the `pharyngealized diacritic' and both appear in various resources representing phonetic data online. This is thus an example for which the Unicode Standard does not solve the linguistic standardization problem. 
	\item There is at least one case in which the character name assigned by the Unicode Consortium does not match the IPA's description: in the Unicode Standard at \uni{01C3} is labeled LATIN LETTER RETROFLEX CLICK, but in IPA is an alveolar or postalveolar click (not retroflex). This naming is probably best seen as a simple error in the Unicode Standard. Note that most linguists simply seem to use or <ou>) while ligatures are single characters (e.g. <ʧ> LATIN SMALL LETTER TESH DIGRAPH at \uni{02A7}). Ligatures arose in the context of printing easier-to-read texts, and are included in the Unicode Standard for reasons of legacy encoding. However, their usage is discouraged by the Unicode core specification. Specifically related to IPA, various phonetic combinations of characters (typically affricates) are available as single code-points in the Unicode Standard, but are designated as ``ligatures'' or ``digraphs'' (confusingly both names appear interchangeably). Such glyphs might be used by software to produce a pleasing display, but they should not be hard-coded into the text itself. In the context of IPA, characters like the following ligatures should thus \emph{not} be used. Instead a combination of two characters is preferred: 
	\item <ʣ> LATIN SMALL LETTER DZ DIGRAPH at \uni{02A3} (use <d> + <z> instead) 
	\item <ʧ> LATIN SMALL LETTER TESH DIGRAPH at \uni{02A7} (use <t> + <ʃ> instead) 
	\item <ʩ> LATIN SMALL LETTER FENG DIGRAPH at \uni{02A9} (use <f> + <ŋ> instead) 
\end{itemize}

However, there are a few Unicode characters that are historically ligatures, but which are today considered as simple characters in the Unicode Standard and thus should be used when writing IPA, namely:
\begin{itemize}
	\item <ɮ> LATIN SMALL LETTER LEZH at \uni{026E} 
	\item <œ> LATIN SMALL LIGATURE OE at \uni{0153} 
	\item <ɶ> LATIN LETTER SMALL CAPITAL OE at \uni{0276} 
	\item <æ> LATIN SMALL LETTER AE at \uni{00E6} 
\end{itemize}

\subsection{Pitfall: The IPA notion of diacritics is not the same as the Unicode Standard's notion of diacritics}\label{pitfall-the-ipa-notion-of-diacritics-is-not-the-same-as-the-unicode-standards-notion-of-diacritics}

Another pitfall is diacritics. The problem is that the meaning of the term `diacritics' as used by the IPA is not the same as it used in the Unicode Standard. Specifically, diacritics in the IPA-sense are either so-called \textsc{Combining Diacritical Marks} or \textsc{Spacing Modifier Letters} in the Unicode Standard. Crucially, Combining Diacritical Marks are by definition combined with the character before them (to form so-called default grapheme clusters, see Section 3). In contrast, Spacing Modifier Letters are by definition \emph{not} combined into grapheme clusters with the preceding character, but simply treated as separate letters. In the context of the IPA, the following IPA-diacritics are actually Spacing Modifier Letters in the Unicode Standard:
\begin{itemize}
	\item Length marks, namely <ː> MODIFIER LETTER TRIANGULAR COLON at \uni{02D0} and <ˑ> MODIFIER LETTER HALF TRIANGULAR COLON at \uni{02D1}. 
	\item Tone letters, like <˥> MODIFIER LETTER EXTRA-HIGH TONE BAR at \uni{02E5}, and others like this. 
	\item Superscript letters, like <ʰ> MODIFIER LETTER SMALL H at \uni{02B0} or <ˤ> MODIFIER LETTER SMALL REVERSED GLOTTAL STOP at \uni{02E4}, and others like this. 
	\item <˞> MODIFIER LETTER RHOTIC HOOK at \uni{02DE}. 
\end{itemize}

Although linguists might expect these characters to belong together with the character in front of them, at least for <ʰ> MODIFIER LETTER SMALL H at \uni{02B0} the Unicode Consortium's decision to treat it as a separate character is also linguistically correct, because according to the IPA it can be used both for aspiration (more precisely postaspiration following the base character) and preaspiration (preceding the base character). Note that there is a mechanism in Unicode to force separate characters to be combined (namely by using the ZERO WIDTH JOINER at \uni{200D}), but this seems to be a rather impractical, and probably not enforceable solution to us.

\subsection{Pitfall: Neither the IPA nor the Unicode Standard enforce a unique diacritic ordering}\label{pitfall-neither-the-ipa-nor-the-unicode-standard-enforce-a-unique-diacritic-ordering}

Also related to diacritics is the question of ordering. To our knowledge, the International Phonetic Association does not specify a specific ordering for diacritics that combine with phonetic base symbols; this exercise is left to the reasoning of the transcriber. However, such marks have to be explicitly ordered if sequences of them are to be interoperable and compatible. An example is a labialized aspirated alveolar plosive: <tʷʰ>. There is nothing holding linguists back from using <tʰʷ> instead (with exactly the same intended meaning). However, from a technical standpoint, these two sequences are different, e.g.~if both sequences are used in a document, searching for <tʷʰ> will not find any instances of <tʰʷ>, and vice versa. Likewise, a creaky voiced syllabic dental nasal can be encoded in various orders, e.g. <n̪̰̩>, <n̩̰̪> or <n̩̪̰>.

In accordance with the absence of any specification of ordering in the IPA, the Unicode Standard likewise does not propose any standard orderings. Both leave it to the user to be consistent. However, there is one aspect of ordering for which the Unicode Standard does present a canonical solution, which is uncontroversial from a linguistic perspective. Diacritics in the Unicode Standard (i.e.~Combining Diacritical Marks, see above) are classified in Canonical Combining Classes. In practice, the diacritics are distinguished by their position relative to the base character.\footnote{See http://unicode.org/reports/tr44/\#Canonical\_Combining\_Class\_Values for a detailed description.) When applying a Unicode normalization (NFC or NFD, see previous section), the diacritics in different positions are put in a specified order. This process therefore harmonizes the difference between different encodings, for example, of a midtone creaky voice vowel <ḛ̄>. This grapheme cluster can be encoded either as <e> + <̄> + <̰> or as <e> + <̰> + <̄> . To prevent this twofold encoding, the Unicode Standard specifies the second ordering as canonical (diacritics below before diacritics above).

When encoding a string according to the Unicode Standard, it is possible to do this either using the NFC (``composition'') or NFD (``decomposition'') normalization. Decomposition implies that precomposed characters (like <á> LATIN SMALL LETTER A WITH ACUTE at \uni{00E1}) will be split into its parts. This might sound preferable for a linguistic analysis, as the different diacritics are separated from the base characters. However, note that most attached elements like strokes (e.g.~in the <ɨ>), retroflex hooks (e.g.~in <ʐ>) or rhotic hooks (e.g.~in <ɝ>) will not be decomposed, but strangely enough a cedilla (like in <ç>) will be decomposed. In general, Unicode decomposition does not behave like a feature decomposition as expected from a linguistic perspective. It is thus important to consider Unicode decomposition only as a technical procedure, and not assume that it is linguistically sensible.

Facing the problem of specifying a consistent ordering of diacritics while developing a large database of phonological inventories from the world's languages, Moran (2012: 540) defines a set of diacritic ordering conventions.\footnote{The most recent version of these conventions is online: http://phoible.github.io/conventions/) The conventions are influenced by the linguistic literature, though some ad-hoc decisions had to be taken. The goal was to explicitly define all character sequences so that the vast variety of phonemes found in descriptions of the world's language were normalized into consistent character sequences, e.g.~if one language description uses and another , when both are intended to be phonetically equivalent, then a decision to normalize to one form was taken. For example, when a character sequence contains more than one character in Spacing Modifier Letters, the order that is proposed is the following (where <\textbar{}> indicates ``or'' and <→> indicates ``precedes''):
\begin{itemize}
	\itemsep1pt\parskip0pt\parsep0pt 
	\item \textsc{Spacing Modifier Letters ordering:} ( unreleased <̚> \textbar{} lateral release <ˡ> \textbar{} nasal release <ⁿ>) → ( palatalized <ʲ>) → ( labialized <ʷ>) → ( velarized <ˠ>) → ( pharyngealized <ˤ>) → ( aspirated <ʰ> \textbar{} ejective <ʼ>) → ( long <ː> \textbar{} half-long <ˑ>) 
\end{itemize}

If a character sequence contains more than one diacritic below the base character, then the place feature is applied first (dental, laminal, apical, fronted, backed, lowered, raised), then the laryngeal setting (voiced, voiceless, creaky voice, breathy voice) and finally the syllabic or non-syllabic marker (for vowels, ATR gets put on between the place and laryngeal setting). So:
\begin{itemize}
	\itemsep1pt\parskip0pt\parsep0pt 
	\item \textsc{Combining Diacritical Marks (below) ordering:} ( dental <t̪> \textbar{} laminal <t̻> \textbar{} apical <t̺>) → ( fronted <u̟> \textbar{} backed <e̠>) →( lowered <e̞> \textbar{} raised <e̝>) → ( ATR <e̘ e̙>) → ( voiced <s̬> \textbar{} voiceless <n̥> \textbar{} creaky voice <b̰> \textbar{} breathy voice <b̤>) → ( syllabic <n̩> \textbar{} non-syllabic <e̯>) 
\end{itemize}

Character sequences with diacritics above the base character were not problematic in Moran 2012 because they include only the centralized, mid-centralized and nasalized combining characters. Moran (2012) marks tones as singletons with Space Modifier Letters, e.g. \textless{}˦\textgreater{} for a phonemic high tone, instead of accent diacritics, alleviating potential conflicts. Building on the work of Moran (2012), if a character sequence contains more than one diacritic above the base character, we propose:
\begin{itemize}
	\itemsep1pt\parskip0pt\parsep0pt 
	\item \textsc{Combining Diacritical Marks (above) ordering:} (centralized <ë> \textbar{} mid-centralized <e̽>) → (extra short <ĕ>) →( tone accents, e.g. <è> ) → ( Spacing Modifier Letters ) → ( tone letters, e.g. <e˦>) 
\end{itemize}

\section{6. Orthography profiles}\label{orthography-profiles}

\subsection{Characterizing writing systems}\label{characterizing-writing-systems}

At this point in the course of rapid ongoing developments, we are left with a situation in which the Unicode Standard offers a highly detailed and flexible approach to deal computationally with writing systems, but it has unfortunately not influenced the linguistic practice very much. In many practical situations, the Unicode Standard is far too complex for the day-to-day practice in linguistics because it does not offer practical solutions for the down-to-earth problems of many linguists. In this section, we propose some simple practical guidelines and methods to improve on this situation.

Our central aims for linguistics, to be approached with a Unicode-based solution, are: (i) to improve the consistency of the encoding of sources, (ii) to transparently document knowledge about the writing system (including transliteration), and (iii) to do all of that in a way that is easy and quick to manage for many different sources with many different writing systems. The central concept in our proposal is the \textsc{orthography profile}, a simple tab-separated CSV text file, that characterizes and documents a writing system. We also offer basic implementations in Python and R to assist with the production of such files, and to apply orthography profiles for consistency testing, grapheme tokenization and transliteration. Not only can orthography profiles be helpful in the daily practice of linguistics, they also succinctly document the orthographic details of a specific source, and, as such, might fruitfully be published alongside sources (e.g.~in digital archives). Also, in high-level linguistic analyses in which the graphemic detail is of central importance (e.g.~phonotactic or comparative-historical studies), orthography profiles can transparently document the decisions that have been taken in the interpretation of the orthography in the sources used.

Given these goals, Unicode locale descriptions (see Section 3) might seem like the ideal orthography profiles. However, there are various practical obstacles preventing the use of such locale descriptions in the daily linguistic practice, namely: (i) the XML-structure is too verbose to easily and quickly produce or correct manually, (ii) locale descriptions are designed for a wide scope on information (like date formats or names of weekdays) most of which is not applicable for documenting writing systems, and (iii) most crucially, even if someone made the effort to produce a technically correct locale description for a specific source at hand, then it is nigh impossible to deploy the description. This is because a locale description has to be submitted to and accepted by the Unicode Common Locale Data Repository. The repository is (rightly so) not interested in descriptions that only apply to a limited set of sources (e.g.~only a single specific dictionary).

The major challenge then is developing an infrastructure to identify the elements that are individual graphemes in a source, specifically for the enormous variety of sources using some kind of alphabetic writing system. Authors of source documents (e.g.~dictionaries, wordlists, corpora) use a variety of writing systems that range from their own idiosyncratic transcriptions to already well-established practical or longstanding orthographies. Although the IPA is one practical choice as a sound-based normalization for writing systems (which can act as an interlingual pivot to attain interoperability across writing systems), graphemes in each writing system must also be identified and standardized if interoperability across different sources is to be achieved. In most cases, this amounts to more than simply mapping a grapheme to an IPA segment because graphemes must first be identified in context (e.g.~is the sequence one sound or two sounds or both?) and strings must be tokenized, which may include taking orthographic rules into account (e.g. between vowels is /n/ and after a vowel but before a consonant is a nasalized vowel /ṽ/). In our experience, data from each source must be individually tokenized into graphemes so that its orthographic structure is identified and its contents can be extracted. To extract data for analysis, a source-by-source approach is required before an orthography profile can be created. For example, almost each available lexicon on the world's languages is idiosyncratic in its orthography and thus requires lexicon-specific approaches to identify graphemes in the writing system and to map graphemes to phonemes, if desired.

Thus, our key proposal for the characterization of a writing system is to use a grapheme tokenization as an inter-orthographic pivot. Basically, any source document is tokenized by graphemes, and only then a mapping to IPA (or any other orthographic conversion) is performed. An orthography profile then is a description of the units and rules that are needed to adequately model a graphemic tokenization for a language variety as described in a particular source document. An orthography profile summarizes the Unicode (tailored) graphemes and orthographic rules used to write a language (the details of the structure and assumptions of such a profile will be presented in the next section).

As an example of graphemic tokenization, note the three different levels of technological and linguistic elements that interact in the hypothetical lexical form <tsʰṍ̰shi> :
\begin{enumerate}
	\def\labelenumi{\arabic{enumi}.} 
	\item code points (10 text elements): t s ʰ o ̃ ̰ ´ s h i 
	\item grapheme clusters (7 text elements): t s ʰ ṍ̰ s h i 
	\item tailored grapheme clusters (4 text elements): tsʰ ṍ̰ sh i 
\end{enumerate}

In (1), the string <tsʰṍ̰shi> has been tokenized into ten Unicode code points (using NFD normalization), delimited here by space. Unicode tokenization is required because sequences of code points can differ in their visual and logical orders. For example, <õ̰> is ambiguous to whether it is the sequence of + <̰> + <̃> or + <̃> + <̰>. Although these two variants are visually homoglyphs, computationally they are different. Unicode normalization should be applied to this string to reorder the code points into a canonical order, allowing the data to be treated canonically equivalently for search and comparison. In (2), the Unicode code points have been logically normalized and visually organized into grapheme clusters, as specified by the Unicode Standard. The combining character sequence <õ̰> is normalized and visually grouped together. Note that , the MODIFIER LETTER SMALL H at \uni{02B0}, is not grouped with . This is because it belongs to Spacing Modifier Letters category in the Unicode Standard. These characters are underspecified for the direction in which they modify a host character. For example, can indicate either pre- or post-aspiration (whereas the nasalization or creaky diacritic is defined in the Unicode Standard to apply to a specified base character). Finally, to arrive at the graphemic tokenization in (3), tailored grapheme clusters are needed (as for example specified in an orthography profile). For example, this orthography profile would specify that the sequence of characters , and form a single grapheme , and that and form a grapheme . The orthography profile could also specify orthographic rules, e.g.~when tokenization graphemes, in say English, the in the forms and should be treated as distinct sequences depending on their contexts.

\subsection{Informal description of orthography profiles}\label{informal-description-of-orthography-profiles}

An orthography profile describes the Unicode code points, characters, graphemes and orthographic rules in a writing system. An orthography profile is a language-specific (and often even resource-specific) description of the units and rules that are needed to adequately model a writing system. An important assumption of our work is that we assume a resource is encoded in Unicode (or has been converted to Unicode). Any data source that the Unicode Standard is unable to capture, will also not be captured by an orthography profile.

Informally, an orthography profile specifies the graphemes (or, in Unicode parlance, ``tailored grapheme clusters'') that are expected to occur in any data to be analysed or checked for consistency. These graphemes are first identified throughout the whole data (a step which we call `tokenization'), and possibly simply returned as such, possibly including error messages about any parts of the data that are not specified by the orthography profile. Once the graphemes are identified, they might also be changed into other graphemes (a step which we call `transliteration'). When a grapheme has different possible transliterations, then these differences should be separated by contextual specification, possibly down to listing individual exceptional cases.

In practice, we foresee a workflow in which orthography profiles are iteratively refined, while at the same time inconsistencies and errors in the data to be tokenized are corrected. In some more complex use-cases there might even be a need for multiple different orthography profiles to be applied in sequence (see Section 8 on various exemplary use-cases). The result of any such workflow will normally be a cleaned dataset and an explicit description of the orthographic structure in the form of an orthography profile. Subsequently, the orthography profiles can be easily distributed in scholarly channels alongside the cleaned data, for example in supplementary material added to journal papers or in electronic archives.

\subsection{Formal specification of orthography profiles}\label{formal-specification-of-orthography-profiles}

The formal specifications of an orthography profile (or simply `profile' for short) are the following:
\begin{enumerate}
	\def\labelenumi{\arabic{enumi}.} 
	\item \textsc{A profile is a} \textsc{Unicode UTF-8 encoded text file} (ideally using NFC, no-BOM, and LF; see Section 4, Pitfall ``File Formats'') that includes the information pertinent to the orthography. 
	\item \textsc{A profile is a} \textsc{tab-separated CSV file with an obligatory header line}. A minimal profile can have just a single column, in which case there will of course no tabs, but the first line will still be the header. For all columns we assume the name in the header of the CSV file to be crucial. The actual ordering of the columns is unimportant. 
	\item \textsc{Lines starting with a hash \textless{}\#\textgreater{} are ignored.} Comments and metadata can be included inside the file, but only as complete lines in the profile, to be marked by lines starting with hash ``\#'' (NUMBER SIGN at \uni{0023}). Hashes somewhere else in the file are to be treated literally, i.e.~hashes are only to be ignored when they occur at the start of a line.\footnote{Comments that belong to specific lines will have to be put in a separate column of the CSV file, e.g.~add a column called `comments'. Further, if the content of a profile contains a hash at the start of a line, either reorder the columns so the hash does not occur at the start of the line, or add a dummy column in front of the data to not have the data start with a hash.) 
	\item \textsc{Metadata are given in commented lines at the beginning of the text file in a basic ``tag: value'' format. }Metadata about the orthographic description given in the orthography profile includes, minimally, (i) author, (ii) date, (iii) title, (iv) a stable language identifier encoded in BCP 47/ISO 639-3, and (v) bibliographic data for resource(s) that illustrate the orthography described in the profile. 
\end{enumerate}

The content of a profile consists of lines, each describing a grapheme of the orthography, using the following columns:
\begin{enumerate}
	\def\labelenumi{\arabic{enumi}.} 
	\item \textsc{A minimal profile consists of a single column with header `graphemes'}, listing each of the different graphemes in a separate line. 
	\item \textsc{Optional columns called `left' and `right' can be used to specify the left and right context of the grapheme, respectively.} The same grapheme can occur multiple times with different contextual specifications, for example to distinguish different pronunciations depending on the context. 
	\item \textsc{The columns `grapheme', `left' and `right' can use regular expression metacharacters.} If regular expressions are used, then all literal usage of the special symbols, like full stops ``.'' or dollar signs ``\$'' (so-called ``metacharacters'') have to be explicitly escaped by adding a backslash before them (i.e.~use ``.'' or ``\$''). Note that any specification of context automatically expects regular expressions, so it is probably better to always escape all regular expression metacharacters when used literally in the orthography, i.e.~the following symbols will need to be preceded by a backslash: {[} {]} ( ) \{ \} ~+ * . - ! ? \^{} \$ . 
	\item \textsc{An optional column called `class' can be used to specify classes of graphemes}, for example to define a class of vowels. Users can simply add ad-hoc identifiers in this column to indicate a group of graphemes, which can then be used in the description of the graphemes or the context. The identifiers should of course be chosen such that they do not conflate with any symbols used in the orthography themselves. Note that such classes only refer to the graphemes, not to the context. 
	\item \textsc{Columns describing transliterations for each graphemes can be added and named at will}. Often more than a single possible transliteration will be of interest. Any software application using these profiles should use the names of these columns to select a specific transliteration column. 
	\item \textsc{Any other columns can be added freely, but will mostly be ignored by any software application using the profiles}. As orthography profiles are also intended to be read and interpreted by humans, it is often highly useful to add extra information on the graphemes in further columns, like for example Unicode codepoints, Unicode names, frequency of occurrence, examples of occurrence, explanation of the contextual restrictions, or comments. 
\end{enumerate}

For the automatic processing of the profiles, the following technical standards will be expected:
\begin{enumerate}
	\def\labelenumi{\arabic{enumi}.} 
	\item \textsc{Each line of a profile will be interpreted as a regular expression. }Software applications using profiles can also offer to interpret a profile in the literal sense to avoid the necessity for the user to escape regular expressions metacharacters in the profile. However, this only is possible when no contexts or classes are described, so this seems only useful in the most basic orthographies. 
	\item \textsc{The `class' column will be used to produce explicit ``or'' chains of regular expressions}, which will then be inserted in the `graphemes', `left' and `right' columns at the position indicated by the class-identifiers. For example, a class ``V'' as a context specification might be replaced by a regular expression like ``(a\textbar{}e\textbar{}i\textbar{}o\textbar{}u\textbar{}ei\textbar{}au)''. Only the graphemes themselves are included here, not any contexts specified for the elements of the class. 
	\item \textsc{The `left' and `right' contexts will be included into the regular expressions by using lookbehind and lookahead}. Basically, the actual regular expression syntax of lookbehind and lookahead is simply hidden to the users by allowing them to only specify the contexts themselves. Internally, the contexts in the columns ``left'' and ``right'' are combined with the column ``graphemes'' to form a complex regular expression like ``(?\textless{}=left)graphemes(?=right)''. 
	\item \textsc{The regular expressions will be applied in the order as specified in the profile, from top to bottom.} A software implementation can offer help in figuring out the optimal ordering of the regular expressions, but should then explicitly report on the order used. 
\end{enumerate}

The actual implementation of the profile on some text-string will function as follows:
\begin{enumerate}
	\def\labelenumi{\arabic{enumi}.} 
	\item \textsc{All graphemes are matched in the text before they are tokenized or transliterated}. In this way, there is no necessity for the user to consider `feeding' and `bleeding' situations, in which the application of a rule either changes the text so another rule suddenly applies (``feeding'') or prevents another rule to apply (`bleeding'). 
	\item \textsc{The matching of the graphemes can occur either globally or linearly. }From a computer science perspective, the most natural way to match graphemes from a profile in some text is by walking linearly through the text-string from left to right, and at each position go through all graphemes in the profile to see which one matches, then go to the position at the end of the matched grapheme and start over. This is basically how a finite state transducer works, which is a well-established technique in computer science. However, from a linguistic point of view, our experience is that most linguists find it more natural to think from a global perspective. In this approach, the first grapheme in the profile is matched everywhere in the text-string first, before moving to the next grapheme in the profile. Theoretically, these approaches will lead to different results, though in practice of actual natural language orthographies they almost always lead to the same result. Still, we suggest that any software application using orthography profiles should offer both approaches (i.e. ``global'' or ``linear'') to the user. The approach used should be documented in the metadata as ``tokenization method''. 
	\item \textsc{The matching of the graphemes can occur either in NFC or NFD. }By default, both the profile and the text-string to be tokenized should be treated as NFC (see section 4, Pitfall ``Canonical equivalence'' above). However, in some use-cases it turns out to be practical to treat both text and profile as NFD. This typically happens when very many different combinations of diacritics occur in the data. An NFD-profile can then be used to first check which individual diacritics are used, before turning to the more cumbersome inspection of all combinations. We suggest that any software application using orthography profiles should offer both approaches (i.e. ``NFC'' or ``NFD'') to the user. The approach used should be documented in the metadata as ``unicode normalization''. 
	\item \textsc{The text-string is always returned in tokenized form} by separating the matched graphemes by a user-specified symbols-string. Any transliteration will be returned on top of the tokenization. 
	\item \textsc{Leftover characters (i.e.~characters that are not matched by the profile) should be reported to the user as errors.} Typically, the unmatched character are replaced in the tokenization by a user-specified symbol-string. 
\end{enumerate}

Any software application offering to use orthography profile:
\begin{enumerate}
	\def\labelenumi{\arabic{enumi}.} 
	\item \textsc{should offer user-options} to specify:
	\begin{enumerate}
		\def\labelenumii{\arabic{enumii}.} 
		\item \textsc{the name of the column to be used for transliteration} (if any). 
		\item \textsc{the symbol-string to be inserted between graphemes.} Optionally, a warning might be given if the chosen string includes characters from the orthography itself. 
		\item \textsc{the symbol-string to be inserted for unmatched strings} in the tokenized and transliterated output. 
		\item \textsc{the tokenization method}, i.e.~whether the tokenization should proceed ``global'' or ``linear''. 
		\item \textsc{unicode normalization}, i.e.~whether the text-string and profile should use ``NFC'' or ``NFD''. 
	\end{enumerate}
	\item \textsc{might offer user-options }to:
	\begin{enumerate}
		\def\labelenumii{\arabic{enumii}.} \setcounter{enumii}{5} 
		\item \textsc{assist in the ordering of the graphemes.} In our experience, it makes sense to apply larger graphemes before shorter graphemes, and to apply graphemes with context before graphemes without context. Further, frequently relevant rules might be applied after rarely relevant rules (though frequency is difficult to establish in practice, as it depends on the available data). Also, if this all fails to give any decisive ordering between rules, it seems useful to offer linguists the option to reverse the ordering from any manual specified ordering, because linguists tend to write the more general rule first, before turning to exceptions or special cases. 
		\item \textsc{assist in dealing with upper and lower case characters.} It seems practical to offer some basic case matching, so characters like <a> and <A> are treated equally. This will be useful in many concrete cases, although the user should be warned that case matching does not function universally in the same way across orthographies. Ideally, users should prepare orthography profiles with all lowercase and uppercase variants explicitly mentioned, so by default no case matching should be performed. 
		\item \textsc{treat the profile literal}, i.e.~to not interpret regular expression metacharacters. Matching graphemes literally often leads to strong speed increase, and would allow users to not needing to worry about escaping metacharacters. However, in our experience all actually interesting use-cases of orthography profiles include some contexts, which automatically prevents any literal interpretation, so by default the matching should not be literal. 
	\end{enumerate}
	\item \textsc{should return the following information} to the user:
	\begin{enumerate}
		\def\labelenumii{\arabic{enumii}.} \setcounter{enumii}{8} 
		\item \textsc{the original text-strings to be processed in the used Unicode normalization}, i.e.~in either NFC or NFD as specified by the user. 
		\item \textsc{the tokenized strings}, with additionally any transliterated strings, if transliteration is requested. 
		\item \textsc{a survey of all errors encountered}, ideally both in which text-strings any errors occurred and which characters in the text-strings lead to errors. 
		\item \textsc{a reordered profile}, when any automatic reordering is offered 
	\end{enumerate}
\end{enumerate}

\section{7. Use cases}\label{use-cases}

We now present several use cases that have motivated the development of orthography profiles. These include:
\begin{itemize}
	\item grapheme tokenization 
	\item normalization of orthographic systems to attain interoperability across different source documents, 
	\item cognate identification for detecting the similarity between words from different languages 
\end{itemize}

An important assumption of our work is that input sources are encoded in Unicode (UTF-8 to be precise). Anything that the Unicode Standard is unable to capture, cannot be captured by an orthography profile. We also use Unicode normalization form NFD to decompose all incoming text input into normalized and (Unicode) logically ordered strings. This process is of fundamental importance when working with text data in the Unicode Standard.

\emph{Grapheme tokenization}

Once text has been normalized, an additional and straightforward process is Unicode grapheme tokenization. More recently this regular expression, often available as short cut ``\X'', identifies all sequences of ``base'' characters followed by 0 or more combining diacritics. For example, the sequence, <ŋ͡mṍraː> would tokenize as a space-delimited sequence as: <ŋ͡ m ṍ r a ː>. For a first pass at identifying grapheme clusters, this is a straightforward tokenization. However, the linguist will note that both the tie bar (COMBINING DOUBLE INVERTED BREVE, a Unicode Combining Modifier) and the length marker (MODIFIER LETTER TRIANGULAR COLON. a Unicode Letter Modifier) do not appear ``correctly'', i.e.~the tie bar is grouped with the first character in the sequence and the length marker appears singly. This is necessary, as defined by the Unicode Standard, because of these character's semantic properties -- with Unicode we cannot know if, for example, the MODIFIER LETTER TRIANGULAR COLON should appear before or after the base character that it modifies, cf.~the IPA aspiration marker, , the MODIFIER LETTER SMALL H, which is also a Unicode Letter Modifier and for linguists a diacritic that be be used for pre- or post-aspiration. These ambiguities of position in tokenization are not uncommon in IPA, thus orthographic (or source) normalization and tokenization is needed.

*Cross-orthography **normalization*

Orthographic tokenization requires a specification of the tailored grapheme clusters (linguists: graphemes) to separate orthographic units within an input source. In the simplest case, the Unicode graphemic tokenization with regular expression ``\X'' captures the grapheme clusters, i.e.~the input data is graphemically tokenized and there are no additional string sequences required to capture each grapheme, e.g.~no specification. This scenario is the most unlikely.

The creation of an orthography typically requires that someone describes knowledge about the input data source's writing system. For example, when orthographically tokenizing a dictionary as input, the sequences of grapheme clusters need to be explicitly stated or a regular expression rule can be specified to transform the input. We illustrate these methods with examples from Thiesen 1998, a bilingual dictionary of Diccionario Bora-Castellano, and with Huber \& Huber 1992, a comparative vocabulary of sixty-nine indigenous languages of Columbia.

Huber \& Huber 1992

({[}a\textbar{}á\textbar{}e\textbar{}é\textbar{}i\textbar{}í\textbar{}o\textbar{}ó\textbar{}u\textbar{}ú{]})(n)(\s)({[}a\textbar{}á\textbar{}e\textbar{}é\textbar{}i\textbar{}í\textbar{}o\textbar{}ó\textbar{}u\textbar{}ú{]}), \1 \2 \4

A preliminary orthography profile can be generated with code available online.\footnote{https://github.com/bambooforest/tokenizer) The Python script creates a unigram model for individual Unicode characters or Unicode grapheme clusters and their frequency in the source input. We find that this gives a broad overview of the input at the character and grapheme levels and it oftens finds typographic errors in the source input by identifying rarely occurring or one-off characters in the input. The models also provides the basis for the ``quick'' development of an initial orthography profile by saving the user keyboarding time.

Note that these symbols are still phonetically unspecified, i.e.~symbol may or may not represent, say, a front open unrounded vowel.

The orthography profile is designed so that specified strings of characters are captured for tokenization.

\% how the tokenization works

The graphemic cluster specifications in the orthography profile are read into a trie data structure, which is used to tokenize the incoming input source by identifying sentinels (tokenization boundaries) in the input stream, which we then separate grapheme clusters by spaces.

An illustration is given in Figure N.

Comparative wordlists often start as the compilation of different sources with different writing systems. The Dogon comparative wordlist is an example of a compilation of slightly divergent transcription practices, which we will use to illustrate our point and to orthographically normalize an input source by transforming different transcriptions into a single IPA-like transcription.

*Orthographic normalization / cross-source **normalization*

Once we have normalized and tokenized our text elements, we can apply various sorts of source normalization across resources to create interoperable and comparable data.

Sound-based normalization is practical because \ldots{}

We use orthography profiles in our work to describe cross-linguistic difference between writing systems.

\emph{Cognate detection}

The identification of cognates is a non-trivial task, which involves identifying the semantic and phonetic similarity of words.

use grapheme correspondences to approximate cognacy and sound correspondences

to identify cognates and sound changes

Examples

Orthographic similarity != Phonetic similarity != Regular sound change

Cognate identification is further complicated by the fact that languages are written differently, and even more so for the low resource languages that we work with, linguists' phonetic transcriptions are heterogeneous and typically document-specific, i.e.~two linguists working on the same language variety most often develop different practical orthographies by using different symbols and different orthographic rules. Thus the essence of language documentation and description is diverse and ephemeral in nature. The encoding of linguistic data and linguistic data models is diverse and abstracting away from individual analytical preferences requires a document-by-document approach and a mechanism for standardization of writing to form an interlingual pivot so that different descriptions can be normalized into one encoding system to undertake analysis.

\% Orthography profile implementation in Lingpy / generic tokenizer

\% describe Tokenizer's function

\% describe algorithm for tokenizing IPA (Unicode grapheme cluster tokenization and then alignment of Letter Modifiers and Tie Bars)

\% insert an example of orthographic tokenization from PAD

\section{move Letter Modifiers left}\label{move-letter-modifiers-left}

(\s)(\p{Lm}), \2

which catches:

\section{Diacritics: Unicode letter modifiers (Lm)}\label{diacritics-unicode-letter-modifiers-lm}

\section{({[}ː\textbar{}ʰ\textbar{}ʼ\textbar{}ʿ\textbar{}ʾ{]})}\label{ux2d0ux2b0ux2bcux2bfux2be}

from the set of this:

\section{Diacritics: all}\label{diacritics-all}

\section{({[}ː\textbar{}ʰ\textbar{}ʼ\textbar{}ʿ\textbar{}ʾ\textbar{}̜\textbar{}̠\textbar{}̟\textbar{}͡\textbar{}̪\textbar{}̰\textbar{}̣\textbar{}̥\textbar{}̩\textbar{}‿\textbar{}.\textbar{}→\textbar{}˻\textbar{}+\textbar{}̃\textbar{}̫\textbar{}̤{]})}\label{ux2d0ux2b0ux2bcux2bfux2be.}

and allows us to with one regular expression do this:

PAD examples

A side effect of character tokenization is identification of outliers, e.g.

Some examples in Python

Some examples in R

{[}UNICODE USE-CASES{]}

The most basic overall Unicode character property is the \textsc{General Category}, which categorizes Unicode characters into: \emph{Letters, Punctuation, Symbols, Marks, Numbers, Separators, }and* Other*. Each Unicode character property also has a character property value. So each General Category is denoted by a property value by a single letter abbreviation, e.g. ``L'' for Letter. Each General Category property also has subcategories that are also properties with property values. For example, ``Letters'' is broken down into Uppercase Letter (Lu), Lowercase Letter (Ll), Titlecase Letter (Lt), Modifier Letter (Lm) and Other Letter (Lo).\footnote{The subcategory ``Other Letter'' includes characters such as uncased letters as in the Chinese writing system.) By defining character properties for each code point, algorithmic implementations such as text processing applications or regular expression engines that are conformant to the Unicode Standard can be used to recognize whole (sub)categories of characters or ranges of characters, e.g.~to match proper names in English in title case or more generally to match character types, such as scripts, or symbols and integers in email addresses, phone numbers, etc.

Script information is useful for algorithmic processes, such as collation, searching and for multilingual text processing. For example, if a regular expression engine includes functionality to match characters at the level of script, then it provides the ability to identify characters, e.g. \match{Greek}, which tests whether letters in a text belong to the Greek script or not. This functionality can be used to determine the boundaries between different writing systems that appear in the same document. In Section 6 (Use cases of grapheme tokenization) we provide an example of a regular expression match at the level of character property.

Unicode-aware regular expression engines may contain a meta-character, ``\X'', that matches this level of grapheme cluster, i.e.~a base character followed by some number of accents or diacritical marks. Grapheme clusters are important to determine word or line boundaries, for collation (`ordering') and for user interface interaction (e.g.~mouse selection, backspacing, cursor movement).

\section{9. Conclusion}\label{conclusion}

In our work we focus on the writing systems used in lesser-described and endangered languages and our orthography profiles are used to describe writing systems and to transpose them into phonetic transcription. Of course the International Phonetic Alphabet and other transcription systems are essentially just orthographies that are an approximation of sound. Nevertheless, sound-based normalization is practical in automatically identifying cognates and sound changes with quantitative methods. We leverage orthography profiles to enable comparative analysis of languages with different writing systems.

An orthography profile lists the graphemes in a particular description of language data, e.g.~a wordlist, dictionary or corpus. Building on the knowledge that can be extracted from that description by tokenizing words by the graphemes made explicit in the orthography profile, it is straightforward to undertake other analyses of the data. For example, various ngram models of the data can be extracted with a few lines of code. A unigram model with counts, frequencies and positive log probability (``plogs''; ``entropy under this model'') (Goldsmith \& Riggle, 2010) provides a fair amount of information about a given data source. Essentially, the orthography profile provides the description that allows this information to be calculated based on the mapping of sequences of characters into graphemic units. Bigram models are also straightforwardly extracted. In the case of bigrams, mutual information can be captured and used in various other statistical analyses, such as quantitative language comparison, inferring phylogenetic trees, etc.

\section{10. Nice text blocks to be included somewhere}\label{nice-text-blocks-to-be-included-somewhere}

Looking at the variety of writing systems used for the world's languages, there are various challenges for the computational processing of resources in which such writing systems are used.

A widespread mechanism to derive new symbols for a writing system is to use a combination of multiple symbols already available. That can lead to completely new symbols, like <æ> or <w>, which are still rather transparently derived from <a e> and <v v>, respectively. More typically, many orthographies use sequences of symbols as special characters, like <sch> or <gh>. To identify such `grapheme clusters' is one of the central problems to be tackled with orthography profiles. A further frequently used mechanism is to add extra material to an already available symbol, either to make it better visible (this is probably the reason for the addition of a dot on top of the <i>) or to derive new symbols. The widespread use of diacritics is the obvious result of such processes (Gaultney 2002). However, also the derivation of new symbols by added extra strokes is widespread (and not only in Far-Eastern character-based writing systems), like for example <ŋ> or <ç>.

The use of punctuation (Daniëls 1994, Humez \& Humez 2008) and spaces (Saenger 1997) to improve readability. {[}e.g.~space after punctuation, but not before{]}

Currently, the Unicode Standard is the standard character encoding for linguistic data (Anderson 2003). However, there are still hundreds of different encoding systems that were invented independently to capture orthographic diversity as different nations adopted and developed computer systems. These different encoding systems are problematic and in conflict with one another because different standards were formalized differently and for different purposes by different standards committees in different countries. No unified encoding scheme contained enough code points to encode all characters, so two different encoding schemes possibly used the same code point for different characters, or used different code points to represent the same character. Because computers support multiple character encoding schemes, data risked being corrupted when handled by different applications and encodings. The Unicode Standard was devised to alleviate these problems.

For orthographies, identifying graphemes can be even more challenging than identifying phones and phonemes in phonetic transcription because although transcriptions may not adhere strictly to IPA, they tend to have straightforward mappings between sounds and symbols. On the other hand, orthographies can introduce orthographic rules, which add an additional challenges in identifying graphemes in words, as mentioned above. Thus for resources not in IPA or IPA-like transcriptions, graphemes often must be manually identified, whether they are encoded as singletons or multi-character sequences. The identification of graphemes and the formulation of orthographic rules are used to create an orthography profile.

Character-to-glyph mappings are particularly not clear-cut for linguists using the IPA.\footnote{We give more linguistic examples in Section 5.) Just look in the IPA Extensions block.\footnote{Character ``blocks'' are described in Section 3 (The Unicode approach).) There are several characters that are glyphs (variants) of the same grapheme in the Latin block, e.g. and \& and .\footnote{LATIN SMALL LETTER ALPHA \uni{0251} vs LATIN SMALL LETTER A \uni{0061} , and LATIN SMALL LETTER SCRIPT G \uni{0261} vs LATIN SMALL LETTER G \uni{0067} .) Other characters like

, , do not appear at all in the IPA Extensions block, but as separate phonetic symbols; they are already encoded in the Basic Latin, Latin Extended-A and Greek \& Coptic blocks. When a linguist transcribes an IPA

on a QWERTY keyboard, if it is from the Basic Latin group, it is valid Unicode IPA (it is not valid IPA if it is from the Cyrillic Block). However, keyboard and and because they have been given specific code points in the IPA Extensions block, where they have been given special status as IPA characters, and which most often go unnoticed by linguists.\footnote{In Section 5 we discuss in detail the Unicode IPA alphabet and its characters dispersal across different Unicode Standards scripts and blocks.)

The Unicode Standard divides text elements roughly into two classes of graphemes. The cover term \textsc{grapheme cluster}\footnote{Formerly referred to as ``locale-independent graphemes'' to emphasize that the term grapheme is used differently in linguistics.) is used to emphasize the difference between the term \emph{grapheme }as used by linguists and what the users perceive as a character (a basic unit of a writing system for a language), which may not be a single Unicode character.\footnote{http://www.unicode.org/reports/tr29/) The Unicode Consortium makes a further distinction between grapheme clusters and tailored grapheme clusters.

The simplest example of a grapheme cluster is essentially a base character followed by a Letter Modifier character. For example, the sequence + \textless{}\textasciitilde{}\textgreater{} (COMBINING TILDE at \uni{0303}) combines visually into , a user-percieved character in writing systems like that of Spanish.\footnote{Although the code point sequence can also be represented with a single code point, LATIN SMALL LETTER N WITH TILDE at \uni{00F1}, it is not the case that all multi-code point character sequences have a single code point equivalent, which can be deduced from normalization. Often for commonly-used letters in European languages, a single code point form exists.)

What the user perceives as a single character may actually involved a multi-code point sequence.\footnote{Unicode-aware regular expression engines may contain a meta-character, ``\X'', that matches this level of grapheme cluster, i.e.~a base character followed by some number of accents or diacritical marks. As mentioned, grapheme clusters are important for collation algorithms, but they are also used to determine other processes such as word or line boundaries and for user interface interaction (e.g.~mouse selection, backspacing, cursor movement).)

More challenging for text processing is what the Unicode Consortium terms tailored grapheme clusters. For example, the sequence + for the Slovak digraph or the sequence . These grapheme clusters are ``tailored'' in the sense that they must be specified on a language-by-language basis. Algorithms can be adapted to use as input these tailored language-specific orthographies for transliteration, localization, etc. In fact, the Unicode Standard provides a method for defining language-specific tailored writing systems called the Unicode Locale Description. Unicode Locale Descriptions allow users to define a set of extended grapheme clusters. Unicode Locale Descriptions are saved in the Common Locale Data Repository (CLDR),\footnote{http://cldr.unicode.org/) a repository of language-specific definitions of writing system properties, each of which describes specific usages of characters. Each can be encoded in a Unicode Locale Data Markup Language (LDML) document. LDML is an XML format and vocabulary for the exchange of structured locale data. It is a format used not only for CLDR, but also for general interchange of locale data, such as in Microsoft's .NET. We return to LDML in Section 5 (Orthography profiles and locale descriptions).

We have noted that the rendering of electronic text requires that characters are encoded and stored in a computer's memory. These character sequences are then mapped from character code points to glyphs by software that takes into account a character's context. Transparent to most users is the fact that different logical orders of characters may result in the same visual output. For example, phonetic characters with certain combinations of diacritics may be homoglyphs (see below), even while the logical order of their character sequences are not the same.\footnote{One example is a vowel that is both nasalized and creaky voice, e.g. {[}ḛ̃{]}. See discussion below.) Thus some type of standardization, what the Unicode Consortium calls ``normalization'' of the ordering of (some classes of) characters, is required to make sure that all data are logically consistent and therefore easily comparable and equally searchable. The aim of standardization is to attain syntactic \textsc{interoperability} of data.\footnote{By interoperability we mean the ability to ubiquitously exchange and merge disparate (textual) data sets, and data encoding formats, to facilitate data sharing and to ``effortlessly'' undertake comparison and analysis. For a discussion of syntactic versus semantic interoperability, see Ide and Pustejovsky, 2010.)

The IPA is often used by linguists in undertaking initial phonetic and phonemic analyses of languages. However, once the distinctive sounds of a given language have been identified (and a phonological description of the target language has been described), a linguist may continue his or her documentation and description of higher-order structures like words and sentences with a broad phoneme-based alphabet. In fact, literary materials development often leapfrogs from a phonemic alphabet to locale-specific practical orthographies, in which numerous factors affect a language's orthography development, e.g.~politics, purpose, speaker communities familiarity with other writing systems, technological limitations.

One simple example is illustrated with two genealogically related languages spoken in Northern Ghana: Western Sisaala\footnote{Western Sisaala lacks /kp/.) and Sisaala Tumulung.\footnote{Sisaala Tumulung (ISO 639-3: sil) is spoken around the city Tumu, which is about 50 kilometers geographically to the east of Western Sisaala (ssl) speaking areas.) Both languages contain the same basic set of contrastive sounds, but their orthographies encode affricates (and vowels) differently graphemically; the influence stem from orthographies of government-sponsored literacy languages, which are taught to children in schools in these areas. In Western Sisaala the sound /tʃ/ is written and /dʒ/ as due to familiarity of these graphemes by students learning Dagaare (Moran 2006). Spoken further to the east, Sisaala Tumuling encodes the sound /tʃ/ as and /dʒ/ as (Blass 1975).

If encoded in the Unicode Standard, the design of both orthographies has to specify that certain sequences of characters, such as + , are single graphemes (compare English and which is ambiguous). Design is important, for example, for collation ordering (sorting), i.e.~the sequence of characters that is treated as a unit for ordering purposes, such as in dictionaries. Indeed, the characters , and are three unique graphemes consisting of two Unicode characters from the Latin script, but developers of a software program that recognizes the Western Sisaala alphabet might want to specify the sort order , , \ldots{} . Thus there is a need for orthography-specific specifications that define sequences of characters as graphemes, whether technologically, linguistically, or both, for multilingual text processing.

A major challenge that the developers of the Unicode Standard faced is that there exists no universal set of fundamental units of text because the divisions of text into elements, such as graphemes, is dependent on language-specific writing systems and orthographies, as just shown. Moreover, the division of electronic text into elements is process dependent, e.g.~sorting and spell checking may or may not need to take capitalization into account, depending on the orthography in question. Thus the encoding and processing of text requires well defined \textsc{text elements} and rules for interpreting them; these elements and rules are language-specific when we talk about major languages or well established orthographies, but more often in our research we face input from lesser-studied languages documented in author-specific transcription or practical orthography.\footnote{And this is usually down to the document-specific level, the description of which has been coined ``doculect'' for purposes of discussing differences of opinion in language descriptions (Cysouw and Good, 2013).)

In Section XXX we noted that the Unicode Consortium makes a crucial distinction between characters and glyphs. The relations between characters and glyphs is a complex one and it is typically hidden from the user. Technologically, we must distinguish between characters and glyphs because:
\begin{enumerate}
	\def\labelenumi{\arabic{enumi}.} 
	\item The logical order of a sequence of characters (in computer memory) may not be the same as the visual order of their (rendered) glyphs. 
	\item There is not always a one-to-one mapping between characters and glyphs. 
\end{enumerate}

Regarding the first point, a user cannot assume that contiguously rendered text on-screen actually reflects the computer's internal storage order of individual and sequential characters in memory, i.e.~the \textsc{logical order}. In other words, in the Unicode Standard the visual order of glyphs may not be the same as the logical order of their digitally encoded characters; contiguous display is not indicative of contiguous text.* *A nice example is Hebrew text.\footnote{http://www.macchiato.com/unicode/globalization-gotchas) Another example is the glyph <ṍ>, which may consist of one, two or three characters in encoded in various physical orders. The possible mismatch in sequences becomes clearer when we consider the second point.

When a base character, i.e.~any graphic character not in the General Category of Combining Mark, is followed by one or more characters having the property General Category of Combining Mark, they can be combined visually into one or more glyphs using the Canonical Ordering Algorithm to create Unicode \textsc{grapheme clusters}. As we have shown, this input may be ambiguous from a technological perspective if normalization is not taken into account. For example, a linguist may insert a single character or a character sequence for to denote a high back unrounded vowel with high tone:
\begin{itemize}
	\item LATIN SMALL LETTER U WITH ACUTE (\uni{00FA}) 
	\item LATIN SMALL LETTER U (\uni{0075}) + COMBINING ACUTE ACCENT (\uni{0301}) 
\end{itemize}

The normalization algorithms in the Unicode Standard are used to make these character sequences canonically equivalent for purposes such as ubiquitous search and comparison.

However, the standard does not specify many (or most) language-specific graphemes, or in Unicode parlance, \textsc{tailored grapheme clusters}, such as English .\footnote{Some (unfortunate) exceptions included for legacy purposes, and used incorrectly by linguists, include digraphs like LATIN SMALL LETTER TS DIGRAPH at \uni{02A6} or LATIN SMALL LETTER DEZH DIGRAPH at \uni{02A4}. These and other digraphs are encoded in the Unicode Standard's IPA Extensions block.) For technological reasons, these language-specific tailored grapheme clusters must be defined outside of the Unicode Standard, in what are called (language-specific) Unicode Locale Descriptions.\footnote{http://www.unicode.org/reports/tr35/) The encoding of these multiple character sequences in Locale Descriptions is beyond the need (or technical ability) of many linguists; thus we have developed a simpler approach which we used in our own research and describe in detailed in Section XXX. Basically, the description of writing systems for lower resource languages, and in particular for document-specific descriptions (e.g.~different descriptions of the same language with different orthographies), requires an orthographic description at the (tailored) grapheme (cluster) level.

An orthography profile is defined as a simple delimited flat text file encoded in the UTF-8, with character sequences in Unicode Normalization Form D. Metadata aside, the column delimited contents of an orthography profile are essential a state-table, i.e.~an implementation of a state transition table, but for ease of use encoded in a simple flat text file.

In an orthography profile, all character strings are converted into NFD because each character in a grapheme may have phonetic value, and by using NFD, all graphemes are decomposed into a standardized order. For example, a vowel that is both nasalized and creaky looks like in IPA, where is some vowel. Although visually the same, a nasalized and creaky vowel can be composed of several different character sequences, as illustrated with in 1-3:
\begin{enumerate}
	\def\labelenumi{\arabic{enumi}.} 
	\item <õ̰> LATIN SMALL LETTER O + COMBINING TILDE + COMBINING TILDE BELOW: \uni{006F} + \uni{0303} + \uni{0330} 
	\item <õ̰> LATIN SMALL LETTER O + COMBINING TILDE BELOW + COMBINING TILDE: \uni{006F} + \uni{0330} + \uni{0303} 
	\item <õ̰> LATIN SMALL LETTER O WITH TILDE + COMBINING TILDE BELOW: \uni{00F5} + \uni{0330} 
\end{enumerate}

Applying NFD to these three character sequences results in one standard sequence; in this example the character sequence is that shown 1. NFD makes different sequences of input interoperable and it retains all of the phonetic information captured by the separate characters that combine to form a vowel with nasalization and creaky voice phonation. This is an important step to consider in linguistic research because each character has phonetic value, i.e.~the combining tilde denotes that the base character is produce by a lowering of the velum, which allows air to escape through the nose when the sound is produced. The tilde below also has meaning: it represents a phonation type in which the vocal cords are vibrating, but they are also being held slightly further apart, allowing more air to escape, which produces a murmured sound. To enable comparative analyses of languages at a deep phonetic level, retaining a code point for each character of phonetic value is deciduous. For example, if the sound systems of two languages are to be compared, one would not want to lose a possibly contrastive phonological feature like nasalization, which may have been collapsed into a single character in its logical representation.

Our approach is similar in nature to the LDML, but we use a simpler state-table method, instead of generating an XML format for the ``exchange of structured locale data''. An orthography profile is a specification of the tailored grapheme clusters in a particular source document and any orthographic re-write rules needed after the application of Unicode normalization and orthographic profile tokenization. Orthography profiles can of course be used for transliteration between scripts, e.g.~from practical orthography to IPA and vice versa, by specifying mapping (replacements) from the list of tailored grapheme clusters in the source document and a set of replacement characters. This will be made clear in the next section in which we provide some use cases.

To summarize, LDML is basically overkill for our intentions of identifying and delimiting grapheme boundaries (or in Unicode parlance ``tailored grapheme clusters''). LDML is appropriate, even necessary, for complex localization complexes for software applications -- localization of, say, an operating system or software application across cultures may require the instantiation of different date stamps an other local standards. For linguistic purposes of source document data tokenization, we therefore develop a simple-to-implement process of ``orthographic'' tokenization, using standard Unicode Standard-specified processes (e.g.~normalization and grapheme cluster tokenization) with an additional level of linguistic ``tailored grapheme cluster'' specification.

\textsc{REFERENCES}

Nancy Ide and James Pustejovsky. 2010. What does inter- operability mean, anyway? toward an operational def- inition of interoperability for language technology. In Proceedings of the Second International Conference on Global Interoperability for Language Resources. Hong Kong, China.

Huber, Randal Q. and Robert B. Reed (compilers).
\begin{enumerate}
	\def\labelenumi{\arabic{enumi}.} \setcounter{enumi}{1991} 
	\itemsep1pt\parskip0pt\parsep0pt 
	\item Vocabulario comparativo: Palabras selectas de lenguas indígenas de Colombia (Comparative vocabulary: Selected words in indigenous languages of Colombia). Bogota: Instituto Lingüísti- co de Verano. 
\end{enumerate}

Thiesen, W. (1998). Diccionario bora-castellano, castellano-bora (1st ed.). Lima Perú: Instituto Linguístico de Verano.Retrieved from: http://www.worldcat.org/title/diccionario-bora-castellano-castellano-bora/oclc/40505215\&referer=brief\_results

\textsc{APPENDIX - ORTHOGRAPHY PROFILE EXAMPLE}

\textsc{APPENDIX - UNICODE IPA}

Provide a clear and full Unicode IPA chart with additional information as an appendix
\begin{verbatim}
	- create this chart
	
	- perhaps we should spell out conformance rules with regard to this chart? i.e. if a source claims conformance to our Unicode IPA chart, it should:
	
	- identify the sections (or range) of the table that the data conforms to
	
	- the source is in accordance with not only the Unicode IPA characters, but their intrinsic meanings (e.g. an Africanist might use <y> for /j/, but this level of conformance requires that <j> symbolizes /j/, etc.)
	
	- someone who wants to use an Africanist transcription system can declare that they use the Unicode IPA chart, except for a range of characters (but they provide the mapping to Unicode IPA chart) 
\end{verbatim}

\textsc{Notes}

\url{http://www.unicode.org/reports/tr29/}

The default boundary determination mechanism specified in this annex provides a straightforward and efficient way to determine some of the most significant boundaries in text: user-perceived characters, words, and sentences. Boundaries used in line breaking (also called \emph{word wrapping}) are defined in {[}\href{http://www.unicode.org/reports/tr41/tr41-11.html\#UAX14}{UAX14}{]}.

\textsc{Nice quotes}

vanZundert: ``How does one design and plan a general unified infrastructure for a target that is all over the place? A `one size fits all' approach would be a disastrous underestimation of the specific needs of humanities research. The essence of humanities research is in its diverse, heterogeneous and ephemeral nature. But the need of big infrastructures to be designed well before implementation and use, means that every single design decision closes the resulting system to some category of users. Anything specific can potentially break the data model and the defined workflow. Of course big infrastructure architects also know this; to compensate, they look for possibilities to make more abstract models and malleable workflows. But abstract away far enough and the model becomes rather meaningless. If the design recommendation of big infrastructure is that we will use XML, what was the use of all that time and effort? Big infrastructure design seems to routinely underestimate this problem. To give but one simple example: the token seems such a logical, granular, easy to automate conceptual catch-all for text models. Can there possibly be a more generalizable model for text than as a series of tokens? You may think so, until someone asks how you handle languages that do not use spaces, or how you account for prefixes and suffixes, let alone ambiguous functions. Regarding text as a series of tokens denies in any case several other essential humanistic aspects of text: its typography, its materiality, its referentiality to name but a few. At second glance, from the humanities research perspective, there is pretty little actually generalizable about tokenization.''

\% Ladefoged, Peter. 1990. Some Reflections on the IPA. In UCLA Working Papers in Phonetics, 74. ``The most interesting ambiguities in the Association's theoretical concepts concern what the symbols are symbolizing.''

There are several ambiguous aspects of the IPA, but the most interesting concerns the theoretical concepts that its symbols are symbolizing \cite[64]{Ladefoged1990a}. Although the IPA is \texttt{intended to be a set of symbols for representing all the possible sounds of the world\textquotesingle{}s languages\textquotesingle{}\textquotesingle{} \textbackslash{}citep\{IPA2005\}, Ladefoged points out that the IPA also states that IPA symbols are}\textit{not} symbols for phones they are simply shorthand for what a phonologist would regard as a bundle of features'' \cite{Ladefoged1990a}. From the IPA:
\begin{quote}
	
	``The representation of these sounds uses a set of phonetic categories which describe how each sound is made. These categories define classes of sounds that operate in phonological rules and historical sounds changes. The symbols of the IPA are shorthand ways of indicating certain intersections of these categories. Thus [p] is a shorthand way of designating the intersection of the categories voiceless, bilabial, and plosive; [m] is the intersection of the categories voiced, bilabial, and nasal; and so on.'' 
\end{quote}

\noindent Thus, the ``association is deliberately not explicit about what is meant by a phonological contrast'' \cite[65]{Ladefoged1990a}. 
