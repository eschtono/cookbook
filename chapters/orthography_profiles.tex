\chapter{Orthography profiles}
\label{orthography-profiles}

\section{Characterizing writing systems}
\label{characterizing-writing-systems}

At this point in the course of rapid ongoing developments, we are left with a
situation in which the Unicode Standard offers a highly detailed and flexible
approach to deal computationally with writing systems, but it has unfortunately
not influenced the linguistic practice very much. In many practical situations,
the Unicode Standard is far too complex for the day-to-day practice in
linguistics because it does not offer practical solutions for the down-to-earth
problems of many linguists. In this section, we propose some simple practical
guidelines and methods to improve on this situation.

Our central aims for linguistics, to be approached with a Unicode-based
solution, are: (i) to improve the consistency of the encoding of sources, (ii)
to transparently document knowledge about the writing system (including
transliteration), and (iii) to do all of that in a way that is easy and quick to
manage for many different sources with many different writing systems. The
central concept in our proposal is the \textsc{orthography profile}, a simple
tab-separated CSV text file, that characterizes and documents a writing system.
We also offer basic implementations in Python and R to assist with the
production of such files, and to apply orthography profiles for consistency
testing, grapheme tokenization and transliteration. Not only can orthography
profiles be helpful in the daily practice of linguistics, they also succinctly
document the orthographic details of a specific source, and, as such, might
fruitfully be published alongside sources (e.g.~in digital archives). Also, in
high-level linguistic analyzes in which the graphemic detail is of central
importance (e.g.~phonotactic or comparative-historical studies), orthography
profiles can transparently document the decisions that have been taken in the
interpretation of the orthography in the sources used.

Given these goals, Unicode locale descriptions (see Section~\ref{terminology})
might seem like the ideal orthography profiles. However, there are various
practical obstacles preventing the use of such locale descriptions in the daily
linguistic practice, namely: (i) the XML-structure is too verbose to easily and
quickly produce or correct manually, (ii) locale descriptions are designed for a
wide scope on information (like date formats or names of weekdays) most of which
is not applicable for documenting writing systems, and (iii) most crucially,
even if someone made the effort to produce a technically correct locale
description for a specific source at hand, then it is nigh impossible to deploy
the description. This is because a locale description has to be submitted to and
accepted by the Unicode Common Locale Data Repository. The repository is
(rightly so) not interested in descriptions that only apply to a limited set of
sources (e.g.~only a single specific dictionary).

The major challenge then is developing an infrastructure to identify the
elements that are individual graphemes in a source, specifically for the
enormous variety of sources using some kind of alphabetic writing system.
Authors of source documents (e.g.~dictionaries, wordlists, corpora) use a
variety of writing systems that range from their own idiosyncratic
transcriptions to already well-established practical or longstanding
orthographies. Although the IPA is one practical choice as a sound-based
normalization for writing systems (which can act as an interlingual pivot to
attain interoperability across writing systems), graphemes in each writing
system must also be identified and standardized if interoperability across
different sources is to be achieved. In most cases, this amounts to more than
simply mapping a grapheme to an IPA segment because graphemes must first be
identified in context (e.g.~is the sequence one sound or two sounds or both?)
and strings must be tokenized, which may include taking orthographic rules into
account (e.g.~between vowels is /n/ and after a vowel but before a consonant is
a nasalized vowel /ṽ/). In our experience, data from each source must be
individually tokenized into graphemes so that its orthographic structure is
identified and its contents can be extracted. To extract data for analysis, a
source-by-source approach is required before an orthography profile can be
created. For example, almost each available lexicon on the world's languages is
idiosyncratic in its orthography and thus requires lexicon-specific approaches
to identify graphemes in the writing system and to map graphemes to phonemes, if
desired.

Thus, our key proposal for the characterization of a writing system is to use a
grapheme tokenization as an inter-orthographic pivot. Basically, any source
document is tokenized by graphemes, and only then a mapping to IPA (or any other
orthographic conversion) is performed. An orthography profile then is a
description of the units and rules that are needed to adequately model a
graphemic tokenization for a language variety as described in a particular
source document. An orthography profile summarizes the Unicode (tailored)
graphemes and orthographic rules used to write a language (the details of the
structure and assumptions of such a profile will be presented in the next
section).

% TODO: add tsoshi figure here?

As an example of graphemic tokenization, note the three different levels of
technological and linguistic elements that interact in the hypothetical lexical
form <tsʰṍ̰shi>:

\begin{enumerate}
	\def\labelenumi{\arabic{enumi}.} 
	\item code points (10 text elements): t s ʰ o ̃ ̰ ´ s h i 
	\item grapheme clusters (7 text elements): t s ʰ ṍ̰ s h i 
	\item tailored grapheme clusters (4 text elements): tsʰ ṍ̰ sh i 
\end{enumerate}

In (1), the string <tsʰṍ̰shi> has been tokenized into ten Unicode code points
(using NFD normalization), delimited here by space. Unicode tokenization is
required because sequences of code points can differ in their visual and logical
orders. For example, <õ̰> is ambiguous to whether it is the sequence of + <̰> +
<̃> or + <̃> + <̰>. Although these two variants are visually homoglyphs,
computationally they are different. Unicode normalization should be applied to
this string to reorder the code points into a canonical order, allowing the data
to be treated canonically equivalently for search and comparison. In (2), the
Unicode code points have been logically normalized and visually organized into
grapheme clusters, as specified by the Unicode Standard. The combining character
sequence <õ̰> is normalized and visually grouped together. Note that, the
MODIFIER LETTER SMALL H at \uni{02B0}, is not grouped with. This is because it
belongs to Spacing Modifier Letters category in the Unicode Standard. These
characters are underspecified for the direction in which they modify a host
character. For example, can indicate either pre- or post-aspiration (whereas the
nasalization or creaky diacritic is defined in the Unicode Standard to apply to
a specified base character). Finally, to arrive at the graphemic tokenization in
(3), tailored grapheme clusters are needed (as for example specified in an
orthography profile). For example, this orthography profile would specify that
the sequence of characters, and form a single grapheme, and that and form a
grapheme. The orthography profile could also specify orthographic rules,
e.g.~when tokenization graphemes, in say English, the in the forms and should be
treated as distinct sequences depending on their contexts.

\section{Informal description}
\label{informal-description-of-orthography-profiles}

An orthography profile describes the Unicode code points, characters, graphemes
and orthographic rules in a writing system. An orthography profile is a
language-specific (and often even resource-specific) description of the units
and rules that are needed to adequately model a writing system. An important
assumption of our work is that we assume a resource is encoded in Unicode (or
has been converted to Unicode). Any data source that the Unicode Standard is
unable to capture, will also not be captured by an orthography profile.

Informally, an orthography profile specifies the graphemes (or, in Unicode
parlance, \textsc{tailored grapheme clusters}) that are expected to occur in any
data to be analyzed or checked for consistency. These graphemes are first
identified throughout the whole data (a step which we call
\textsc{tokenization}), and possibly simply returned as such, possibly including
error messages about any parts of the data that are not specified by the
orthography profile. Once the graphemes are identified, they might also be
changed into other graphemes (a step which we call \textsc{transliteration}).
When a grapheme has different possible transliterations, then these differences
should be separated by contextual specification, possibly down to listing
individual exceptional cases.

In practice, we foresee a workflow in which orthography profiles are iteratively
refined, while at the same time inconsistencies and errors in the data to be
tokenized are corrected. In some more complex use-cases there might even be a
need for multiple different orthography profiles to be applied in sequence (see
Section~\ref{use-cases} on various exemplary use-cases). The result of any such
workflow will normally be a cleaned dataset and an explicit description of the
orthographic structure in the form of an orthography profile. Subsequently, the
orthography profiles can be easily distributed in scholarly channels alongside
the cleaned data, for example in supplementary material added to journal papers
or in electronic archives.

\section{Formal specification}
\label{formal-specification-of-orthography-profiles}

The formal specifications of an orthography profile (or simply \textsc{profile}
for short) are the following:

\begin{enumerate}
	\def\labelenumi{\arabic{enumi}.} 
	\item \textsc{A profile is a} \textsc{Unicode UTF-8 encoded text file} (ideally using NFC, no-BOM, and LF; see Section~\ref{pitfall-file-formats}, Pitfall: File Formats) that includes the information pertinent to the orthography. 
	\item \textsc{A profile is a} \textsc{tab-separated CSV file with an obligatory header line}. A minimal profile can have just a single column, in which case there will of course no tabs, but the first line will still be the header. For all columns we assume the name in the header of the CSV file to be crucial. The actual ordering of the columns is unimportant. 
	\item \textsc{Lines starting with a hash \textless{}\#\textgreater{} are ignored.} Comments and metadata can be included inside the file, but only as complete lines in the profile, to be marked by lines starting with hash \textsc{\#} (\textsc{number sign} at \uni{0023}). Hashes somewhere else in the file are to be treated literally, i.e.~hashes are only to be ignored when they occur at the start of a line.\footnote{Comments that belong to specific lines will have to be put in a separate column of the CSV file, e.g.~add a column called \textsc{comments}. Further, if the content of a profile contains a hash at the start of a line, either reorder the columns so the hash does not occur at the start of the line, or add a dummy column in front of the data to not have the data start with a hash.} 
	\item \textsc{Metadata are given in commented lines at the beginning of the text file in a basic \textsc{tag: value} format. }Metadata about the orthographic description given in the orthography profile includes, minimally, (i) author, (ii) date, (iii) title, (iv) a stable language identifier encoded in BCP 47/ISO 639-3, and (v) bibliographic data for resource(s) that illustrate the orthography described in the profile. 
\end{enumerate}

The content of a profile consists of lines, each describing a grapheme of the
orthography, using the following columns:

\begin{enumerate}
	\def\labelenumi{\arabic{enumi}.} 
	\item \textsc{A minimal profile consists of a single column with header \textsc{graphemes}}, listing each of the different graphemes in a separate line. 
	\item \textsc{Optional columns called \textsc{left} and \textsc{right} can be used to specify the left and right context of the grapheme, respectively.} The same grapheme can occur multiple times with different contextual specifications, for example to distinguish different pronunciations depending on the context. 
	\item \textsc{The columns \textsc{grapheme}, \textsc{left} and \textsc{right} can use regular expression metacharacters.} If regular expressions are used, then all literal usage of the special symbols, like full stops <.> or dollar signs <\$> (so-called \textsc{metacharacters}) have to be explicitly escaped by adding a backslash before them (i.e.~use <.> or <\$>). Note that any specification of context automatically expects regular expressions, so it is probably better to always escape all regular expression metacharacters when used literally in the orthography, i.e.~the following symbols will need to be preceded by a backslash: {[} {]} ( ) \{ \} ~+ * . - ! ? \^{} \$ . 
	\item \textsc{An optional column called \textsc{class} can be used to specify classes of graphemes}, for example to define a class of vowels. Users can simply add ad-hoc identifiers in this column to indicate a group of graphemes, which can then be used in the description of the graphemes or the context. The identifiers should of course be chosen such that they do not conflate with any symbols used in the orthography themselves. Note that such classes only refer to the graphemes, not to the context. 
	\item \textsc{Columns describing transliterations for each graphemes can be added and named at will}. Often more than a single possible transliteration will be of interest. Any software application using these profiles should use the names of these columns to select a specific transliteration column. 
	\item \textsc{Any other columns can be added freely, but will mostly be ignored by any software application using the profiles}. As orthography profiles are also intended to be read and interpreted by humans, it is often highly useful to add extra information on the graphemes in further columns, like for example Unicode codepoints, Unicode names, frequency of occurrence, examples of occurrence, explanation of the contextual restrictions, or comments. 
\end{enumerate}

For the automatic processing of the profiles, the following technical standards
will be expected:

\begin{enumerate}
	\def\labelenumi{\arabic{enumi}.} 
	\item \textsc{Each line of a profile will be interpreted as a regular expression. }Software applications using profiles can also offer to interpret a profile in the literal sense to avoid the necessity for the user to escape regular expressions metacharacters in the profile. However, this only is possible when no contexts or classes are described, so this seems only useful in the most basic orthographies. 
	\item \textsc{The \textsc{class} column will be used to produce explicit \textsc{or} chains of regular expressions}, which will then be inserted in the \textsc{graphemes}, \textsc{left} and \textsc{right} columns at the position indicated by the class-identifiers. For example, a class \textsc{V} as a context specification might be replaced by a regular expression like: (a\textbar{}e\textbar{}i\textbar{}o\textbar{}u\textbar{}ei\textbar{}au). Only the graphemes themselves are included here, not any contexts specified for the elements of the class. 
	\item \textsc{The \textsc{left} and \textsc{right} contexts will be included into the regular expressions by using lookbehind and lookahead}. Basically, the actual regular expression syntax of lookbehind and lookahead is simply hidden to the users by allowing them to only specify the contexts themselves. Internally, the contexts in the columns \textsc{left} and \textsc{right} are combined with the column \textsc{graphemes} to form a complex regular expression like: (?\textless{}=left)graphemes(?=right). 
	\item \textsc{The regular expressions will be applied in the order as specified in the profile, from top to bottom.} A software implementation can offer help in figuring out the optimal ordering of the regular expressions, but should then explicitly report on the order used. 
\end{enumerate}

The actual implementation of the profile on some text-string will function as
follows:

\begin{enumerate}
	\def\labelenumi{\arabic{enumi}.} 
	\item \textsc{All graphemes are matched in the text before they are tokenized or transliterated}. In this way, there is no necessity for the user to consider `feeding' and `bleeding' situations, in which the application of a rule either changes the text so another rule suddenly applies (feeding) or prevents another rule to apply (`bleeding'). 
	\item \textsc{The matching of the graphemes can occur either globally or linearly. }From a computer science perspective, the most natural way to match graphemes from a profile in some text is by walking linearly through the text-string from left to right, and at each position go through all graphemes in the profile to see which one matches, then go to the position at the end of the matched grapheme and start over. This is basically how a finite state transducer works, which is a well-established technique in computer science. However, from a linguistic point of view, our experience is that most linguists find it more natural to think from a global perspective. In this approach, the first grapheme in the profile is matched everywhere in the text-string first, before moving to the next grapheme in the profile. Theoretically, these approaches will lead to different results, though in practice of actual natural language orthographies they almost always lead to the same result. Still, we suggest that any software application using orthography profiles should offer both approaches (i.e. \textsc{global} or \textsc{linear}) to the user. The approach used should be documented in the metadata as \textsc{tokenization method}. 
	\item \textsc{The matching of the graphemes can occur either in NFC or NFD. }By default, both the profile and the text-string to be tokenized should be treated as NFC (see section \ref{pitfall-canonical-equivalence}, Pitfall: Canonical equivalence, above). However, in some use-cases it turns out to be practical to treat both text and profile as NFD. This typically happens when very many different combinations of diacritics occur in the data. An NFD-profile can then be used to first check which individual diacritics are used, before turning to the more cumbersome inspection of all combinations. We suggest that any software application using orthography profiles should offer both approaches (i.e. \textsc{NFC} or \textsc{NFD}) to the user. The approach used should be documented in the metadata as \textsc{unicode normalization}. 
	\item \textsc{The text-string is always returned in tokenized form} by separating the matched graphemes by a user-specified symbols-string. Any transliteration will be returned on top of the tokenization. 
	\item \textsc{Leftover characters (i.e.~characters that are not matched by the profile) should be reported to the user as errors.} Typically, the unmatched character are replaced in the tokenization by a user-specified symbol-string. 
\end{enumerate}

Any software application offering to use orthography profile:

\begin{enumerate}
	\def\labelenumi{\arabic{enumi}.} 
	\item \textsc{should offer user-options} to specify:
	\begin{enumerate}
		\def\labelenumii{\arabic{enumii}.} 
		\item \textsc{the name of the column to be used for transliteration} (if any). 
		\item \textsc{the symbol-string to be inserted between graphemes.} Optionally, a warning might be given if the chosen string includes characters from the orthography itself. 
		\item \textsc{the symbol-string to be inserted for unmatched strings} in the tokenized and transliterated output. 
		\item \textsc{the tokenization method}, i.e.~whether the tokenization should proceed \textsc{global} or \textsc{linear}. 
		\item \textsc{unicode normalization}, i.e.~whether the text-string and profile should use \textsc{NFC} or \textsc{NFD}. 
	\end{enumerate}
	\item \textsc{might offer user-options }to:
	\begin{enumerate}
		\def\labelenumii{\arabic{enumii}.} \setcounter{enumii}{5} 
		\item \textsc{assist in the ordering of the graphemes.} In our experience, it makes sense to apply larger graphemes before shorter graphemes, and to apply graphemes with context before graphemes without context. Further, frequently relevant rules might be applied after rarely relevant rules (though frequency is difficult to establish in practice, as it depends on the available data). Also, if this all fails to give any decisive ordering between rules, it seems useful to offer linguists the option to reverse the ordering from any manual specified ordering, because linguists tend to write the more general rule first, before turning to exceptions or special cases. 
		\item \textsc{assist in dealing with upper and lower case characters.} It seems practical to offer some basic case matching, so characters like <a> and <A> are treated equally. This will be useful in many concrete cases, although the user should be warned that case matching does not function universally in the same way across orthographies. Ideally, users should prepare orthography profiles with all lowercase and uppercase variants explicitly mentioned, so by default no case matching should be performed. 
		\item \textsc{treat the profile literal}, i.e.~to not interpret regular expression metacharacters. Matching graphemes literally often leads to strong speed increase, and would allow users to not needing to worry about escaping metacharacters. However, in our experience all actually interesting use-cases of orthography profiles include some contexts, which automatically prevents any literal interpretation, so by default the matching should not be literal. 
	\end{enumerate}
	\item \textsc{should return the following information} to the user:
	\begin{enumerate}
		\def\labelenumii{\arabic{enumii}.} \setcounter{enumii}{8} 
		\item \textsc{the original text-strings to be processed in the used Unicode normalization}, i.e.~in either NFC or NFD as specified by the user. 
		\item \textsc{the tokenized strings}, with additionally any transliterated
        strings, if transliteration is requested. 
		\item \textsc{a survey of all errors encountered}, ideally both in which
        text-strings any errors occurred and which characters in the
        text-strings lead to errors. 
		\item \textsc{a reordered profile}, when any automatic reordering is offered 
	\end{enumerate}
\end{enumerate}

\section{Examples}

[Here should a few abstract short simple examples be added]

\ 

Note that to deal with ambiguous parsing cases, we can use the Unicode approach
using the zero width joiner. This is actually a non-joiner (the name is
confusing): the idea is to add this character into the text to identify cases in
which a sequence of characters is not supposed to be a complex grapheme (even
though the sequence is in the orthography profile)

