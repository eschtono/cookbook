\chapter{Encoding} \label{encoding}

There are many in-depth histories of the origin and development of writing
systems (e.g.~\citet{Robinson1997,Powell2012}), a story that we will not repeat
here. However, the history of turning writing into computer readable code is not
so often told, so we decided to offer a short survey of the major developments
here. This history turns out to be intimately related to the history of
telegraphic communication.\footnote{Because of the recent history as summarized
in this section, we have used mostly rather ephemeral internet sources. When not
references by traditional literature in the bibliography, we have specifically
used \url{http://www.unicode.org/history/} and various Wikipedia pages for the
information presented here. A useful survey of the historical development of the
physical hardware of telegraphy and telecommunication is \citet{Huurdeman2003}.
Most books that discuss the development of encoding of telegraphic communication
focus of cryptography, e.g.~\citet{Singh1999}, and forego the rather interesting
story of ``open encoding'' that is related here.}

Writing systems have existed for roughly 6000 years, allowing people to exchange
messages through time and space. Additionally, to bridge large geographic
distances, \textsc{telegraphic systems} of communication (from Greek \emph{τῆλε
γράφειν} `distant writing') have a long and widespread history since ancient
times. The most widespread telegraphic systems worldwide are so-called whistled
languages \citep{Meyer2015}, but also drumming languages \citep{Meyer_etal2012}
and smoke, fire, hydraulic or flag signals are forms of telegraphy.

Telegraphy was reinvigorated in the end of the eighteenth century through the
introduction of semaphoric systems by Claude Chapelle (since then using flags,
flashing lights, or various specially designed contraptions) to convey messages
over large distances. The \textit{innovation} of those systems was that all
characters of the written language were replaced one-to-one by visual signals.
Since then, all telegraphic systems have taken this principle, namely that any
language to be transmitted first has to be turned into some orthographic system,
which subsequently is encoded for transmission by the sender, and then turned
back into orthographic representation at the receiver side.\footnote{Sound and
video-based telecommunication of course takes a different approach by ignoring
the written version of language and directly encode sound waves or light
patterns.} This of course implies that the usefulness of any such telegraphic
encoding completely depends on the sometimes rather haphazard structure of
orthographic systems.

In the nineteenth century, \textsc{electric telegraphy} lead to a new approach
in which written language characters were encoded by signals sent through a
copper wire. Originally, \textsc{bisignal} codes were used, consisting of two
different signals. For example, Carl Friedrich Gauss in 1833 used positive and
negative current \citep[282]{Mania2008}. More famous and influential, Samuel
Morse in 1836 used long and short pulses.

In those \textsc{bisignal codes} each character from the written language was
encoded with a different number of signals (between one and five), so two
different separators are needed: one between signals and one between characters.
For example, in Morse-code there is a short pause between signals and a long
pause between characters. Actually, Morse-code also includes an extra long pause
between words. Interestingly, it took a long time to consider the written word
boundary---using white-space---as a bona-fide character that should simply be
encoded with its own code point. This happened only with the revision of the
Baudot-code (see below) by Donald Murray in 1901, in which he introduced a
specific white-space code. This principle has been followed ever since.

From those bisignal encodings, true \textsc{binary codes} developed with a fixed
length of signals per character. In such systems only a single separator between
signals is needed, because the separation between characters can be established
by counting until a fixed number of signals.\footnote{Of course, no explicit
separator is needed when the timing of the signals is known, which is the
principle used in all modern telecommunication systems. An important modern
consideration is also how to know where to start counting when you did not catch
the start of a message, something that is known in Unicode as \textsc{self
synchronization}.}

In the context of electric telegraphy, such a binary code system was first
established by Émile Baudot in 1870, using a fixed combination of five signals
for each written character.\footnote{True binary codes have a longer history,
going at least back to the Baconian cipher devised by Francis Bacon in 1605.
However, the proposal by Baudot was the quintessential proposal leading to all
modern systems.} There are $2^5 = 32$ possible combination when using five
binary signals; an encoding today designated as \textsc{5-bit}. These codes are
sufficient for all Latin letters, but of course they do not suffice for all
written symbols, including punctuation and digits. As a solution, the Baudot
code uses a so-called \textsc{shift} character, which signifies that from that
point onwards---until shifted back---a different encoding is used, allowing for
yet another set of 32 codes. In effect, this means that the Baudot code, and the
\textsc{International Telegraph Alphabet} (ITA) derived from it, had an extra
\textsc{bit} of information, so the encoding is actually 6-bit (with $2^6 = 64$
different possible characters). For decades, this encoding was the standard for
all telegraphy and it is still in limited use today.

To also allow for different uppercase and lowercase letters and a large variety
of control characters to be used in the newly developing technology of
computers, the American Standards Association decided to propose a new 7-bit
encoding in 1963 (with $2^7 = 128$ different possible characters), known as the
\textsc{American Standard Code for Information Interchange} (ASCII), geared
towards the encoding of English orthography.

With the ascent of other orthographies in computer usage, the wish to encode
further variation of Latin letters (like German <ß> or various letters with
diacritics like <è>) led the Digital Equipment Corporation to introduce an 8-bit
\textsc{Multinational Character Set} (MCS, with $2^8 = 256$ different possible
characters), first used with the introduction of the {\small VT}220 Terminal in
1983.

Because 256 characters were clearly not enough for the many different characters
needed in the world's writing systems, the ISO/IEC~8859 standard in 1987
extended the MCS to include 16 different 8-bit code pages. For example, part 5
was used for Cyrillic characters, part 6 for Arabic, and part 7 for
Greek.\footnote{In effect, because $16 = 2^4$, this means that ISO/IEC~8859 was
actually a $8+4=12$-bit encoding, though with very many duplicates by design
(e.g.~all ASCII codes were repeated in each 8-bit code page). To be precise,
ISO/IEC~8859 used the 7-bit ASCII as the basis for each code page, and defined
16 different 7-bit extensions, leading to $(1+16)\cdot{2^7} = 2,176$ possible
characters.}

This system almost immediately was understood to be insufficient and
impractical, so various initiatives to extend and reorganize the encoding
started in the 1980s. This led, for example, to various proprietary encodings
from Microsoft (e.g.~Windows Latin 1) and Apple (e.g.~Mac OS Roman), which one
still sometimes encounters today.

More wide-ranging, various people in the 1980s started to develop true
international code sets. In the United States, a group of computer scientists
formed the \textsc{Unicode Consortium}, proposing a 16-bit encoding in 1991
(with $2^{16} = 65,536$ different possible characters). At the same time in
Europe, the \textsc{International Organization for Standardization} (ISO) was
working on ISO~10646 to supplant the ISO/IEC~8859 standard. Their first draft of
the \textsc{Universal Character Set} (UCS) in 1990 was 31-bit (with
theoretically $2^{31} = 2,147,483,648$ possible characters, but because of some
technical restrictions only 679,477,248 were allowed).

Since 1991, the Unicode Consortium and the ISO jointly develop the
\textsc{unicode standard}, or ISO/IEC~10646, leading to the current system
including the original 16-bit Unicode proposal as the \textsc{basic multilingual
plane}, and 16 additional planes of 16-bit for further extensions (with in total
$(1+16) \cdot 2^{16} = 1,114,112$ possible characters). The most recent version
of the Unicode Standard (7.0) was published in June 2014 and it defines 112,956
different characters \citep{Unicode2014}.
