nld10K <- read.delim("data/nld10Kwords.txt", header = FALSE)
str(nld10K)
nld10K[1:10,2]
words <- as.character(nld10K)
head(words)
str(words)
words <- as.character(nld10K[,2])
str(words)
library(qlcTokenize)
tmp <- write.profile(words)
str(tmp)
tmp
nld10K <- read.delim("data/nld10Kwords.txt", header = FALSE, fill = FALSE)
words <- as.character(nld10K[,2])
tmp <- write.profile(words)
str(tmp)
words[grep("\t",words)][1:10]
str(nld10K)
nld10K <- read.delim("data/nld10Kwords.txt", header = FALSE, quote = "")
str(nld10K)
words <- as.character(nld10K[,2])
tmp <- write.profile(words)
str(tmp)
tmp
tmp <- write.profile(words, file = "sandbox/dutch_draft_profile.tsv")
words[grep("0",words)][1:10]
words[grep("Ã",words)][1:10]
words[grep("Ô",words)][1:10]
words[grep("ô",words)][1:10]
tmp <- tokenize(words, profile = "data/nld10K_corrected_profile.tsv")
str(tmp)
tmp$errors[1:10,]
tmp <- tokenize(words, profile = "data/nld10K_corrected_profile.tsv")
str(tmp)
tmp$errors
tmp$errors[1:100,]
tmp$errors[100:500,]
write.profile(words, file = "sandbox/nld10K_draft_profile.tsv", editing =  TRUE)
words[grep("î",words)][1:10]
words[grep("û",words)][1:10]
tokens <- tokenize(words, profile = "data/nld10K_corrected_profile.tsv", transliterate = "Replacement")
tokens <- tokenize(words, profile = "data/nld10K_corrected_profile.tsv", transliterate = "Replacement", sep = "")
str(tokens)
errors <- as.numeric(rownames(tokens$errors))
new_words <- tokens$strings[-errors,]$transliterated
str(new_words)
new_words[1:30]
tokens$strings[1:30,]
tokens <- tokenize(words, profile = "data/nld10K_corrected_profile.tsv", transliterate = "Replacement", sep = " ")
tokens$strings[1:30,]
nchar("")
nchar(" ")
substr("asdf",1,4)
substr("asdf",2,3)
library(qlcTokenize)
nld10K <- read.delim("data/nld10Kwords.txt", header = FALSE, quote = "")#
words <- as.character(nld10K[,2])
tokens <- tokenize(words, profile = "data/nld10K_corrected_profile.tsv", transliterate = "Replacement", sep = "")
tokens$strings[1:10,]
errors <- as.numeric(rownames(tokens$errors))
new_words <- tokens$strings[-errors,]$transliterated
str(new_words)
words[grep("í",words)]
words[grep("ì",words)]
words[grep("eì",words)]
words[grep("ó",words)]
dutch <- tokenize(new_words, profile = "data/dutch_IPA_profile.tsv", transliterate = "IPA")
str(dutch)
tokens$missing
dutch$missing
dutch$errors
dutch <- tokenize(new_words, profile = "data/dutch_IPA_profile.tsv", transliterate = "IPA", file = "tmp")
words[grep("â",words)]
tokens <- tokenize(words, profile = "data/nld10K_corrected_profile.tsv", transliterate = "Replacement", sep = "")#
#
# just remove all words that include some strange characters#
errors <- as.numeric(rownames(tokens$errors))#
new_words <- tokens$strings[-errors,]$transliterated#
#
dutch <- tokenize(new_words, profile = "data/dutch_IPA_profile.tsv", transliterate = "IPA", file = "tmp")
str(new_words)
words[grep("î",words)]
words[grep("ô",words)]
words[grep("Ö",words)]
words[grep("ì",words)]
tokens <- tokenize(words, profile = "data/nld10K_corrected_profile.tsv", transliterate = "Replacement", sep = "")#
#
# just remove all words that include some strange characters#
errors <- as.numeric(rownames(tokens$errors))#
new_words <- tokens$strings[-errors,]$transliterated#
#
dutch <- tokenize(new_words, profile = "data/dutch_IPA_profile.tsv", transliterate = "IPA", file = "tmp")
words[grep("È",words)]
tokens <- tokenize(words, profile = "data/nld10K_corrected_profile.tsv", transliterate = "Replacement", sep = "")#
#
# just remove all words that include some strange characters#
errors <- as.numeric(rownames(tokens$errors))#
new_words <- tokens$strings[-errors,]$transliterated#
#
dutch <- tokenize(new_words, profile = "data/dutch_IPA_profile.tsv", transliterate = "IPA", file = "tmp")
selection <- read.delim("data/dutch_IPA_profile.tsv", quote = "")
head(selection)
examples <- read.delim("data/dutch_IPA_profile.tsv", quote = "")$Examples
str(examples)
tmp <- strsplit(examples, ", ")
tmp <- strsplit(examples, ", ",fixed=T)
tmp <- strsplit(examples, split=", ",fixed=T)
?strsplit
tmp <- strsplit(as.character(examples), ", ")
str(tmp)
tmp <- unlist(strsplit(as.character(examples), ", "))
str(tmp)
tmp <- sort(unique(unlist(strsplit(as.character(examples), ", "))))
str(tmp)
tmp[1:50]
words <- c(examples, corpus_words)
which(tmp=="")
write(words, files = "sandbox/dutch_words.txt")
examples <- read.delim("data/dutch_IPA_profile.tsv", quote = "")$Examples#
examples <- sort(unique(unlist(strsplit(as.character(examples), ", "))))#
#
# get examples from corpus#
nld10K <- read.delim("data/nld10Kwords.txt", header = FALSE, quote = "")#
corpus_words <- as.character(nld10K[,2])#
#
# write draft profile#
write.profile(corpus_words, file = "sandbox/nld10K_draft_profile.tsv", editing =  TRUE)#
#
# remove all obviously non-dutch characters#
tokens <- tokenize(corpus_words, profile = "data/nld10K_corrected_profile.tsv", transliterate = "Replacement", sep = "")#
#
# just remove all words that include some strange characters#
errors <- as.numeric(rownames(tokens$errors))#
corpus_words <- tokens$strings[-errors,]$transliterated#
#
words <- c(examples, corpus_words)#
write(words, file = "sandbox/dutch_words.txt")
words <- sort(unique(c(examples, corpus_words)))#
write(words, file = "sandbox/dutch_words.txt")
str(words)
dutch <- tokenize(words, profile = "data/dutch_IPA_profile.tsv", transliterate = "IPA", file = "tmp")
dutch <- tokenize(words, profile = "data/dutch_IPA_profile.tsv", transliterate = "IPA", file = "tmp", regex = TRUE)
str(dutch)
grep("(?<=^|-)A","Aap")
grep("(?<=(^|-))A","Aap")
grep("(?<=^)A","Aap")
grep("(?<=^)A","Aap",perl=T)
grep("(?<=^|-)A","Aap",perl=T)
grep("(?<=^|-)A","-Aap",perl=T)
grep("(?<=^|-)A","rAap",perl=T)
dutch <- tokenize(words, profile = "data/dutch_IPA_profile.tsv", transliterate = "IPA", file = "tmp", regex = TRUE)
str(dutch)
which(words == "bínnen")
dutch$strings[3187,]
dutch$strings[which(words = "kúnnen"),]
dutch$strings[which(words == "kúnnen"),]
dutch$strings[which(words == "Beieren"),]
scan(file = "data/dutch_words.txt")#
#
dutch <- tokenize(words, profile = "data/dutch_IPA_profile.tsv", transliterate = "IPA", file = "tmp", regex = TRUE)
library(qlcTokenize)
scan(file = "data/dutch_words.txt", what = "character")
words <- scan(file = "data/dutch_words.txt", what = "character")
dutch <- tokenize(words, profile = "data/dutch_IPA_profile.tsv", transliterate = "IPA", file = "tmp", regex = TRUE)
which(words=="ingang")
dutch$strings[11484,]
dutch$strings[which(words=="Ubbergen"),]
dutch <- tokenize(words, profile = "data/dutch_IPA_profile.tsv", transliterate = "IPA", file = "tmp", regex = TRUE)
dutch$strings[which(words=="ongenzond"),]
dutch$strings[which(words=="ongedaan"),]
examples <- read.delim("data/dutch_IPA_profile.tsv", quote = "")$Examples#
examples <- unlist(strsplit(as.character(examples), ", "))
nld10K <- read.delim("data/nld10Kwords.txt", header = FALSE, quote = "")#
corpus_words <- as.character(nld10K[,2])#
#
# write draft profile#
write.profile(corpus_words, file = "sandbox/nld10K_draft_profile.tsv", editing =  TRUE)#
#
# remove all obviously non-dutch characters#
tokens <- tokenize(corpus_words, profile = "data/nld10K_corrected_profile.tsv", transliterate = "Replacement", sep = "")#
#
# just remove all words that include some strange characters#
errors <- as.numeric(rownames(tokens$errors))#
corpus_words <- tokens$strings[-errors,]$transliterated#
#
words <- sort(unique(c(examples, corpus_words)))#
write(words, file = "sandbox/dutch_words.txt")#
#
words <- scan(file = "data/dutch_words.txt", what = "character")
dutch <- tokenize(words, profile = "data/dutch_IPA_profile.tsv", transliterate = "IPA", file = "tmp", regex = TRUE)
examples <- read.delim("data/dutch_IPA_profile.tsv", quote = "")$Examples#
examples <- unlist(strsplit(as.character(examples), ", "))#
#
# get examples from corpus#
nld10K <- read.delim("data/nld10Kwords.txt", header = FALSE, quote = "")#
corpus_words <- as.character(nld10K[,2])#
#
# write draft profile#
write.profile(corpus_words, file = "sandbox/nld10K_draft_profile.tsv", editing =  TRUE)#
#
# remove all words with obviously non-dutch characters#
tokens <- tokenize(corpus_words, profile = "data/nld10K_corrected_profile.tsv", transliterate = "Replacement", sep = "")
str(tokens)
errors <- as.numeric(rownames(tokens$errors))#
corpus_words <- tokens$strings[-errors,]$transliterated
corpus_words[1:100]
words <- sort(unique(c(examples, corpus_words)))#
write(words, file = "sandbox/dutch_words.txt")#
#
words <- scan(file = "data/dutch_words.txt", what = "character")#
#
dutch <- tokenize(words, profile = "data/dutch_IPA_profile.tsv", transliterate = "IPA", file = "tmp", regex = TRUE)
nld10K <- read.delim("data/nld10Kwords.txt", header = FALSE, quote = "")#
corpus_words <- as.character(nld10K[,2])#
#
# write draft profile#
write.profile(corpus_words, file = "sandbox/nld10K_draft_profile.tsv", editing =  TRUE)#
#
# remove all words with obviously non-dutch characters#
tokens <- tokenize(corpus_words, profile = "data/nld10K_corrected_profile.tsv", transliterate = "Replacement", sep = "", regex = TRUE)#
#
# just remove all words that include some strange characters#
errors <- as.numeric(rownames(tokens$errors))#
corpus_words <- tokens$strings[-errors,]$transliterated
words <- sort(unique(c(examples, corpus_words)))
words[1:100]
write(words, file = "sandbox/dutch_words.txt")#
#
words <- scan(file = "data/dutch_words.txt", what = "character")
words <- scan(file = "data/dutch_words.txt", what = "character")
dutch <- tokenize(words, profile = "data/dutch_IPA_profile.tsv", transliterate = "IPA", file = "tmp", regex = TRUE)
words <- scan(file = "data/dutch_words.txt", what = "character")#
#
dutch <- tokenize(words, profile = "data/dutch_IPA_profile.tsv", transliterate = "IPA", file = "tmp", regex = TRUE)
library(qlcTokenize)
dutch <- tokenize(words, profile = "data/dutch_IPA_profile.tsv", transliterate = "IPA", file = "tmp", regex = TRUE)
# get examples from corpus#
nld10K <- read.delim("data/nld10Kwords.txt", header = FALSE, quote = "")#
corpus_words <- as.character(nld10K[,2])#
#
# write draft profile#
write.profile(corpus_words, file = "sandbox/nld10K_draft_profile.tsv", editing =  TRUE)#
#
# remove all words with obviously non-dutch characters#
tokens <- tokenize(corpus_words, profile = "data/nld10K_corrected_profile.tsv", transliterate = "Replacement", sep = "", regex = TRUE)
errors <- as.numeric(rownames(tokens$errors))
str(errors)
corpus_words <- tokens$strings[-errors,]$transliterated
words <- sort(unique(c(examples, corpus_words)))
examples <- read.delim("data/dutch_IPA_profile.tsv", quote = "")$Examples#
examples <- unlist(strsplit(as.character(examples), ", "))
words <- sort(unique(c(examples, corpus_words)))
str(words)
which(words=="aan-")
words[1:30]
nld10K <- read.delim("sources/nld_mixed_2012_10K-words.txt", header = FALSE, quote = "")
corpus_words <- as.character(nld10K[,2])
write.profile(corpus_words, file = "sandbox/nld10K_draft_profile.tsv", editing =  TRUE)
tokens <- tokenize(corpus_words, profile = "data/nld10K_corrected_profile.tsv", transliterate = "Replacement", sep = "", regex = TRUE)
tokens$errors[1:30,]
errors <- as.numeric(rownames(tokens$errors))#
corpus_words <- tokens$strings[-errors,]$transliterated
words <- sort(unique(c(examples, corpus_words)))
words[1:30]
library(qlcTokenize)#
#
# get examples from profile#
examples <- read.delim("data/dutch_IPA_profile.tsv", quote = "")$Examples#
examples <- unlist(strsplit(as.character(examples), ", "))#
#
# get examples from corpus#
nld10K <- read.delim("sources/nld_mixed_2012_10K-words.txt", header = FALSE, quote = "")#
corpus_words <- as.character(nld10K[,2])#
#
# write draft profile#
write.profile(corpus_words, file = "sandbox/nld10K_draft_profile.tsv", editing =  TRUE)#
#
# remove all words with obviously non-dutch characters#
tokens <- tokenize(corpus_words, profile = "data/nld10K_corrected_profile.tsv", transliterate = "Replacement", sep = "", regex = TRUE)#
#
# just remove all words that include some strange characters#
errors <- as.numeric(rownames(tokens$errors))#
corpus_words <- tokens$strings[-errors,]$transliterated#
#
# and everything that starts or ends with a dash#
words <- sort(unique(c(examples, corpus_words)))
words[1:30]
# get examples from profile#
examples <- read.delim("data/dutch_IPA_profile.tsv", quote = "")$Examples#
examples <- unlist(strsplit(as.character(examples), ", "))#
#
# get examples from corpus#
nld10K <- read.delim("sources/nld_mixed_2012_10K-words.txt", header = FALSE, quote = "")#
corpus_words <- as.character(nld10K[,2])#
#
# write draft profile#
write.profile(corpus_words, file = "sandbox/nld10K_draft_profile.tsv", editing =  TRUE)#
#
# remove all words with obviously non-dutch characters#
tokens <- tokenize(corpus_words, profile = "data/nld10K_corrected_profile.tsv", transliterate = "Replacement", sep = "", regex = TRUE)#
#
# just remove all words that include some strange characters#
errors <- as.numeric(rownames(tokens$errors))#
corpus_words <- tokens$strings[-errors,]$transliterated#
#
# and everything that starts or ends with a dash#
words <- sort(unique(c(examples, corpus_words)))
library(qlcTokenize)#
#
# get examples from profile#
examples <- read.delim("data/dutch_IPA_profile.tsv", quote = "")$Examples#
examples <- unlist(strsplit(as.character(examples), ", "))#
#
# get examples from corpus#
nld10K <- read.delim("sources/nld_mixed_2012_10K-words.txt", header = FALSE, quote = "")#
corpus_words <- as.character(nld10K[,2])#
#
# write draft profile#
write.profile(corpus_words, file = "sandbox/nld10K_draft_profile.tsv", editing =  TRUE)#
#
# remove all words with obviously non-dutch characters#
tokens <- tokenize(corpus_words, profile = "data/nld10K_corrected_profile.tsv", transliterate = "Replacement", sep = "", regex = TRUE)#
#
# just remove all words that include some strange characters#
errors <- as.numeric(rownames(tokens$errors))#
corpus_words <- tokens$strings[-errors,]$transliterated#
#
# and everything that starts or ends with a dash#
words <- sort(unique(c(examples, corpus_words)))
words[1:30]
write(words, file = "sandbox/dutch_words.txt")
dutch <- tokenize(words, profile = "data/dutch_IPA_profile.tsv", transliterate = "IPA", file = "tmp", regex = TRUE)
library(qlcTokenize)#
#
# get examples from profile#
examples <- read.delim("data/dutch_IPA_profile.tsv", quote = "")$Examples#
examples <- unlist(strsplit(as.character(examples), ", "))#
#
# get examples from corpus#
nld10K <- read.delim("sources/nld_mixed_2012_10K-words.txt", header = FALSE, quote = "")#
corpus_words <- as.character(nld10K[,2])#
#
# write draft profile#
write.profile(corpus_words, file = "sandbox/nld10K_draft_profile.tsv", editing =  TRUE)#
#
# remove all words with obviously non-dutch characters#
tokens <- tokenize(corpus_words, profile = "data/nld10K_corrected_profile.tsv", transliterate = "Replacement", sep = "", regex = TRUE)#
#
# just remove all words that include some strange characters#
errors <- as.numeric(rownames(tokens$errors))#
corpus_words <- tokens$strings[-errors,]$transliterated#
#
# and everything that starts or ends with a dash#
words <- sort(unique(c(examples, corpus_words)))#
write(words, file = "sandbox/dutch_words.txt")
words[1:30]
dutch <- tokenize(words, profile = "data/dutch_IPA_profile.tsv", transliterate = "IPA", file = "tmp", regex = TRUE)
library(qlcTokenize)#
#
# get examples from profile#
examples <- read.delim("data/dutch_IPA_profile.tsv", quote = "")$Examples#
examples <- unlist(strsplit(as.character(examples), ", "))#
#
# get examples from corpus#
nld10K <- read.delim("sources/nld_mixed_2012_10K-words.txt", header = FALSE, quote = "")#
corpus_words <- as.character(nld10K[,2])#
#
# write draft profile#
write.profile(corpus_words, file = "sandbox/nld10K_draft_profile.tsv", editing =  TRUE)#
#
# remove all words with obviously non-dutch characters#
tokens <- tokenize(corpus_words, profile = "data/nld10K_corrected_profile.tsv", transliterate = "Replacement", sep = "", regex = TRUE)#
#
# just remove all words that include some strange characters#
errors <- as.numeric(rownames(tokens$errors))#
corpus_words <- tokens$strings[-errors,]$transliterated#
#
# and everything that starts or ends with a dash#
words <- sort(unique(c(examples, corpus_words)))#
write(words, file = "sandbox/dutch_words.txt")
dutch <- tokenize(words, profile = "data/dutch_IPA_profile.tsv", transliterate = "IPA", file = "tmp", regex = TRUE)
