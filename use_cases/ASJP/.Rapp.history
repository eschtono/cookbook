tmp <- read.csv("data/ASJP.csv")
str(tmp)
tmp <- read.csv("data/ASJP.csv", header = FALSE)
str(tmp)
words <- asjp[,5]
asjp <- read.csv("data/ASJP.csv", header = FALSE)
words <- asjp[,5]
library(qlcTokenize)
write.profile(words, file = "sandbox/asjp_draft_profile.csv")
write.profile(words)
?save.file
write.profile
?write.table
write.profile(words)->tmp
write.table(tmp,qmethod="double")
write.table(tmp,qmethod="double",file="tmp.txt")
write.table(tmp,qmethod="double",file="tmp.txt",quote=F)
tokenize(words[1:10], profile = "sandbox/asjp_draft_profile.csv")
asjp <- read.csv("data/ASJP.csv")
str(asjp)
words <- asjp$Value
write.profile(words, file = "sandbox/asjp_draft_profile.csv")
write.profile(words)
write.profile(words)->tmp
str(tmp)
?cat
cat("\t")
cat("\t",file="test.txt")
print("\t")
as.character("\t")
tmp[1,]
write.table(as.character(tmp[,1]),file="test.txt")
write.table(print(tmp[,1]),file="test.txt")
tmp[,1]=="\t"
tmp[1,1] <- "\\t"
write.table(tmp[,1],file="test.txt")
write.profile(words)->tmp
gsub("\\t","bla",tmp[,1])
write.profile(words)->tmp
gsub("\\([tnr])","\\$1",tmp[,1])
gsub("\\([tnr])","\\\$1",tmp[,1])
gsub("\\([tnr])","\\\\$1",tmp[,1])
gsub("\t","\\\\$1",tmp[,1])
gsub("\([tnr])","\\$1",tmp[,1])
gsub("\([tnr])","\$1",tmp[,1])
gsub("\([tnr])","\\\$1",tmp[,1])
gsub("\([tnr])","\\t",tmp[,1])
gsub("\\([tnr])","\\t",tmp[,1])
gsub("\\([tnr])","\\\t",tmp[,1])
gsub("\\([tnr])","\\\\t",tmp[,1])
gsub("\t","\\t",tmp[,1],fixed = T)
#' ---#
#' title: "Analysis of ASJP data"#
#' author: "Michael Cysouw"#
#' date: "30 September 2015"#
#' ---#
#
# install library from github#
# only necessary once#
# install.packages("devtools")#
# devtools::install_github("cysouw/qlcTokenize")#
library(qlcTokenize)#
#
# read ASJP data#
# this is pretty large, so it might take a bit#
asjp <- read.csv("data/ASJP.csv")#
words <- asjp$Value#
#
# first make draft profile using default settings#
write.profile(words, file = "sandbox/asjp_draft_profile.csv")
tokenize(words[1:10],profile="sandbox/asjp_draft_profile.csv",regex=T)
tokenize(words[1:10],profile="sandbox/asjp_draft_profile.csv")
tokenize("absd\tfdas",profile="sandbox/asjp_draft_profile.csv")
tokenize(words[1:10],profile="sandbox/asjp_draft_profile.csv",regex=T)
tokenize("absd\tfdas",profile="sandbox/asjp_draft_profile.csv",regex=T)
tokenize("asdf")
tokenize("asd\tf")
tokenize("asd\tf",file="test")
read.table("test_strings.tsv",sep="\t")
tokenize("absd\tfdas",profile="sandbox/asjp_draft_profile.csv",regex=T)->tmp
tmp$profile$Grapheme
tmp$profile$Grapheme=="\t"
#' ---#
#' title: "Analysis of ASJP data"#
#' author: "Michael Cysouw"#
#' date: "30 September 2015"#
#' ---#
#
# install library from github#
# only necessary once#
# install.packages("devtools")#
# devtools::install_github("cysouw/qlcTokenize")#
library(qlcTokenize)#
#
# read ASJP data#
# this is pretty large, so it might take a bit#
asjp <- read.csv("data/ASJP.csv")#
words <- asjp$Value#
#
# first make draft profile using default settings#
write.profile(words, file = "sandbox/asjp_draft_profile.csv")#
#
tokenize("absd\tfdas",profile="sandbox/asjp_draft_profile.csv",regex=T)
tokenize("absd\tfdas",profile="sandbox/asjp_draft_profile.csv",regex=T)
tokenize("absd\tfdas",profile="sandbox/asjp_draft_profile.csv",regex=F)
tokenize("absda\tfdas",profile="sandbox/asjp_draft_profile.csv",regex=T)
tokenize("absda\tfdas",profile="sandbox/asjp_draft_profile.csv",regex=T,file="test")
tokenize("absd\tfdas",profile="sandbox/asjp_draft_profile.csv",regex=F,file="test")
tokenize("absd\tfdas",profile="sandbox/asjp_draft_profile.csv",regex=F)->tmp
tmp
read.table("test_strings.tsv")
read.table("test_strings.tsv",sep="\t")
str(read.table("test_strings.tsv",sep="\t"))
read.table("test_strings.tsv",sep="\t")[1,1]
read.table("test_strings.tsv",sep="\t")[2,1]
gsub("\t","XXX",read.table("test_strings.tsv",sep="\t")[2,1]
read.table("test_strings.tsv",sep="\t")[2,1]->x
gsub("\t","XXX",x)
x
grep("\t",x)
grep("\t",c(x,x,x))
?grep
c(x,x,x)
x
x <- as.character(x)
grep("\t",c(x,x,x))
grep("\t",c("bla","bla","bla"))
lgrep("\t",c("bla","bla","bla"))
grepl("\t",c("bla","bla","bla"))
#' ---#
#' title: "Analysis of ASJP data"#
#' author: "Michael Cysouw"#
#' date: "30 September 2015"#
#' ---#
#
# install library from github#
# only necessary once#
# install.packages("devtools")#
# devtools::install_github("cysouw/qlcTokenize")#
library(qlcTokenize)#
#
# read ASJP data#
# this is pretty large, so it might take a bit#
asjp <- read.csv("data/ASJP.csv")#
words <- asjp$Value#
#
# first make draft profile using default settings#
write.profile(words, file = "sandbox/asjp_draft_profile.csv")
system.time(#
	tokens <- tokenize(asjp$Value#
						, profile = "profiles/asjp_corrected_profile.csv"#
						, regex = T#
						, file.out = "sandbox/asjp_tokenized")#
	)
tokens <- tokenize(asjp$Value#
					, profile = "profiles/asjp_corrected_profile.csv"#
					, regex = T#
					)
tokens$errors
tokenize("asdf\tasdf")
tokenize("asdf\tasdf",file="tmp")
#' ---#
#' title: "Analysis of ASJP data"#
#' author: "Michael Cysouw"#
#' date: "30 September 2015"#
#' ---#
#
# install library from github#
# only necessary once#
# install.packages("devtools")#
# devtools::install_github("cysouw/qlcTokenize")#
library(qlcTokenize)#
#
# read ASJP data#
# this is pretty large, so it might take a bit#
asjp <- read.csv("data/ASJP.csv")#
#
# the actual words are in the fifth column of this csv-file ("Value")#
# first make draft profile using default settings#
write.profile(asjp$Value, file = "sandbox/asjp_draft_profile.csv")#
#
# There is a warning because there are tabs in the input data#
# This is an error in the ASJP data, but we leave it in for now#
# It has to be changed in the orthography profile though!#
# Check the manually changed profile in profiles/asjp_corrected_profile.csv#
# after manually correcting the profile we can use it for tokenization#
# there are regular expressions in the corrected profile, so "regex = TRUE"#
# the code is not very fast, might take about half a minute for all of ASJP#
tokens <- tokenize(asjp$Value#
					, profile = "profiles/asjp_corrected_profile.csv"#
					, regex = T#
					)
str(tokens$strings)
str(asjp)
head(tokens$strings)
tail(tokens$strings)
head(tokens$strings,n=20)
tail(tokens$strings,n=20)
tokens <- tokenize(asjp$Value[1:100]#
					, profile = "profiles/asjp_corrected_profile.csv"#
					, regex = TRUE#
					)
str(tokens)
tokens <- tokenize(asjp$Value[1:100000]#
					, profile = "profiles/asjp_corrected_profile.csv"#
					, regex = TRUE#
					)
tokens <- tokenize(asjp$Value[1:50000]#
					, profile = "profiles/asjp_corrected_profile.csv"#
					, regex = TRUE#
					)
tokens <- tokenize(asjp$Value[50000:80000]#
					, profile = "profiles/asjp_corrected_profile.csv"#
					, regex = TRUE#
					)
tokens <- tokenize(asjp$Value[80000:100000]#
					, profile = "profiles/asjp_corrected_profile.csv"#
					, regex = TRUE#
					)
tokens <- tokenize(asjp$Value[90000:100000]#
					, profile = "profiles/asjp_corrected_profile.csv"#
					, regex = TRUE#
					)
tokens <- tokenize(asjp$Value[80000:85000]#
					, profile = "profiles/asjp_corrected_profile.csv"#
					, regex = TRUE#
					)
tokens <- tokenize(asjp$Value[85000:87000]#
					, profile = "profiles/asjp_corrected_profile.csv"#
					, regex = TRUE#
					)
tokens <- tokenize(asjp$Value[87000:88000]#
					, profile = "profiles/asjp_corrected_profile.csv"#
					, regex = TRUE#
					)
tokens <- tokenize(asjp$Value[87000:89000]#
					, profile = "profiles/asjp_corrected_profile.csv"#
					, regex = TRUE#
					)
tokens <- tokenize(asjp$Value[89000:90000]#
					, profile = "profiles/asjp_corrected_profile.csv"#
					, regex = TRUE#
					)
tokens <- tokenize(asjp$Value[90000:95000]#
					, profile = "profiles/asjp_corrected_profile.csv"#
					, regex = TRUE#
					)
tokens <- tokenize(asjp$Value[90000:93000]#
					, profile = "profiles/asjp_corrected_profile.csv"#
					, regex = TRUE#
					)
tokens <- tokenize(asjp$Value[93000:94000]#
					, profile = "profiles/asjp_corrected_profile.csv"#
					, regex = TRUE#
					)
tokens <- tokenize(asjp$Value[94000:95000]#
					, profile = "profiles/asjp_corrected_profile.csv"#
					, regex = TRUE#
					)
tokens <- tokenize(asjp$Value[94000:94500]#
					, profile = "profiles/asjp_corrected_profile.csv"#
					, regex = TRUE#
					)
tokens <- tokenize(asjp$Value[94500:94700]#
					, profile = "profiles/asjp_corrected_profile.csv"#
					, regex = TRUE#
					)
tokens <- tokenize(asjp$Value[94500:94900]#
					, profile = "profiles/asjp_corrected_profile.csv"#
					, regex = TRUE#
					)
tokens <- tokenize(asjp$Value[94700:94800]#
					, profile = "profiles/asjp_corrected_profile.csv"#
					, regex = TRUE#
					)
tokens <- tokenize(asjp$Value[94700:94750]#
					, profile = "profiles/asjp_corrected_profile.csv"#
					, regex = TRUE#
					)
tokens <- tokenize(asjp$Value[94750:94800]#
					, profile = "profiles/asjp_corrected_profile.csv"#
					, regex = TRUE#
					)
asjp$Value[94750:94800]
tokens$strings
asjp$Value[94786]
asjp$Value[94785]
utf8ToInt("ôŽ¿°")
tokens <- tokenize(asjp$Value[94750:94800]#
					, profile = "profiles/asjp_corrected_profile.csv"#
					, regex = TRUE#
					,method = "linear")
tokens$strings
straplit("sdfgababasdfg","aba")
strsplit("sdfgababasdfg","aba")
strsplit("sdfgababasdfg","b")
strsplit("asdfgababasdfga","b")[[1]]->tmp
tmp
tail("")
substr("",3,4)
?substr
substr(c("asdf","asdf","asdf"),1,1)
tmp <- c("asdf","asdf","asdf")
substr(tmp,2,nchar(tmp)-1)
tmp <- c("asdf","asdf","asdf","a")
substr(tmp,2,nchar(tmp)-1)
#' ---#
#' title: "Analysis of ASJP data"#
#' author: "Michael Cysouw"#
#' date: "30 September 2015"#
#' ---#
#
# install library from github#
# only necessary once#
# install.packages("devtools")#
# devtools::install_github("cysouw/qlcTokenize")#
library(qlcTokenize)#
#
# read ASJP data#
# this is pretty large, so it might take a bit#
asjp <- read.csv("data/ASJP.csv")#
#
# the actual words are in the fifth column of this csv-file ("Value")#
# first make draft profile using default settings#
write.profile(asjp$Value, file = "sandbox/asjp_draft_profile.csv")#
#
# There is a warning because there are tabs in the input data#
# This is an error in the ASJP data, but we leave it in for now#
# It has to be changed in the orthography profile though!#
# Check the manually changed profile in profiles/asjp_corrected_profile.csv#
# after manually correcting the profile we can use it for tokenization#
# there are regular expressions in the corrected profile, so "regex = TRUE"#
# the code is not very fast, might take about half a minute for all of ASJP#
tokens <- tokenize(asjp$Value[1:100]#
					, profile = "profiles/asjp_corrected_profile.csv"#
					, regex = TRUE#
					)
tokens$strings
strsplit("asdfasdf",split="a")
#' ---#
#' title: "Analysis of ASJP data"#
#' author: "Michael Cysouw"#
#' date: "30 September 2015"#
#' ---#
#
# install library from github#
# only necessary once#
# install.packages("devtools")#
# devtools::install_github("cysouw/qlcTokenize")#
library(qlcTokenize)#
#
# read ASJP data#
# this is pretty large, so it might take a bit#
asjp <- read.csv("data/ASJP.csv")#
#
# the actual words are in the fifth column of this csv-file ("Value")#
# first make draft profile using default settings#
write.profile(asjp$Value, file = "sandbox/asjp_draft_profile.csv")#
#
# There is a warning because there are tabs in the input data#
# This is an error in the ASJP data, but we leave it in for now#
# It has to be changed in the orthography profile though!#
# Check the manually changed profile in profiles/asjp_corrected_profile.csv#
# after manually correcting the profile we can use it for tokenization#
# there are regular expressions in the corrected profile, so "regex = TRUE"#
# the code is not very fast, might take about half a minute for all of ASJP#
tokens <- tokenize(asjp$Value[1:100]#
					, profile = "profiles/asjp_corrected_profile.csv"#
					, regex = TRUE#
					)
tokens <- tokenize(asjp$Value[94750:94800]#
					, profile = "profiles/asjp_corrected_profile.csv"#
					, regex = TRUE#
					)
tokens$strings
tokens <- tokenize(asjp$Value#
					, profile = "profiles/asjp_corrected_profile.csv"#
					, regex = TRUE#
					)
str(tokens)
tokens$errors
tokens$missing
which(tokens$errors)
error_lines <- as.numeric(rownames(tokens$errors))#
asjp[error_lines,]#
#
# remove errors for now (this has to be done in the original data of course!)#
# make "empirical" profile of graphemes, showing all tailored graphemes in ASJP#
# there are #
no_errors <- tokens$strings$tokenized[-error_lines]#
write.profile(no_errors, sep = " ", file = "sandbox/asjp_emprical_profile.csv")
tmp <- write.profile(no_errors, sep = " ", file = "sandbox/asjp_emprical_profile.csv")
str(tmp)
tmp
rmarkdown::render("manual.R")
tokens$errors
write.profile(no.errors, sep = " ")
no_errors <- tokens$strings$tokenized[-error_lines]
write.profile(no.errors, sep = " ")
write.profile(no_errors, sep = " ")
rmarkdown::render("manual.R")
paste(c(3,4,5),collapse = "#")
paste(c(3,4,5,4,6),collapse = "#")
paste(c(3,4,5,4,6),collapse = "#")->tmp
strsplit(tmp,4)
paste(c(4,3,4,5,4,6,4),collapse = "#")->tmp
strsplit(tmp,4)
rmarkdown::render("manual.R")
rmarkdown::render("manual.R")
?invisible
f <- function(x){x}
f(4)
f <- function(x){invisible(x)}
f(4)
f(4)->tmp
tmp
# install library from github#
# only necessary once#
# install.packages("devtools")#
# devtools::install_github("cysouw/qlcTokenize")#
library(qlcTokenize)#
#
# read ASJP data#
# this is pretty large, so it might take a bit#
asjp <- read.csv("data/ASJP.csv")#
#
# the actual words are in the fifth column of this csv-file ("Value")#
# first make draft profile using default settings#
write.profile(asjp$Value, file = "sandbox/asjp_draft_profile.tsv")#
#
# There is a warning because there are tabs in the input data#
# This is an error in the ASJP data, but we leave it in for now#
# It has to be changed in the orthography profile though!#
# Check the manually changed profile in profiles/asjp_corrected_profile.csv#
#
# after manually correcting the profile we can use it for tokenization#
# there are regular expressions in the corrected profile, so "regex = TRUE"#
# save all the information from the tokenization in the sandbox#
# the code is not very fast, might take about half a minute for all of ASJP#
tokens <- tokenize(asjp$Value#
					, profile = "profiles/asjp_corrected_profile.tsv"#
					, regex = TRUE#
					, file.out = "sandbox/asjp"#
					)
tokens <- tokenize(asjp$Value#
					, profile = "profiles/asjp_corrected_profile.tsv"#
					, regex = TRUE#
					, file.out = "sandbox/asjp"#
					)
tokens$errors
asjp[1:10,]
asjp["5",]
tokens <- tokenize(asjp$Value#
					, profile = "profiles/asjp_corrected_profile.tsv"#
					, regex = TRUE#
					)
tokens$errors
asjp[rownames(tokens$errors),]
tokens$strings[1:10,]
tokens$strings[rownames(tokens$errors),]
tokens$strings[rownames(tokens$errors),]$tokenized
