library(qlcTokenize)
text <- scan("data/Brazilian_Portuguese_input.txt")
text <- scan("data/Brazilian_Portuguese_input.txt", what = "character")
text
text <- scan("data/Brazilian_Portuguese_input.txt", what = "character", sep = "\n")
text
tokens <- tokenize(text, profile = "profiles/validIPA2005_codepoints.tsv", sep = " ", sep.replace = "#")
tokens
tokens <- tokenize(text, profile = "profiles/validIPA2005_codepoints.tsv", sep = " ", sep.replace = "#", normalize = "NFD")
str(tokens)
output <- scan("data/Brazilian_Portuguese_output2.txt", what = "character", sep = "\n")
input <- scan("data/Brazilian_Portuguese_input.txt", what = "character", sep = "\n")#
output <- scan("data/Brazilian_Portuguese_output2.txt", what = "character", sep = "\n")#
#
out <- tokenize(input, profile = "profiles/validIPA2005_codepoints.tsv", sep = " ", sep.replace = "#", normalize = "NFD")
all.equal(output, out$strings$tokenized)
out <- tokenize(input, profile = "profiles/validIPA2005_codepoints.tsv", sep = " ", sep.replace = "#", normalize = "NFD", file = "tmp")
out <- tokenize(input, profile = "profiles/validIPA2005_codepoints.tsv", sep = " ", sep.replace = "#", normalize = "NFC", file = "tmp")
out <- tokenize(input, profile = "profiles/validIPA2005_codepoints.tsv", sep = " ", sep.replace = "#", normalize = "NFD", file = "tmp", regex = TRUE)
all.equal(output, out$strings$tokenized)
out <- tokenize(input, profile = "profiles/validIPA2005_codepoints.tsv", sep = " ", sep.replace = "#", normalize = "NFD", file = "tmp", regex = TRUE)
input <- scan("data/Brazilian_Portuguese_input.txt", what = "character", sep = "\n")#
output <- scan("data/Brazilian_Portuguese_output2.txt", what = "character", sep = "\n")#
#
out <- tokenize(input, profile = "profiles/validIPA2005_codepoints.tsv", sep = " ", sep.replace = "#", normalize = "NFD", file = "tmp", regex = TRUE)
out <- tokenize(input, profile = "profiles/validIPA2005_codepoints.tsv", sep = " ", sep.replace = "#", normalize = "NFC", file = "tmp", regex = TRUE)
input
out <- tokenize(input, profile = "profiles/strictIPA2005_codepoints.tsv", sep = " ", sep.replace = "#", normalize = "NFC", file = "tmp", regex = TRUE)
out <- tokenize(input, profile = "profiles/strictIPA2005_graphemes.tsv", sep = " ", sep.replace = "#", file = "tmp", regex = TRUE)
out <- tokenize(input, profile = "profiles/validIPA2005_graphemes.tsv", sep = " ", sep.replace = "#", file = "tmp", regex = TRUE)
out <- tokenize(input, profile = "profiles/strictIPA2005_graphemes.tsv", sep = " ", sep.replace = "#", file = "tmp", regex = TRUE)
out <- tokenize(input, profile = "profiles/validIPA2005_graphemes.tsv", sep = " ", sep.replace = "#", file = "tmp", regex = TRUE)
out <- tokenize(input, profile = "profiles/strictIPA2005_graphemes.tsv", sep = " ", sep.replace = "#", file = "tmp", regex = TRUE)
out <- tokenize(input, profile = "profiles/validIPA2005_graphemes.tsv", sep = " ", sep.replace = "#", file = "tmp", regex = TRUE)
out <- tokenize(input, profile = "profiles/validIPA2005_graphemes.tsv", sep = " ", sep.replace = "#", file = "tmp", regex = TRUE, normalize = "NFC")
out <- tokenize(input, profile = "profiles/validIPA2005_graphemes.tsv", sep = " ", sep.replace = "#", file = "tmp", regex = TRUE)
tokenize("ẽⁿ", profile = "profiles/validIPA2005_graphemes.tsv", sep = " ", sep.replace = "#", file = "tmp", regex = TRUE)
tokenize("ẽⁿɐ̃ⁿ", profile = "profiles/validIPA2005_graphemes.tsv", sep = " ", sep.replace = "#", file = "tmp", regex = TRUE)
library(qlcTokenize)#
#
input <- scan("data/Brazilian_Portuguese_input.txt", what = "character", sep = "\n")#
output <- scan("data/Brazilian_Portuguese_output2.txt", what = "character", sep = "\n")#
#
out <- tokenize(input, profile = "profiles/validIPA2005_graphemes.tsv", sep = " ", sep.replace = "#", file = "tmp", regex = TRUE)
out <- tokenize(input, profile = "profiles/validIPA2005_graphemes.tsv", sep = " ", sep.replace = "#", file = "tmp", regex = TRUE)
out <- tokenize("ẽⁿaⁿ", profile = "profiles/validIPA2005_graphemes.tsv", sep = " ", sep.replace = "#", file = "tmp", regex = TRUE)
out <- tokenize("ẽⁿaⁿ", profile = "profiles/validIPA2005_graphemes.tsv", sep = " ", sep.replace = "#", file = "tmp", regex = TRUE, normalization = "NFC")
out <- tokenize("ẽⁿaⁿ", profile = "profiles/validIPA2005_graphemes.tsv", sep = " ", sep.replace = "#", file = "tmp", regex = TRUE, normalize = "NFC")
out <- tokenize("ẽⁿãⁿ", profile = "profiles/validIPA2005_graphemes.tsv", sep = " ", sep.replace = "#", file = "tmp", regex = TRUE, normalize = "NFC")
out <- tokenize("ẽⁿãⁿ", profile = "profiles/validIPA2005_graphemes.tsv", sep = " ", sep.replace = "#", file = "tmp", regex = TRUE, normalize = "NFD")
out <- tokenize(input, profile = "profiles/validIPA2005_graphemes.tsv", sep = " ", sep.replace = "#", file = "tmp", regex = TRUE, normalize = "NFD")
tokenize("asdf")
str(input)
input
length(input)
tokenize("asdf",profile="profiles/validIPA2005_graphemes.tsv")->tmp
str(tmp)
tokenize("asdf",profile="profiles/validIPA2005_graphemes.tsv",sep.replace="#")->tmp
str(tmp)
tokenize("asdf",profile="profiles/validIPA2005_graphemes.tsv",regex=T)->tmp
str(tmp)
tokenize("asdf",regex=T)
out <- tokenize(input, profile = "profiles/validIPA2005_graphemes.tsv", sep = " ", sep.replace = "#", file = "tmp", regex = TRUE, normalize = "NFD")
all.equal(output, out$strings$tokenized)
all.equal(output, out$strings$tokenized[1])
input <- scan("data/Brazilian_Portuguese_input.txt", what = "character", sep = "\n")#
output <- scan("data/Brazilian_Portuguese_output2.txt", what = "character", sep = "\n")#
#
out <- tokenize(input, profile = "profiles/validIPA2005_graphemes.tsv", sep = " ", sep.replace = "#", file = "tmp", regex = TRUE, normalize = "NFD")#
#
all.equal(output, out$strings$tokenized)
compare <- function(jipa) {#
	text_in <- paste0(jipa, "_input.txt")#
	text_out <- paste0(jipa, "_output.txt")#
	input <- scan(text_in, what = "character", sep = "\n")#
	output <- scan(text_out, what = "character", sep = "\n")#
	tokenized <- tokenize(input#
						, profile = "profiles/validIPA2005_graphemes.tsv"#
						, sep = " "#
						, sep.replace = "#"#
						, file = "tmp"#
						, regex = TRUE#
						, normalize = "NFD"#
						)#
	all.equal(output, tokenized$strings$tokenized)#
}#
#
compare("Brazilian_Poruguese")
compare <- function(jipa) {#
	text_in <- paste0("data/", jipa, "_input.txt")#
	text_out <- paste0("data/", jipa, "_output.txt")#
	input <- scan(text_in, what = "character", sep = "\n")#
	output <- scan(text_out, what = "character", sep = "\n")#
	tokenized <- tokenize(input#
						, profile = "profiles/validIPA2005_graphemes.tsv"#
						, sep = " "#
						, sep.replace = "#"#
						, file = "tmp"#
						, regex = TRUE#
						, normalize = "NFD"#
						)#
	all.equal(output, tokenized$strings$tokenized)#
}#
#
compare("Brazilian_Poruguese")
compare("Brazilian_Portuguese")
compare <- function(jipa) {#
	text_in <- paste0("data/", jipa, "_input.txt")#
	text_out <- paste0("data/", jipa, "_output.txt")#
	input <- scan(text_in, what = "character", sep = "\n")#
	output <- scan(text_out, what = "character", sep = "\n")#
	tokenized <- tokenize(input#
						, profile = "profiles/validIPA2005_graphemes.tsv"#
						, sep = " "#
						, sep.replace = "#"#
						, file = "tmp"#
						, regex = TRUE#
						, normalize = "NFD"#
						)#
#	all.equal(output, tokenized$strings$tokenized)#
#
		tokenized$strings$tokenized#
}#
#
compare("Brazilian_Portuguese")
compare <- function(jipa) {#
	text_in <- paste0("data/", jipa, "_input.txt")#
	text_out <- paste0("data/", jipa, "_output2.txt")#
	input <- scan(text_in, what = "character", sep = "\n")#
	output <- scan(text_out, what = "character", sep = "\n")#
	tokenized <- tokenize(input#
						, profile = "profiles/validIPA2005_graphemes.tsv"#
						, sep = " "#
						, sep.replace = "#"#
						, file = "tmp"#
						, regex = TRUE#
						, normalize = "NFD"#
						)#
	all.equal(output, tokenized$strings$tokenized)#
#
}#
#
compare("Brazilian_Portuguese")
compare <- function(jipa) {#
	text_in <- paste0("data/", jipa, "_input.txt")#
	text_out <- paste0("data/", jipa, "_output.txt")#
	input <- scan(text_in, what = "character", sep = "\n")#
	output <- scan(text_out, what = "character", sep = "\n")#
	tokenized <- tokenize(input#
						, profile = "profiles/validIPA2005_graphemes.tsv"#
						, sep = " "#
						, sep.replace = "#"#
						, file = "tmp"#
						, regex = TRUE#
						, normalize = "NFD"#
						)#
	all.equal(output, tokenized$strings$tokenized)#
#
}#
#
compare("Brazilian_Portuguese")
compare("Kabiye")
compare("Vietnamese")
compare("Zurich_German")
