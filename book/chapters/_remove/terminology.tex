% \chapter{Terminology}
% \label{terminology}

\ 

\noindent In the next section we provide a very brief overview of the linguistic terminology
concerning writing systems before turning to the slightly different
computational terminology in the next section on the Unicode Standard. 

\section{Linguistic Terminology}
\label{linguistic-terminology}

Linguistically speaking, a \textsc{writing system} is a symbolic system that
uses visible or tactile signs to represent language in a systematic way. The
term writing system has two mutually exclusive meanings. First, it may
refer to the way a particular language is written. In this sense the term refers
to the writing system of a particular language, as, for example, in \emph{`the
Serbian writing system uses two scripts: Latin and Cyrillic.'} Second, the term
writing system may also refer to a type of symbolic system as used among the
world's languages to represent the language, as, for example, in
\emph{`alphabetic writing system.'} In this latter sense the term refers to how
scripts have been classified according to the way that they encode language, as
in, for example, \emph{`the Latin and Cyrillic scripts are both alphabetic
writing systems.'} To avoid confusion, this second notion of writing system
would more aptly have been called \textsc{script system}. 

\subsubsection*{Writing systems}

Focussing on the first sense, we distinguish two different kinds of 
writing systems used for a particular language, namely transcriptions and
orthographies. First, \textsc{transcription} is a scientific procedure (and also
the result of that procedure) for graphically representing the sounds of human
speech at the phonetic level. It incorporates a set of unambiguous symbols to
represent speech sounds, including conventions that specify how these symbols
should be combined. A transcription system is a specific system of symbols and
rules used for transcription of the sounds of a spoken language variety. In
principle, a transcription system should be language-independent, in that it
should be applicable to all spoken human languages. The \textsc{International
Phonetic Alphabet} (IPA) is a commonly used transcription system that provides a
medium for transcribing languages at the phonetic level. However, there is a
long history of alternative kinds of transcription systems
\citep[see][]{Kemp2006} and today various alternatives are in widespread use
(e.g.~X-SAMPA and Cyrillic-based phonetic transcription systems). Many
users of IPA do not follow the standard to the letter, and many dialects based
on the IPA have emerged, e.g.~the Africanist and Americanist transcription
systems. Note that IPA symbols are also often used to represent language on a
phonemic level. However, it is important to realize that in this usage the IPA
symbols are not a transcription system, but rather an orthography (though with
strong links to the pronunciation). Further, a transcription system does not
need to be as highly detailed as the IPA.\ It can also be a system of broad sound
classes. Although such an approximative transcription is not normally used in
linguistics, it is widespread in technological approaches
\citetext{\citealp[Soundex and variants, e.g.~][391--392]{Knuth1973};
\citealp{postel1969,Beider2008}}, and it is sometimes fruitfully used in
automatic approaches to historical linguistics
\citep{Dolgopolsky1986,List2012esslli,Brown2013}.

Second, an \textsc{orthography} specifies the symbols, punctuations, and the
rules in which a specific language is written in a standardized way.
Orthographies are often based on a phonemic analysis, but they almost always
include idiosyncrasies because of historical developments (like sound changes or
loans) and because of the widely-followed principle of lexical integrity
(i.e.~the attempt to write the same lexical root in a consistent way, also when
synchronic phonemic rules change the pronunciation, as for example with final
devoicing in many Germanic languages). All orthographies are language-specific
(and often even resource-specific), although individual symbols or rules might
be shared between languages. A \textsc{practical orthography} is a strongly
phoneme-based writing system designed for practical use by speakers. The mapping
relation between phonemes and graphemes in practical orthographies is purposely
shallow, i.e.~there is mostly a systematic and faithful mapping from a phoneme
to a grapheme. Practical orthographies are intended to jumpstart written
materials development by correlating a writing system with the sound units of a
language \citep[cf.~][]{MeinhofJones1928}. Symbols from the IPA are often used
by linguists in the development of such practical orthographies for languages
without writing systems, though this usage of IPA symbols should not be confused
with transcription (as defined above). 

Further, a \textsc{transliteration} is a mapping between two different
orthographies. It is the process of ``recording the graphic symbols of one
writing system in terms of the corresponding graphic symbols of a second writing
system'' \citep[396]{Kemp2006}. In straightforward cases, such a transliteration
is simply a matter of replacing one symbol with another. However, there are
widespread complications, like one-to-many or many-to-many mappings, which are
not always easy, or even possible, to solve without listing all cases
individually \citep[cf.~][Ch.~2]{Moran2012}.

\subsubsection*{Script systems}

Different kinds of writing systems are classified into script
systems. A \textsc{script} is a collection of distinct symbols as
employed by one or more orthographies. For example, both Serbian and Russian are
written with subsets of the Cyrillic script. A single language, like Serbian or
Japanese, can also be written using orthographies based on different scripts.
Over the years linguists have classified script systems in a variety of ways,
with the tripartite classification of logographic, syllabic, and alphabetic
remaining the most popular, even though there are at least half a dozen
different types of script systems that can be distinguished
\citep{Daniels1990,Daniels1996}.

Breaking it down further, a script consists of \textsc{graphemes}, and graphemes
consist of \textsc{characters}. In the linguistic terminology of writing
systems, a \textsc{character} is a general term for any self-contained element
in a writing system.\footnote{There is a second interpretation of the term
\textsc{character}, i.e.~a conventional term for a unit in the Chinese writing
system \citep{Daniels1996}. This interpretation will not be further explored in
this paper.} Although in literate societies most people have a strong intuition
about what the characters are in their particular orthography or orthographies,
it turns out that the separation of an orthography into separate characters is
far from trivial. The widespread intuitive notion of a character is strongly
biased towards educational traditions, like the alphabet taught at schools, and
technological possibilities, like the available type pieces in a printer's job
case, the keys on a typewriter, or the symbols displayed in Microsoft Word's
symbol browser. In practice, characters often consist of multiple building
blocks, each of which could be considered a character in its own right. For
example, although a Chinese character may be considered to be a single basic
unanalyzable unit, at a more fine-grained level of analysis the internal
structure of Chinese characters is often comprised of smaller semantic and
phonetic units that should be considered characters \citep{Sproat2000}. In
alphabetic scripts, this problem is most forcefully exemplified by diacritics. 

A \textsc{diacritic} is a mark, or series of marks, that may be above, below, or
through other characters \citep{Gaultney2002}. Diacritics are sometimes used to
distinguish homophonous words, but they are more often used to indicate a
modified pronunciation \citep[xli]{DanielsBright1996}. The central question is
whether, for example, <e>, <è>, <a> and <à> should be considered four
characters, or different combinations of three characters. In general, multiple
characters together can form another character, and it is not always possible to
decide on principled grounds what should be the basic building blocks of an
orthography.

For that reason, it is better to analyze an orthography as a collection of
graphemes. A \textsc{grapheme} is the basic, minimally distinctive symbol of a
particular writing system, alike to the phoneme is an abstract representation of
a distinct sound in a specific language. The term \textsc{grapheme} was modeled
after the term \textsc{phoneme} and represents a contrastive graphical unit in a
writing system (see \citealp{Kohrt1986} for a historical overview of the term
grapheme). Most importantly, a single grapheme regularly consists of multiple
characters, like <th>, <ou> and <gh> in English (note that each character in
these graphemes is also a separate grapheme in English). Such complex graphemes
are often used to represent single phonemes. So, a combination of characters is
used to represent a single phoneme. Note that the opposite is also found in
writing systems, in cases in which a single character represents a combination
of two or more phonemes. For example, <x> in English orthography represents a
combination of the phonemes /k/ and /s/. 

Further, conditioned or free variants of a grapheme are called
\textsc{allographs}. For example, the distinctive forms of Greek sigma are
conditioned, with <σ> being used word-internally and <ς> being used at the end
of a word. In sum, there are many-to-many relationships between phonemes and
graphemes as they are expressed in the myriad of language- and resource-specific
orthographies.

\ 

\noindent This exposition of the linguistic terminology involved in describing writing
systems has been purposely brief. We have highlighted some of the linguistic
notions that are pertinent, yet sometimes confused with, the technological
definitions developed for the computational processing of the world's writing
systems, which we describe in the next section.

\section{The Unicode approach}
\label{the-unicode-approach}

The conceptualization and terminology of writing systems was rejuvenated by
the development of the Unicode Standard, with major input from Mark Davis,
co-founder and long-term president of the Unicode Consortium. For many years,
``exotic'' writing systems and phonetic transcription systems on personal
computers were constrained by the American Standard Code for Information
Interchange (ASCII) character encoding scheme (based on the Latin script), which
only allowed for a strongly limited number of different symbols to be encoded.
This implied that users could either use and adopt the (extended) Latin alphabet
or they could assign new symbols to the small number of code points in the ASCII
encoding scheme to be rendered by a specifically designed font
\citep{BirdSimons2003}. In this situation, it was necessary to specify the font
together with each document to ensure the rightful display of its content. To
alleviate this problem of assigning different symbols to the same code points,
in the late 80's and early 90's the Unicode Consortium set itself the ambitious
goal of developing a single universal character encoding to provide a unique
number, a code point, for every character in the world's writing systems.
Nowadays, the Unicode Standard is the default encoding of the technologies that
support the World Wide Web and for all modern operating systems, software and
programming languages.

\subsubsection*{The Unicode Standard}

The Unicode Standard represents a massive step forward because it aims to
eradicate the distinction between \textsc{universal} (ASCII) versus
\textsc{language/resource-particular} (Font) by adding as much as possible
language-specific information into the universal standard. However, there are
still language/resource-specific specifications necessary for the proper usage
of Unicode, as will be discussed below. Within the Unicode structure many of
these specifications can be captured by so-called \textsc{Locale Descriptions}
or simply locales, so we are moving to a new distinction of universal (Unicode
Standard) versus language-particular (Locale Description). The major gain is a
much larger compatibility on the universal level (because Unicode standardizes a
much larger portion of writing system diversity), and much better possibilities
for automated processing on the language-particular level (because Locale
Descriptions are computer readable specifications).

Each version of the Unicode Standard (\citealp{Unicode2014}, as of writing at
version 7) consists of a set of specifications and guidelines that include (i) a
core specification, (ii) code charts, (iii) standard annexes and (iv) a
character database.\footnote{All documents of the Unicode Standard are available
at \url{http://www.unicode.org/versions/latest/}. For a quick survey of the use
of terminology inside the Unicode Standard, their glossary is particularly
useful, available at \url{http://www.unicode.org/glossary/}. For a general
introduction to the principles of Unicode, Chapter 2 of the core specification,
called \textsc{general structure}, is particularly insightful. Different from
many other documents of the Unicode Standard, this general introduction is
relatively easy to read and illustrated with many interesting examples from
various orthography traditions all over the world.} The \textsc{core
specification} is a book directed toward human readers that describes the formal
standard for encoding multilingual text. The \textsc{code charts} provide a
humanly readable online reference to the character contents of the Unicode
Standard in the form of PDF files. The \textsc{Unicode Standard Annexes (UAX)}
are a set of technical standards that describe the implementation of the Unicode
Standard for software development, Web standards, and programming languages.
The \textsc{Unicode Character Database (UCD)} is a set of computer-readable text
files that describe the character properties, including a set of rich character
and writing system semantics, for each character in the Unicode Standard. In
this section, we introduce the basic Unicode concepts, but we will leave out
many details. Please consult the above mentioned full documentation for a more
detailed discussion. Further note that the Unicode Standard is exactly that,
namely a standard. It normatively describes notions and rules to be followed. In
the actual practice of applying this standard in a computational setting, a
specific implementation is necessary. The most widely used implementation of the
Unicode Standard is the \textsc{International Components for Unicode (ICU)},
which offers C/C++ and Java libraries trying to implement the Unicode
Standard.\footnote{\url{http://icu-project.org}}

\subsubsection*{Character encoding system}

The Unicode Standard is a \textsc{character encoding system} which
goal it is to support the interchange and processing of written characters and
text in a computational setting. Underlyingly, the character encoding is
represented by a range of numerical values called a \textsc{code space}, which
is used to encode a set of characters. A \textsc{code point} is a unique
non-negative integer within a code space (i.e.~within a certain numerical
range). In the Unicode Standard character encoding system, an \textsc{abstract
character}, for example the \textsc{latin small letter p}, is mapped to a
particular code point, in this case the decimal value 112, normally represented in
hexadecimal, which then looks in Unicode parlance as
\uni{0070}.\footnote{Hexadecimal (base-16) 0070 is equivalent to decimal
(base-10) 112, which can be calculated by considering that $(0\cdot16^3) +
(0\cdot16^2) + (7\cdot16^1) + (0\cdot16^0) = 7\cdot16 = 112$. Underlyingly,
computers will of course treat this code point binary (base-2) as 11100000, as
can be seen by calculating that $(1\cdot2^7) + (1\cdot2^6) + (1\cdot2^5) +
(0\cdot2^4) + (0\cdot2^3) + (0\cdot2^2) + (0\cdot2^1) + (0\cdot2^0) = 64 + 32 +
16 = 112$.} That encoded abstract character is rendered on a computer screen (or
printed page) as a \textsc{glyph}, e.g. <p>, depending on the \textsc{font} and
the context in which that character appears.

In Unicode Standard terminology, an (abstract) \textsc{character} is the basic
encoding unit. The term \textsc{character} can be quite confusing due to its
alternative definitions across different scientific disciplines and because in
general the word \textsc{character} means many different things to different
people. It is therefore often preferable to refer to Unicode characters simply
as \textsc{code points}, because there is a one-to-one mapping between Unicode
characters and their numeric representation. In the Unicode approach, a
character refers to the abstract meaning and/or general shape, rather than a
specific shape, though in code tables some form of visual representation is
essential for the reader's understanding. Unicode defines characters as
abstractions of orthographic symbols, and it does not define visualizations for
these characters (although it does presents examples). In contrast, a
\textsc{glyph} is a concrete graphical representation of a character as it
appears when rendered (or rasterized) and displayed on an electronic device or
on printed paper. For example, <g {\large \textit{g}} \textbf{g}
{\fontspec{ArialMT} {\small g} \textit{g} \textbf{g}}> are different glyphs of the
same character, i.e.~they may be rendered differently depending on the
typography being used, but they all share the same code point. From the
perspective of Unicode they are \textit{the same thing}. In this approach, a
\textsc{font} is then simply a collection of glyphs linked to code points.
Allography is not specified in Unicode (expect for a few exceptional cases, due
to legacy encoding issues), but can be specified in a font as a
\textsc{contextual variant} (a.k.a.~presentation form).

Each code point in the Unicode Standard is associated with a set of
\textsc{character properties} as defined by the Unicode character property
model.\footnote{The character property model is described in
\url{http://www.unicode.org/reports/tr23/}, but the actual properties are
described in \url{http://www.unicode.org/reports/tr44/}. A simplified overview
of the properties is available at
\url{http://userguide.icu-project.org/strings/properties}. The actual code
tables listing all properties for all Unicode code points are available at
\url{http://www.unicode.org/Public/UCD/latest/ucd/}.} Basically, those
properties are just a long list of values for each character. For example, code
point \uni{0047} has the following properties (among many others): 
\begin{itemize}
	\item Name: LATIN CAPITAL LETTER G 
	\item Alphabetic: YES 
	\item Uppercase: YES 
	\item Script: LATIN 
	\item Extender: NO 
	\item Simple\_Lowercase\_Mapping: 0067 
\end{itemize}

These properties contain the basic information of the Unicode Standard and they
are necessary to define the correct behavior and conformance required for
interoperability in and across different software implementations (as defined in
the Unicode Standard Annexes). The character properties assigned to each code
point is based on each character's behavior in the real-world writing
traditions. For example, the corresponding lowercase character to \uni{0047} is
\uni{0067} (though note that the relation between uppercase and lowercase is in
many situations much more complex than this, and Unicode has further
specifications for those cases). Another use of properties is to define the
script of a character.\footnote{The Unicode Glossary defines the term \textsc{script} as
a ``collection of letters and other written signs used to represent textual
information in one or more writing systems. For example, Russian is written with
a subset of the Cyrillic script; Ukrainian is written with a different subset.
The Japanese writing system uses several scripts.''} In practice, script is
simply defined for each character as the explicit \textsc{script} property in
the Unicode Character Database.

One frequently references property is the \textsc{block} property, which is
often used in software applications to impose some structure to the large number
of Unicode characters. Each character in Unicode belongs to a specific block.
These blocks are basically an organizational structure to alleviate the
administrative burden of keeping Unicode up-to-date. Blocks consist of
characters that in some way belong together, so that characters are easier to
find. Some blocks are connected with a specific script, like the Hebrew block or
the Gujarati block. However, blocks are predefined ranges of code points, and
often there will come a point after which the range is completely filled. Any
extra characters will have to be assigned somewhere else. There is, for example,
a block \textsc{Arabic}, which contains most Arabic symbols. However, there is
also a block \textsc{Arabic Supplement}, \textsc{Arabic Presentation Forms-A}
and \textsc{Arabic Presentation Form-B}. The situation with Latin symbols is
even more extra. In general, the names for block should be taken as a
definitional statement. For example, many IPA symbols are not located in the
aptly-names block \textsc{IPA extensions}, but in other blocks (see Section~\ref{ipa-meets-unicode}).

\subsubsection*{Grapheme clusters}

There are many cases in which a sequence of characters (i.e.~a sequence of more
than one code point) represents what a user perceives as an individual unit in a
particular orthographic writing system. For this reason the Unicode Standard
differentiates between \textsc{abstract character} and \textsc{user-perceived
character}. Sequences of multiple code points that correspond to a single
user-perceived characters are called grapheme clusters in Unicode parlance.
Grapheme clusters come in two flavors: (default) grapheme clusters and tailored
grapheme clusters.

The (default) \textsc{grapheme clusters} are locale-independent graphemes,
i.e.~they always apply when a particular combination of characters occurs
independent of the writing system in which they are used. These character
combinations are defined in the Unicode Standard as functioning as one
\textsc{text element}.\footnote{The Unicode Glossary defines text element as:
``A minimum unit of text in relation to a particular text process, in the
context of a given writing system. In general, the mapping between text elements
and code points is many-to-many.''} The simplest example of a grapheme cluster
is a base character followed by a letter modifier character. For example, the
sequence <n> + < ̃> (i.e.~\textsc{latin small letter n} at \uni{006E}, followed
by \textsc{combining tilde} at \uni{0303}) combines visually into <ñ>, a
user-perceived character in writing systems like that of Spanish. So, what the
user perceives as a single character actually involves a multi-code-point
sequence. Note that this specific sequence can also be represented with a single
so-called \textsc{precomposed} code point, \textsc{latin small letter n with
tilde} at \uni{00F1}, but this is not the case for all multi-code point
character sequences. The problem that there multiple encodings possible for the
same text element has been acknowledged early on in the Unicode Standard
(e.g.~for <ñ>, the sequence \uni{006E} \uni{0303} should in all situations be
treated identically to the precomposed \uni{00F1}), and a system of
\textsc{canonical equivalence} is available for such situations. Basically, the
Unicode Standard offers different kind of normalizations to either decompose all
precomposed characters (called \textsc{NFD}, \textsc{Normalization Form
canonical Decomposition}), or compose as much as possible combinations (called
\textsc{NFC}, \textsc{Normalization Form canonical Composition}). In current
practice of software development, NFC seems to be preferred in most situations
and is widely proposed as the preferred canonical form.

More difficult for text processing, because less standardized, is what the
Unicode Standard terms \textsc{tailored grapheme clusters}. Tailored grapheme
clusters are locale-dependent graphemes, i.e.~such combination of characters do
not function as text elements in all situations. For example, the sequence <c>~+~<h> for the Slovak digraph <ch> or the sequence <ky> in the Sisaala practical
orthography (pronounced as IPA /tʃ/,~\cite{Moran2006}). These grapheme clusters
are \textsc{tailored} in the sense that they must be specified on a
language-by-language or a writing system-by-writing system basis. The Unicode
Standard provides technological specifications for creating locale specific data
in so-called \textsc{Unicode Locale Descriptions}, i.e.~a set of specification
that defines a set of language-specific elements (e.g.~tailored grapheme
clusters, collation order, capitalization-equivalence), as well as other special
information, like how to format numbers, dates, or currencies. Locale
descriptions are saved in the \textsc{Common Locale Data Repository
(CLDR)},\footnote{\url{http://cldr.unicode.org/}} a repository of
language-specific definitions of writing system properties, each of which
describes specific usages of characters. Each locale can be encoded in a
document using the \textsc{Locale Data Markup Language (LDML)}. LDML is an XML
format and vocabulary for the exchange of structured locale data. Unicode Locale
Descriptions allow users to define language- or even resource-specific writing
systems or orthographies.\footnote{The Unicode Glossary defines \textsc{writing
system} only very loosely, as it is not a central concept in the Unicode
Standard. A writing system is, ``A set of rules for using one or more scripts to
write a particular language. Examples include the American English writing
system, the British English writing system, the French writing system, and the
Japanese writing system.''} However, there are various drawbacks of locale
descriptions for the daily practice of linguistic work in a multilingual setting
(see Section~\ref{use-cases}).