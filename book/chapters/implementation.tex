\chapter{Implementation}
\label{implementation}

To illustrate the practical application of orthography profiles as defined in
the previous Chapter \ref{orthography-profiles}, we have implemented two 
versions of the specifications presented there: one in Python and one
in R.

These two libraries have rather different implementation histories,
and we are not 100\% sure that they will in all situations give the same
results. We do however provide a test suite for each implementation. 
Users should refer to the tests cases and to the documentation in each release 
for specifics about each implementation. Note also that function names differ 
in the two implementations.

Note also that the performance with larger datasets differs, and the code is not
always as clean as we would like it to be. In sum, the two implementations
should be considered as `proof of concept' and not as the final word on the
practical application of the specifications above. In our own experience, the
current implementations are sufficiently fast and stable to be useful for
academic practice (e.g. checking data consistency, or analyzing and
transliterating small to medium sized data sets), but they should probably
better not be used for full-scale industry applications.

This chapter will introduce the implementations, and give practical step-by-step 
guidelines for installing them and using them. Various simple and sometimes 
somewhat abstract examples will be discussed to show the different options 
available, and to illustrate the intended usage of orthography profiles. 

This chapter is split into two sections, one for the Python implementation, the 
other for R. Each section has a number of subsections that ...

In addition to the material presented here to get users started, we maintain 
several case studies that use each OP implementation in action. These use cases 
are an the accompanying Github repository available with this book at: 
\url{https://github.com/unicode-cookbook/recipes/}.

% TODO: point to the case studies
% In the next chapter we will then discuss a few full-scale examples to illustrate the application and the usefulness of orthography profiles.


\section{Python}
\label{python-implementations}

The Python package is available both as a command line interface (CLI) 
and as an appplication programming interface (API).

\subsection*{Installation}

To install the Python package from the Python Package Index (PyPI) run:

\begin{lstlisting}[language=bash]
  $ pip install segments
\end{lstlisting}

\noindent on the command line. This will give you access to both 
the CLI and programmatic functionality in Python scripts, when 
you import the \texttt{segments} library.

You can also install the \texttt{segments} 
package with from the Github repository:

\begin{lstlisting}[language=bash]
  $ git clone https://github.com/bambooforest/segments.git
  $ cd segments
  $ python setup.py develop
\end{lstlisting}


\subsection*{Command line interface}

From the command line, access \texttt{segments} and its 
various arguments. For help, run:

\begin{lstlisting}[language=bash]
  $ segments -h
\end{lstlisting}

\noindent Here is an example of how to create and use an orthography 
profile for segmentation. Create a text file:

\begin{lstlisting}[language=bash]
  $ more text.txt
  aäaaöaaüaa
\end{lstlisting}

\noindent Now look at the profile:

\begin{lstlisting}[language=bash,texcl=true]
  $ cat text.txt | segments profile
  Grapheme        frequency       mapping
  a       7       a
  ä       1       ä
  ü       1       ü
  ö       1       ö
\end{lstlisting}

\noindent Write the profile to a file:

\begin{lstlisting}[language=bash]
  $ cat text.txt | segments profile > profile.prf
\end{lstlisting}

\noindent Edit the profile:

\begin{lstlisting}[language=bash]
  $ more profile.prf
  Grapheme        frequency       mapping
  aa      0       x
  a       7       a
  ä       1       ä
  ü       1       ü
  ö       1       ö
\end{lstlisting}

\noindent Now tokenize the text without profile:

\begin{lstlisting}[language=bash]
  $ cat text.txt | segments tokenize
  a ä a a ö a a ü a a	
\end{lstlisting}

\noindent And with profile:
\begin{lstlisting}[language=bash]
  $ cat text.txt | segments --profile=profile.prf tokenize
  a ä aa ö aa ü aa

  $ cat text.txt | segments --mapping=mapping --profile=profile.prf tokenize
  a ä x ö x ü x
\end{lstlisting}


\subsection*{Application programming interface}
The \texttt{segments} API can be accesses by importing 
the package into Python a script. Here is an example of 
how to import the libraries, create a tokenizer object, 
tokenize a string and create a profile.

\begin{lstlisting}[language=python]
>>> from __future__ import unicode_literals, print_function
>>> from segments import Profile, Tokenizer
>>> t = Tokenizer()
>>> t('abcd')
'a b c d'
>>> prf = Profile({'Grapheme': 'ab', 'mapping': 'x'}, \
{'Grapheme': 'cd', 'mapping': 'y'})
>>> print(prf)
mapping Grapheme
ab  x
cd  y
>>> t = Tokenizer(profile=prf)
>>> t('abcd')
'ab cd'
>>> t('abcd', column='mapping')
'x y'
\end{lstlisting}


\section{R}
\label{r-implementation}

% \subsection*{Installing the R implementation}
\subsection*{Installation}

The R implementation is available in the package \texttt{qlcData}, which is 
directly available from the central R repository CRAN (Comprehensive R Archive 
Network). The R software environment itself has to be downloaded from its 
website.\footnote{\url{https://www.r-project.org}} After starting the included 
R program, the \texttt{qlcData} package for dealing with orthography profiles can be 
simply installed as follows:

<<eval=FALSE>>=
# download and install the qlcData software
install.packages('qlcData') 
# load the software, so it can be used
library(qlcData) 
@

The version available through CRAN is the latest stable version.
To obtain the most recent bug-fixes and experimental additions, please use the
development version, which is available on
GitHub.\footnote{\url{http://github.com/cysouw/qlcData}} This development
version can be easily installed using the github-install helper software from the
\texttt{devtools} package.

<<eval=FALSE>>=
# download and install helper software
install.packages('devtools') 
# install the qlcData package from GitHub
devtools::install_github('cysouw/qlcData', build_vignettes = TRUE)
# load the software, so it can be used 
library(qlcData) 
@

Inside the \texttt{qlcData} package, there are two functions for
orthography processing, \texttt{write.profile} and \texttt{tokenize}. R includes
help files with illustrative examples, and also a so-called `vignette' with
explanations and examples.

<<eval=FALSE>>=
# view help files
help(write.profile)
help(tokenize)
# view vignette with explanation and examples
vignette('orthography_processing')
@

Basically, the idea is to use \texttt{write.profile} to produce a
basic orthography profile from some data and then \texttt{tokenize} to apply the
(possibly edited) profile on some data, as exemplified in the next section. This
can of course be performed though R, but additionally there are two more
interfaces to the R code supplied in the \texttt{qlcData} package: (i) bash
executables and (ii) `shiny' webapps.

The bash executables are little files providing an interface to the R code that
can be used in a shell on a UNIX-alike machine. The location of these
executables is a bit hidden away by the install procedure of R packages. The
location can be found by the following command in R. These executables can be 
used from here, or they can be linked and/or copied to any location as wanted.

<<eval=FALSE>>=
# show the path to the bash executables
file.path(find.package('qlcData'), 'exec')
@

For example, a good way to use the executables in a terminal is to
make softlinks (using \texttt{ln}) from the executables to a directory in your
PATH, e.g. to \texttt{/usr/local/bin/}. The two executables are named
\texttt{tokenize} and \texttt{writeprofile}, and the links can be made directly 
by using Rscript to get the paths to the executables within the terminal.

<<eval=FALSE, tidy=FALSE, size='scriptsize', engine='bash'>>=
# get the paths to the R executables in bash
pathT=`Rscript -e 'cat(file.path(find.package("qlcData"), "exec", "tokenize"))'`
pathW=`Rscript -e 'cat(file.path(find.package("qlcData"), "exec", "writeprofile"))'`

# make softlinks to the R executables in /usr/local/bin
ln -is $pathT $pathW /usr/local/bin
@

After inserting this softlink it should be possible to access the
\texttt{tokenize} function from the shell. Try \texttt{tokenize --help} to test
the functionality.

<<size='scriptsize', engine='bash', tidy=FALSE>>=
tokenize --help
@

To make the functionality even more accessible, we have prepared webapps with 
the \texttt{shiny} framework for the R functions. These webapps are available 
online at \url{TODO}. The webapps are also included inside the \texttt{qlcData} 
package and can be started with the following helper function:

<<eval=FALSE>>=
launch_shiny('tokenize')
@

% TO DO

\subsection*{Profiles and error reporting}
\label{error-reporting}

The first example of how to use these functions concerns finding errors in the
encoding of texts. In the following example, it looks as if we have two
identical strings, \texttt{AABB}. However, this is just an surface-impression
delivered by the current font, which renders Latin and Cyrillic capitals
identically. We can identify this problem when we produce an orthography profile
from the strings. Using here the R implementation of orthography profiles, we
first assign the two strings to a variable \texttt{test}, and then produce an
orthography profile with the function \texttt{write.profile}. As it turns out,
some of the letters are Cyrillic.

<<>>=
test <- c('AABB', '\u0041\u0410\u0042\u0412')
write.profile(test)
@

The function of error-message reporting can also nicely be illustrated
with this example. Suppose we made an orthography profile with just the two
Latin letters <A> and <B> as possible graphemes, then this profile would not be
sufficient to tokenize the strings. There are graphemes in the data that are not
in the profile, so the tokenization produces an error, which can be used to fix
the encoding (or the profile). In the example below, we can see that the
Cyrillic encoding is found in the second string of the \texttt{test} input.

<<>>=
test <- c('AABB', '\u0041\u0410\u0042\u0412')
tokenize( test, profile = c('A', 'B') )
@

\subsection*{Different ways to write a profile}
\label{write-profile}

The function \texttt{write.profile} can be used to prepare a skeleton for an
orthography profile from some data. The preparation of an orthography profile
from some data might sound like a trivial problem, but actually there are
various different ways in which strings can be separated into graphemes by
\texttt{write.profile}. Consider the following string of characters called
\texttt{example} below. The default settings of \texttt{write.profile} separates
the string into Unicode graphemes according to grapheme clusters
(`user-perceived characters', see Section~\ref{the-unicode-approach}). As it 
turns out, some of these graphemes are single codepoints, others are combinations
of two codepoints (see Section~\ref{pitfall-characters-are-not-glyphs}).

<<>>=
example <- '\u00d9\u00da\u00db\u0055\u0300\u0055\u0301\u0055\u0302'
profile_1 <- write.profile(example)
@

<<results = 'asis', echo = FALSE>>=
colnames(profile_1) <- c('Gr.', 'Freq.', 'Codepoint', 'Unicode Name')
print(xtable(profile_1
        , caption = 'Profile 1 (default settings, splitting grapheme clusters)'
        , label = 'tab:profile1'
        )
  , include.rownames = FALSE
  , booktabs = TRUE
  , size = 'scriptsize'
  , table.placement = 'H'
  )
@

By specifying the splitting separator as the empty string
\texttt{sep~=~""}, it is possible to split the string into Unicode codepoints,
thus separating the combining diacritics. The idea behind this option
\texttt{sep} is that separating by a character allows for user-determined
separation. The most extreme choice here is the empty string \texttt{sep~=~""},
which is interpreted as separation everywhere. The other extreme is the default
setting \texttt{sep~=~NULL}, which means that the separation is not
user-defined, but relegated to the Unicode grapheme definitions.

<<>>=
profile_2 <- write.profile(example, sep = "")
@

<<results = 'asis', echo = FALSE>>=
colnames(profile_2) <- c('Grapheme', 'Frequency', 'Codepoint', 'Unicode Name')
print(xtable(profile_2
        , caption = 'Profile 2 (splitting by codepoints)'
        , label = 'tab:profile2'
        )
  , include.rownames = FALSE
  , booktabs = TRUE
  , size = 'scriptsize'
  , table.placement = 'H'
  )
@

Some characters look identical, although they are encoded differently.
Unicode offers different ways of normalization (see
Section~\ref{pitfall-canonical-equivalence}), which can be invoked here as well
using the option \texttt{normalize}. NFC-normalization turns everything into the
precomposed characters, while NFD-normalization separates everything into base
characters with combining diacritics. Splitting by codepoints (i.e. \texttt{sep~=~""}) shows the results of these two normalizations.

<<>>=
# after NFC normalization unicode codepoints have changed
profile_3 <- write.profile(example, normalize = "NFC", sep = "")
# NFD normalization gives yet another structure of the codepoints
profile_4 <- write.profile(example, normalize = "NFD", sep = "")
@

<<results = 'asis', echo = FALSE>>=
colnames(profile_3) <- c('Grapheme', 'Frequency', 'Codepoint', 'Unicode Name')
print(xtable(profile_3
        , caption = 'Profile 3 (splitting by NFC codepoints)'
        , label = 'tab:profile3'
        )
  , include.rownames = FALSE
  , booktabs = TRUE
  , size = 'scriptsize'
  , table.placement = 'H'
  )
@

<<results = 'asis', echo = FALSE>>=
colnames(profile_4) <- c('Grapheme', 'Frequency', 'Codepoint', 'Unicode Name')
print(xtable(profile_4
        , caption = 'Profile 4 (splitting by NFD codepoints)'
        , label = 'tab:profile4'
        )
  , include.rownames = FALSE
  , booktabs = TRUE
  , size = 'scriptsize'
  , table.placement = 'H'
  )
@

It is important to realize that for Unicode grapheme definitions, NFC
and NFD normalization are equivalent. This can be shown by normalizing the
example in either NFC or NFD, but using default separation in
\texttt{write.profile}. To be precise, default separation means setting
\texttt{sep~=~NULL}, but that has not be added explicitly.

<<tidy = FALSE>>=
# note that NFC and NFD normalization are identical
# for unicode grapheme definitions
profile_5 <- write.profile(example, normalize = "NFD")
profile_6 <- write.profile(example, normalize = "NFC")
@

<<results = 'asis', echo = FALSE>>=
colnames(profile_5) <- c('Gr.', 'Freq.', 'Codepoint', 'Unicode Name')
print(xtable(profile_5
        , caption = 'Profile 5 (splitting by graphemes after NFD)'
        , label = 'tab:profile5'
        )
  , include.rownames = FALSE
  , booktabs = TRUE
  , size = 'scriptsize'
  , table.placement = 'H'
  )
@

<<results = 'asis', echo = FALSE>>=
colnames(profile_6) <- c('Gr.', 'Freq.', 'Codepoint', 'Unicode Name')
print(xtable(profile_6
        , caption = 'Profile 6 (splitting by graphemes after NFC)'
        , label = 'tab:profile6'
        )
  , include.rownames = FALSE
  , booktabs = TRUE
  , size = 'scriptsize'
  , table.placement = 'H'
  )
@

Note that these different profiles can also be produced using the bash
executable \texttt{writeprofile} (see \ref{r-implemenation} for notes
on how to install the bash executable). Exactly this example is also included in
the help file of the executable.

<<size='scriptsize', engine='bash'>>=
writeprofile --help
@

\subsection*{Using an orthography profile skeleton}
\label{profile-skeleton}

A common workflow to use these functions is to first make a skeleton for an
orthography profile and then edit this profile by hand. For example, Table
\ref{tab:profile_skeleton1} shows the profile skeleton after a few graphemes have
been added to the file. Note that in this example, the profile is written to the
desktop, and this file has to be edited manually. We simply add a few
multigraphs to the column \texttt{Grapheme} and leave the other columns empty.
these new graphemes are then included in the graphemic parsing.

<<eval=FALSE>>=
# a few words to be graphemically parsed
example <- c("mishmash", "mishap", "mischief", "scheme")
# write a profile skeleton to a file
write.profile(example, file = "~/Desktop/profile_skeleton.txt")
# edit the profile, and then use the edited profile to tokenize
tokenize(example, profile = "~/Desktop/profile_skeleton.txt")$strings
@

<<echo=FALSE>>=
example <- c("shampoo", "mishap", "mischief", "scheme")
profile_skeleton <- write.profile(example)
profile_skeleton <- rbind( c('sh','','','')
                         , c('ch','','','')
                         , c('sch','','','')
                         , c('ie','','','')
                         , c('oo','','','')
                         , profile_skeleton
                         )
tokenize(example, profile = profile_skeleton)$strings
@

<<results='asis', echo=FALSE>>=
print(xtable(profile_skeleton
        , caption = 'Manually edited profile skeleton'
        , label = 'tab:profile_skeleton1'
        )
  , include.rownames = FALSE
  , booktabs = TRUE
  , size = 'scriptsize'
  , table.placement = 'htb'
  )
@

It is also possible to leave out the Unicode information in
the profile skeleton by using the option \texttt{info = FALSE}. It is also
possible not to use a separate file at all, but process everything within R. In
simple situations this is often useful (see below), but in general we prefer to
handle everything through a separately saved orthography profile. This profile
often contains highly useful information that is nicely coded and saved inside
this one file, and can thus be easily distributed and shared. Doing the same as
above completely within R might look as follows:

<<>>=
# make a profile, just select the column 'Grapheme'
profile <- write.profile(example)[,"Grapheme"]
# extend the profile with multigraphs
profile <- c("sh", "ch", "sch", "ie", "oo", profile)
# use the profile to tokenize
tokenize(example, profile)$strings
@

\subsection*{Rule ordering}
\label{rule-ordering}

Not yet everything is correct in this graphemic parsing of the example discussed
previously. The sequence <sh> in `mishap' should not be a digraph, and
conversely the sequence <sch> in `mischief' should of course be separated into
<s> and <ch>. One of the important issues to get the graphemic parsing right is
the order in which graphemes are parsed. For example, currently the grapheme
<sch> is parsed before the grapheme <ch>, leading to <m\ i\ sch\ ie\ f> instead
of the intended <m\ i\ s\ ch\ ie\ f>. The reason that <sch> is parsed before
<ch> is that by default longer graphemes are parsed before shorter ones. Our
experience is that in most cases this is expected behaviour. You can change the
ordering by specifying the option \texttt{ordering}. Setting this option to
\texttt{NULL} results in no preferential ordering, i.e. the graphemes are parsed
in the order of the profile, from top to bottom. Now `mischief' is parsed
correctly, but `scheme' is wrong. So this ordering is not the solution in this
case.

<<eval=FALSE, tidy=FALSE>>= 
# do not reorder the profile
# just apply the graphemes from top to bottom
tokenize( example
         , profile = "~/Desktop/profile_skeleton.txt"
         , ordering = NULL
        )$strings
@

<<echo=FALSE>>=
tokenize( example, profile = profile_skeleton, ordering = NULL)$strings
@

There are various additional options for rule ordering implemented. Please check
the help description in R \texttt{help(tokenize)} for more details on the
possible rule ordering specificiations. In summary, there are four different 
ordering options, that can also be combined:

\begin{itemize}
  
   \item \textsc{size}\\
         This option orders the lines in the profile by the size of the
         grapheme, largest first. Size is measured by number of Unicode
         characters after normalization as specified in the option
         \texttt{normalize}. For example, <é> has a size of 1 with
         \texttt{normalize = "NFC"}, but a size of 2 with
         \texttt{normalize = "NFD"}.

   \item \textsc{context}\\ This option orders the lines by whether they have
           any context specified (see next section). Lines with context will
           then be used first. Note that this only works when the option
           \texttt{regex = TRUE} is also chosen (otherwise context
           specifications are not used).

   \item \textsc{reverse}\\ This option orders the lines from bottom to top.
         Reversing order can be useful because hand-written profiles tend to put
         general rules before specific rules, which mostly should be applied in
         reverse order.

  \item \textsc{frequency}\\
         This option orders the lines by the frequency with which they
         match in the specified strings before tokenization, least frequent
         coming first. This frequency of course depends crucially on the
         available strings, so it will lead to different orderings when applied
         to different data. Also note that this frequency is (necessarily)
         measured before graphemes are identified, so these ordering frequencies
         are not the same as the final frequencies shown in the output.
         Frequency of course also strongly differs on whether context is used
         for the matching through \texttt{regex = TRUE}.
  
\end{itemize}

By specifying more than one ordering, these orderings are used to break ties,
e.g. the default setting \texttt{ordering = c("size", "context", "reverse")}
will first order by size, and for those with the same size, it will order by
whether there is any context specified or not. For lines that are still tied
(i.e. the have the same size and both/neither have context) the order will be
reversed compared to the order as attested in the profile, because most
hand-written specifications of graphemes will first write the general rule,
followed by more specific regularities. To get the right tokenization, these 
rules should in most cases be applied in reverse order.

It is important to realize that different ordering of the rules does not have
so-called `feeding' and `bleeding' effects as known from finite-state rewrite
rules.\footnote{`Bleeding' is the effect that the application of a rule changes
the string, so as to prevent a next rule to apply. `Feeding' is the opposite: a
specific rule will only be applied becuase a previous rule changed the string
already. The interaction of rules with such `feeding' and `bleeding' effects is
extremely difficult to predict.} The graphemic parsing advocated here works
crucially different from rewrite rules in that there is nothing being rewritten:
each line in an orthography profile specifies a grapheme to be `captured' in the 
string. All lines in the profile are processed in a specifed order (as determined
by the option \texttt{ordering}). At the processing of a specific line, all 
matching graphemes in the data are marked as `captured', but not changed. 
Captured parts cannot be captured again, but they can still be used to match 
contexts of other lines in the profile. Only when all lines are processed the 
captured graphemes are separated (and possibly transliterated). In this way the 
result of the application of the `rules' is rather easy to predict.

To document a specific case of graphemic parsing, it is highly useful to save
all results of the tokenization to file by using the option \texttt{file.out},
for example as follows: 

<<eval=FALSE, tidy=FALSE>>= 
# save the results to various files
tokenize( example
         , profile = "~/Desktop/profile_skeleton.txt"
         , file.out = "~/Desktop/result"
        )
@

This will lead to the following four files being written. Crucially, a
new profile is produced with the re-ordered orthography profile. To reproduce
the tokenization, this re-ordered profile can be used with the option
\texttt{ordering~=~NULL}.

\begin{itemize}
  
   \item \textsc{result\_strings.tsv}:\\ A tab-separated file with the original
         and the tokenized/transliterated strings

   \item \textsc{result\_profile.tsv}:\\ A tab-separated file with the
         graphemes with added frequencies of occurence in the data. The lines
         in the file is re-ordered according to the order that resulted from the
         ordering specifications (see Section~\ref{rule-ordering}).

   \item \textsc{result\_errors.tsv}:\\ A tab-separated file with all original
         strings that contain unmatched parts. Unmatched parts are indicated
         with the character as specified with the option \texttt{missing}. By
         default the character \textsc{double question mark} <⁇> at
         \uni{2047} is used. When there are no errors, then this file is 
         absent.

    \item \textsc{result\_missing.tsv}:\\ A tab-separated file with the graphemes
          that are missing from the original orthography profile, as indicated in
          the errors. When there are no errors, then this file is absent.
          
\end{itemize}

\subsection*{Contextually specified graphemes}
\label{contextual-specification}

To refine a profile, it is also possible to add graphemes with contextual
specifications. An orthography profile can have columns called \texttt{Left} and
\texttt{Right} to specify the context in which the grapheme is to be
separated.\footnote{The columns names \textttf{Left}, \textttf{Right} and
\textttf{Grapheme} are currently hard-coded, so exactly these column names
should be used for these effects to take place. The position of the columns in
the profile is unimportant. So the column \textttf{Left} can occur anywhere.}
For example, we are adding an extra line to the profile from above, resulting in
the profile shown in Table~\ref{tab:profile_skeleton2}. The extra line specifies
that <s> is a grapheme when it occurs after <mi>. Such contextually-specified
graphemes are based on regular expressions so you can also use regular
expressions in the description of the context. For such contextually specified
graphemes to be included in the graphemic parsing we have to specify the option
\texttt{regex = TRUE}. This contextually specified grapheme should actually be
handled first, so we could try \texttt{ordering = NULL}. However, we can also
explicitly specifiy that rules with contextual information should be applied
first by using \texttt{ordering = "context"}. That gives the right results for
this toy example.

<<eval=FALSE, tidy=FALSE>>=
# add a contextual grapheme, and then use the edited profile to tokenize
tokenize( example
         , profile = "~/Desktop/profile_skeleton.txt"
         , regex = TRUE
         , ordering = "context"
        )$strings
@

<<echo=FALSE>>=
Left <- rep('', times = nrow(profile_skeleton))
profile_skeleton <- as.matrix(cbind(Left, profile_skeleton))
profile_skeleton <- rbind(c('mi','s','','',''), profile_skeleton)
tokenize(example, profile = profile_skeleton, regex = TRUE, order='context')$strings
@

<<results='asis', echo=FALSE>>=
print(xtable(profile_skeleton
        , caption = 'Orthography profile with contextual specification for <s>'
        , label = 'tab:profile_skeleton2'
        )
  , include.rownames = FALSE
  , booktabs = TRUE
  , size = 'scriptsize'
  , table.placement = 'htb'
  )
@

It is important to realize that with the option \texttt{regex = TRUE} all
content in the profile is treated as regular expressions, so the characters with
special meaning in regular expressions should be either omitted or escaped (by
putting a <\ \backslash\ > \textsc{reverse solidus} at \uni{005C} before the
character). Specifically, this concerns the following characters:

\begin{itemize}
  
  \item[] <-> \textsc{hyphen-minus} at \uni{002D}
  \item[] <!> \textsc{exclamation mark} at \uni{0021}
  \item[] <?> \textsc{question mark} at \uni{003F}
  \item[] <.> \textsc{full stop} at \uni{002E}
  \item[] <(> \textsc{left parenthesis} at \uni{0028}
  \item[] <)> \textsc{right parenthesis} at \uni{0029}
  \item[] <[> \textsc{left square bracket} at \uni{005B}
  \item[] <]> \textsc{right square bracket} at \uni{005D}
  \item[] <\{> \textsc{left curly bracket} at \uni{007B}
  \item[] <\}> \textsc{right curly bracket} at \uni{007D}
  \item[] <|> \textsc{vertical line} at \uni{007C}
  \item[] <*> \textsc{asterisk} at \uni{002A}
  \item[] <\backslash> \textsc{reverse solidus} at \uni{005C}
  \item[] <ˆ> \textsc{circumflex accent} at \uni{005E}
  \item[] <+> \textsc{plus sign} at \uni{002B}
  \item[] <\$> \textsc{dollar sign} at \uni{0024}
  
\end{itemize}

\subsection*{Profile skeleton with columns for editing}
\label{profile-editing}

When it is expected that context might be important for a profile, then the
profile skeleton can be created with columns prepared for the contextual
specifications. This is done by using the option \texttt{editing = TRUE} (cf.
Table~\ref{tab:profile_editing_1} for a toy profile of some Italian words).

<<eval=FALSE, tidy=FALSE>>=
example <- c('cane', 'cena', 'cine')
write.profile(example
              , file = "~/Desktop/profile_skeleton.txt"
              , editing = TRUE
              , info = FALSE
              )
@

<<results='asis', echo=FALSE>>=
example <- c('cane', 'cena', 'cine')
profile_editing <- write.profile(example, editing=T, info=F)
print(xtable(profile_editing
        , caption = 'Orthography profile with empty columns for editing contexts'
        , label = 'tab:profile_editing_1'
        )
  , include.rownames = FALSE
  , booktabs = TRUE
  , size = 'scriptsize'
  , table.placement = 'htb'
  )
@

Besides the columns \texttt{Left}, \texttt{Grapheme}, and \texttt{Right} as
discussed in the previous sections, there are also columns \texttt{Class} and
\texttt{Replacement}. The column \texttt{Class} can be used to specify classes
of graphemes that can then be used in the contextual specification. The column
\texttt{Replacement} is just a copy of the column \textt{Grapheme}, providing a
skeleton to specify transliteration. The name of the column
\texttt{Replacement} is not fixed -- there can actually be multiple columns with 
different kinds of transliterations in a single profile.

To achieve contextually determined replacements it is possible to use a regular
expression in the contextual column. For example, consider the edited toy
profile for Italian in Table~\ref{tab:profile_editing_2} (where <c> becomes /k/
except before <i,e>, then it becomes /tʃ/). 

<<results='asis', echo=FALSE>>=
colnames(profile_editing)[5] <- 'IPA'
profile_editing[2,5] <- 'k'
profile_editing <- rbind(c('', 'c', '[ie]', '', 'tʃ'), profile_editing)
profile_editing <- profile_editing[c(1,2,6,3,4,5),]
print(xtable(profile_editing
        , caption = 'Orthography profile with regex as context'
        , label = 'tab:profile_editing_2'
        )
  , include.rownames = FALSE
  , booktabs = TRUE
  , size = 'scriptsize'
  , table.placement = 'htb'
  )
@

To use this profile, you have to add the option \texttt{regex = TRUE}. Also note
that we have changed the name of the transliteration-column, so we have to tell
the tokenization process to use this column to transliterate. This is done by
adding the option \texttt{transliterate = "IPA"}.

<<eval=FALSE, tidy=FALSE>>=
# add a contextual grapheme, and then use the edited profile to tokenize
tokenize( example
         , profile = "~/Desktop/profile_skeleton.txt"
         , regex = TRUE
         , transliterate = "IPA"
        )$strings
@

<<echo=FALSE>>=
tokenize(example
  , profile = profile_editing
  , regex = TRUE
  , transliterate = 'IPA'
  )$strings
@

Another equivalent possibility is to use a column \texttt{Class} to specify a
class of graphemes, and then use this class in the specification of context.
This is useful to keep track of recurrent classes in larger profiles. You are
free to use any class-name you like, as long as it doesn't clash with the rest
of the profile. The example shown in Table~\ref{tab:profile_editing_3} should 
give the same result as obtained previously by using a regular expression.

<<results='asis', echo=FALSE>>=
profile_editing[1,3] <- 'Vfront'
profile_editing[5,4] <- 'Vfront'
profile_editing[6,4] <- 'Vfront'
print(xtable(profile_editing
        , caption = 'Orthography profile with Class as context'
        , label = 'tab:profile_editing_3'
        )
  , include.rownames = FALSE
  , booktabs = TRUE
  , size = 'scriptsize'
  , table.placement = 'htb'
  )
@

<<eval=FALSE, tidy=FALSE>>=
# add a class, and then use the edited profile to tokenize
tokenize( example
         , profile = "~/Desktop/profile_skeleton.txt"
         , regex = TRUE
         , transliterate = "IPA"
        )$strings
@

<<echo=FALSE>>=
tokenize(example
  , profile = profile_editing
  , regex = TRUE
  , transliterate = 'IPA'
  )$strings
@

\subsection*{Formatting grapheme separation}
\label{formattingseparation}

In all examples above we have used the default formatting for grapheme
separation using space as a separator, which is obtained by the default setting
\texttt{sep~=~"~"}. It is possible to specify any other separator here,
including the empty string, i.e. \texttt{sep = ""}. This will not show the
graphemic tokenization anymore (although it has of course been used in the
background).

<<eval=FALSE, tidy=FALSE>>=
# Use the empty string as separator
tokenize( example
         , profile = "~/Desktop/profile_skeleton.txt"
         , regex = TRUE
         , transliterate = "IPA"
         , sep = ""
        )$strings
@

<<echo=FALSE>>=
tokenize(example
  , profile = profile_editing
  , regex = TRUE
  , transliterate = 'IPA'
  , sep = ''
  )$strings
@

Normally, the separator specified should not occur in the data. If it does,
unexpected things might happen, so consider removing the chosen seperator from
your strings first. However, there is also an option \texttt{sep.replace} to
replace the separator with something else. When \code{sep.replace} is specified,
this mark is inserted in the string at those places where the separator occurs.
Typical usage in linguistics would be \texttt{sep = " ", sep.replace = "\#"} adding
spaces between graphemes and replacing spaces in the input string by hashes in
the output string.

<<tidy=FALSE>>=
# Replace separator in string to be tokenized
tokenize( "test test test"
         , sep = " "
         , sep.replace = "#"
        )$strings$tokenized
@

\subsection*{Remaining issues}

Given a set of graphemes, there are at least two different methods to tokenize
strings. The first is called \texttt{method = "global"}. This approach
takes the first grapheme in the profile, then matches this grapheme globally at
all places in the string, and then turns to the next string in the profile. To
us, this seems to be the most intuitive way for linguists to deal with graphemic
parsing. The other approach is called \texttt{method = "linear"}. This
approach walks through the string from left to right. At the first character it
looks through all graphemes whether there is any match, and then walks further
to the end of the match and starts again. This approach is more alike to
finite-state rewrite rules (though note that it still works differently from
such rewrite rules, see Section~\ref{rule-ordering}). The global method is used 
by default.

In some special cases these two tokenization methods can lead to different
results, but these special situations are very unlikely to happen in natural
language. The example below shows that a string \texttt{'abc'} can be parsed
differently in case of a very special profile with a very special ordering of
the graphemes.

<<tidy=FALSE>>=
# different parsing methods can lead to different results
# the global method first catches 'bc'
tokenize( "abc"
         , profile = c("bc","ab","a","c")
         , order = NULL
         , method = "global"
         )$strings
         
# the linear method catches the first grapheme, which is 'ab'
tokenize( "abc"
         , profile = c("bc","ab","a","c")
         , order = NULL
         , method = "linear"
         )$strings
@

Further, the current R-implementation has a limitation when regular expressions
are used. The problem is that overlapping matches are not captured when using
regular expressions.\footnote{This restriction is an effect of the underlyingly
used ICU implementation of the Unicode standard as implemented in R through the
package \texttt{stringi}.} Everything works as expected without regular
expressions, but there might be warnings/errors in case of \texttt{regex =
TRUE}. However, just as in the previous issue, this problem should only very
rarely (when at all) happen in natural language data.

The problem can be exemplified by a sequence <bbbb> in which a 
grapheme <bb> should be matched. With the default \texttt{regex = FALSE} 
there are three possible matches, but with \texttt{regex = TRUE} only the first 
two or the last two are matched. The middle two are not matched because they 
overlap with the other matches. In the example below this leads to an error, 
because the second <bb> is not matched. We have not been able to produce a real 
example in any natural language in which this limitation might lead to an error.

<<tidy=FALSE>>=
# Everything perfekt without regular expressions
tokenize( "abbb"
        , profile = c("ab","bb")
        , order = NULL
        , regex = FALSE
        )$strings
        
# Matching with regular expressions does not catch overlap
tokenize( "abbb"
        , profile = c("ab","bb")
        , order = NULL
        , regex = TRUE
        )$strings
@



