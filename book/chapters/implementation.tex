\chapter{Implementation}
\label{implementation}

\section{Available implementations}

To illustrate the practical application of orthography profiles as defined in
Chapter \ref{orthography-profiles}, we have implemented two different versions
of the specifications presented there: one in Python and one in R. These two
implementations have rather different implementation histories, and we are not
100\% sure that they will in all situations give the same results. Also the
perfomance with larger datasets differs, and the code is not always as clean as
we would like it to be. In sum, the two implementations should be considered as
`proof of concept' and not as the final word on the practical application of the
specifications above. In our own experience, the current implementations are
sufficiently fast and stable to be useful for academic practice (e.g. checking
data consistency, or analyzing and transliterating small to medium sized data
sets), but they should probably better not be used for full-scale industry
applications.

This chapter will introduce the implementations, and give practical step-by-step 
guidelines for installing them and using them. Various simple, and sometimes 
somewhat abstract, examples will be discussed to show the different options 
available, and to illustrate the intended usage of orthography profiles. In the 
next chapter we will then discuss a few full-scale examples to illustrate the 
application and the usefulness of orthography profiles.

\subsection*{Installing the R implementation}

The R implementation is available in the package \texttt{qlcData}, which is 
directly available from the central R repository CRAN (Comprehensive R Archive 
Network). The R software environment itself has to be downloaded from its 
website.\footnote{\url{https://www.r-project.org}} After starting the included 
R program, the \texttt{qlcData} package for dealing with orthography profiles can be 
simply installed as follows:

<<eval=FALSE>>=
# download and install the qlcData software
install.packages('qlcData') 
# load the software, so it can be used
library(qlcData) 
@

\noindent The version available through CRAN is the most recent stable version.
To obtain the most recent bug-fixes and experimental additions, please use the
development versions, which is available on
GitHub.\footnote{\url{http://github.com/cysouw/qlcData}} This development
version can be easily installed using the github-install helper software from the
\texttt{devtools} package.

<<eval=FALSE>>=
# download and install helper software
install.packages('devtools') 
# install the qlcData package from GitHub
devtools::install_github('cysouw/qlcData')
# load the software, so it can be used 
library(qlcData) 
@

\noindent Inside the \texttt{qlcData} package, there are two functions for
orthography processing, \texttt{write.profile} and \texttt{tokenize}. R includes
help files with illustrative examples, and also a so-called `vignette' with
explanations and examples.

<<eval=FALSE>>=
# view help files
help(write.profile)
help(tokenize)
# view vignette with explanation and examples
vignette('orthography_processing')
@

\noindent Basically, the idea is to use \texttt{write.profile} to produce a
basic orthography profile from some data and then \texttt{tokenize} to apply the
(possibly edited) profile on some data, as exemplified in the next section. This
can of course be performed though R, but additionally there are two more
interfaces to the R code supplied in the \texttt{qlcData} package: (i) bash
executables and (ii) `shiny' webapps.

The bash executables are little files providing an interface to the R code that
can be used in a shell on a UNIX-alike machine. The location of these
executables is a bit hidden away by the install procedure of R packages. The
location can be found by the following command in R. These executables can be 
used from here, or they can be linked and/or copied to any location as wanted.

<<eval=FALSE>>=
# show the path to the bash executables
file.path(find.package('qlcData'), 'exec')
@

\noindent For example, a good way to use the executables in a terminal is to
make softlinks (using \texttt{ln}) from the executables to a directory in your
PATH, e.g. to \texttt{/usr/local/bin/}. The two executables are named
\texttt{tokenize} and \texttt{writeprofile}, and the links can be made directly 
by using Rscript to get the paths to the executables within the terminal.

<<eval=FALSE, tidy=FALSE, size='scriptsize', engine='bash'>>=
# get the paths to the R executables in bash
pathT=`Rscript -e 'cat(file.path(find.package("qlcData"), "exec", "tokenize"))'`
pathW=`Rscript -e 'cat(file.path(find.package("qlcData"), "exec", "writeprofile"))'`

# make softlinks to the R executables in /usr/local/bin
ln -is $pathT $pathW /usr/local/bin
@

\noindent After this softlink it should be possible to access the \texttt{tokenize}
function from the shell. Try \texttt{tokenize --help} to test the functionality.

<<size='scriptsize', engine='bash'>>=
tokenize --help
@

To make the functionality even more accessible, we have prepared webapps with 
the \texttt{shiny} framework for the R functions. These webapps are available 
online at \url{TODO}. The webapps are also included inside the \texttt{qlcData} 
package and can be started with the following helper function:

<<eval=FALSE>>=
launch_shiny('tokenize')
@

TO DO

\subsection*{Installung the Python implementation}

TO DO

\section{Error reporting}
\label{error-reporting}

In the following example, it looks as if we have two identical strings,
\texttt{AABB}. However, this is just an surface-impression delivered by the
current font, which renders Latin and Cyrillic capitals identically. We can
identify this problem when we produce an orthography profile from the strings.
Using here the R implementation of orthography profiles, we first assign the two
strings to a variable \texttt{test}, and then produce an orthography profile
with the function \texttt{write.profile}. As it turns out, some of the letters 
are Cyrillic.

<<>>=
test <- c('AABB', '\u0041\u0410\u0042\u0412')
write.profile(test)
@

\noindent The working of error-message reporting can also nicely be illustrated
with this example. Suppose we made an orthography profile with just the two
Latin letters <A> and <B> as possible graphemes, then this profile would not be
sufficient to tokenize the strings. There are graphemes in the data that are not
in the profile, so the tokenization produces an error, which can be used to fix
the encoding. In the example below, we can see that the Cyrillic encoding is
found in the second string of the \texttt{test} input.

<<>>=
test <- c('AABB', '\u0041\u0410\u0042\u0412')
tokenize( test, profile = c('A', 'B') )
@

\section{Different ways to write a profile}
\label{write-profile}

The function \texttt{write.profile} can be used to prepare a skeleton for an
orthography profile from some data. The preparation of an orthography profile
from some data might sound like a trivial problem, but actually there are
various different ways in which strings can be separated into graphemes by
\texttt{write.profile}. Consider the following example string. The default
settings of \texttt{write.profile} separates the string into Unicode graphemes
according to grapheme clusters (`user-perceived characters', see
Section~\ref{the-unicode-approach}).

<<>>=
example <- '\u00d9\u00da\u00db\u0055\u0300\u0055\u0301\u0055\u0302'
profile_1 <- write.profile(example)
@

<<results = 'asis', echo = FALSE>>=
colnames(profile_1) <- c('Gr.', 'Freq.', 'Codepoint', 'Unicode Name')
print(xtable(profile_1
        , caption = 'Profile 1 (default settings, splitting grapheme clusters)'
        , label = 'tab:profile1'
        )
  , include.rownames = FALSE
  , booktabs = TRUE
  , size = 'scriptsize'
  , table.placement = 'H'
  )
@

\noindent Some of these graphemes are single codepoints, others are combinations
of two codepoints. By specifying the splitting separator as empty, it is
possible to split the string into Unicode codepoints.

<<>>=
profile_2 <- write.profile(example, sep = "")
@

<<results = 'asis', echo = FALSE>>=
colnames(profile_2) <- c('Grapheme', 'Frequency', 'Codepoint', 'Unicode Name')
print(xtable(profile_2
        , caption = 'Profile 2 (splitting by codepoints)'
        , label = 'tab:profile2'
        )
  , include.rownames = FALSE
  , booktabs = TRUE
  , size = 'scriptsize'
  , table.placement = 'H'
  )
@

\noindent Some characters look identical, although they are encoded differently. 
Unicode offers different ways of normalization, which can be invoked here as 
well (see Section~\ref{pitfall-canonical-equivalence}).

<<>>=
# after NFC normalization unicode codepoints have changed
profile_3 <- write.profile(example, normalize = "NFC", sep = "")
# NFD normalization gives yet another structure of the codepoints
profile_4 <- write.profile(example, normalize = "NFD", sep = "")
@

<<results = 'asis', echo = FALSE>>=
colnames(profile_3) <- c('Grapheme', 'Frequency', 'Codepoint', 'Unicode Name')
print(xtable(profile_3
        , caption = 'Profile 3 (splitting by NFC codepoints)'
        , label = 'tab:profile3'
        )
  , include.rownames = FALSE
  , booktabs = TRUE
  , size = 'scriptsize'
  , table.placement = 'H'
  )
@

<<results = 'asis', echo = FALSE>>=
colnames(profile_4) <- c('Grapheme', 'Frequency', 'Codepoint', 'Unicode Name')
print(xtable(profile_4
        , caption = 'Profile 4 (splitting by NFD codepoints)'
        , label = 'tab:profile4'
        )
  , include.rownames = FALSE
  , booktabs = TRUE
  , size = 'scriptsize'
  , table.placement = 'H'
  )
@

\noindent It is important to realize that for Unicode grapheme definitions, NFC 
and NFD normalization are equivalent. This can be shown by normalizing the 
example in either NFC or NFD, but using default separation in 
\texttt{write.profile}. To be precise, default separation means setting \texttt{sep = NULL} 
but that has not be added explicitly.

<<tidy = FALSE>>=
# note that NFC and NFD normalization are identical
# for unicode grapheme definitions
profile_5 <- write.profile(example, normalize = "NFD")
profile_6 <- write.profile(example, normalize = "NFC")
@

<<results = 'asis', echo = FALSE>>=
colnames(profile_5) <- c('Gr.', 'Freq.', 'Codepoint', 'Unicode Name')
print(xtable(profile_5
        , caption = 'Profile 5 (splitting by graphemes after NFD)'
        , label = 'tab:profile5'
        )
  , include.rownames = FALSE
  , booktabs = TRUE
  , size = 'scriptsize'
  , table.placement = 'H'
  )
@

<<results = 'asis', echo = FALSE>>=
colnames(profile_6) <- c('Gr.', 'Freq.', 'Codepoint', 'Unicode Name')
print(xtable(profile_6
        , caption = 'Profile 6 (splitting by graphemes after NFC)'
        , label = 'tab:profile6'
        )
  , include.rownames = FALSE
  , booktabs = TRUE
  , size = 'scriptsize'
  , table.placement = 'H'
  )
@

\noindent Note that these different profiles can also be produced using the bash 
executable \texttt{writeprofile} (see \ref{implementations} for notes on how to 
install the bash executable). Exactly this example is also included in the help 
file of the executable.

<<size='scriptsize', engine='bash'>>=
## show help
writeprofile --help
@

\section{Using an orthography profile skeleton}
\label{profile-skeleton}

A common workflow to use these functions is to first make a skeleton for an
orthography profile and then edit this profile by hand. For example, Table
\ref{tab:profile_skeleton1} shows the profile skeleton after a few graphemes have
been added to the file. Note that in this example, the profile is written to the
desktop, and this file has to be edited manually. We simply add a few
multigraphs to the column \texttt{Grapheme} and leave the other columns empty.
these new graphemes are then included in the graphemic parsing.

<<eval=FALSE>>=
# a few words to be graphemically parsed
example <- c("mishmash", "mishap", "mischief", "scheme")
# write a profile skeleton to a file
write.profile(example, file = "~/Desktop/profile_skeleton.txt")
# edit the profile, and then use the edited profile to tokenize
tokenize(example, profile = "~/Desktop/profile_skeleton.txt")$strings
@

<<echo=FALSE>>=
example <- c("shampoo", "mishap", "mischief", "scheme")
profile_skeleton <- write.profile(example)
profile_skeleton <- rbind( c('sh','','','')
                         , c('ch','','','')
                         , c('sch','','','')
                         , c('ie','','','')
                         , c('oo','','','')
                         , profile_skeleton
                         )
tokenize(example, profile = profile_skeleton)$strings
@

<<results='asis', echo=FALSE>>=
print(xtable(profile_skeleton
        , caption = 'Manually edited profile skeleton'
        , label = 'tab:profile_skeleton1'
        )
  , include.rownames = FALSE
  , booktabs = TRUE
  , size = 'scriptsize'
  , table.placement = 'htb'
  )
@

For quick solutions, it is also possible to leave out the Unicode information in
the profile skeleton by using the option \texttt{info = FALSE}. It is also
possible not to use a separate file at all, but process everything within R. In
simply situations this is often useful (see below), but in general we prefer to
handle everything through a separately saved orthography profile. This profile
often contains highly useful information that is nicely coded and saved inside
this one file, and can thus be easily distributed and shared. Doing the same as
above completely within R might look as follows:

<<>>=
# make a profile, just select the column 'Grapheme'
profile <- write.profile(example)[,"Grapheme"]
# extend the profile with multigraphs
profile <- c("sh", "ch", "sch", "ie", "oo", profile)
# use the profile to tokenize
tokenize(example, profile)$strings
@

To document a specific case of graphemic parsing, it is highly useful to save
all results of the tokenization to file by using the option \texttt{file.out},
for example as follows: 

<<eval=FALSE, tidy=FALSE>>= 
# save the results to various files
tokenize( example
         , profile = "~/Desktop/profile_skeleton.txt"
         , file.out = "~/Desktop/result"
        )
@

\noindent This will lead to the following four files being written:

\begin{itemize}
  
   \item \textsc{result\_strings.tsv}:\\ A tab-separated file with the original
         and the tokenized/transliterated strings

   \item \textsc{result\_profile.tsv}:\\ A tab-separated file with the
         graphemes with added frequencies of occurence in the data. The lines
         in the file is re-ordered according to the order that resulted from the
         ordering specifications.

   \item \textsc{result\_errors.tsv}:\\ A tab-separated file with all original
         strings that contain unmatched parts. Unmatched parts are indicated
         with the character as specified with the option \texttt{missing}. By
         default the character \textsc{double question mark} <⁇> at
         \uni{2047} is used. When there are no errors, then this file is 
         absent.

    \item \textsc{result\_missing.tsv}:\\ A tab-separated file with the graphemes
          that are missing from the original orthography profile, as indicated in
          the errors. When there are no errors, then this file is absent.
          
\end{itemize}

\section{Rule ordering}
\label{rule-ordering}

Not yet everything is correct in this graphemic parsing. The sequence <sh> in
`mishap' should not be a digraph, and likewise the sequence <sch> in `mischief'
should of course be separated into <s> and <ch>. One of the important issues to
get the graphemic parsing right is the order in which graphemes are parsed. For
example, currently the grapheme <sch> is parsed before the grapheme <ch>,
leading to <m\ i\ sch\ ie\ f> instead of the intended <m\ i\ s\ ch\ ie\ f>. The
reason that <sch> is parsed before <ch> is that by default longer graphemes are
parsed before shorter ones. Our experience is that in most cases this is
expected behaviour. You can change the ordering by specifying the option
\texttt{ordering}. Setting this option to \texttt{NULL} results in no
preferential ordering, i.e. the graphemes are parsed in the order of the
profile, from top to bottom. Now `mischief' is parsed correctly, but `scheme' is
wrong. So ordering is not the solution in this case.

<<eval=FALSE, tidy=FALSE>>= 
# do not reorder the profile
# just apply the graphemes from top to bottom
tokenize( example
         , profile = "~/Desktop/profile_skeleton.txt"
         , ordering = NULL
        )$strings
@

<<echo=FALSE>>=
tokenize( example, profile = profile_skeleton, ordering = NULL)$strings
@

There are various additional options for rule ordering implemented. Please check
the help description in R \texttt{help(tokenize)} for more details on the
possible rule ordering specificiations. In summary, there are four different 
ordering options, that can also be combined:

\begin{itemize}
  
   \item \textsc{size}\\
         order the lines in the profile by the size of the
         grapheme, largest first. Size is measured by number of Unicode
         characters after normalization as specified in the option
         \texttt{normalize}. For example, <é> has a size of 1 with
         \texttt{normalize = "NFC"}, but a size of 2 with
         \texttt{normalize = "NFD"}.

   \item \textsc{context}\\
         order the lines by whether they have any context
         specified (see next section). Lines with context coming first. Note
         that this only works when the option \texttt{regex = TRUE} is also chosen.

  \item \textsc{reverse}\\
        order the lines from bottom to top.

  \item \textsc{frequency}\\
         order the lines by the frequency with which they
         match in the specified strings before tokenization, least frequent
         coming first. This frequency of course depends crucially on the
         available strings, so it will lead to different orderings when applied
         to different data. Also note that this frequency is (necessarily)
         measured before graphemes are identified, so these ordering frequencies
         are not the same as the final frequencies shown in the output.
         Frequency of course also strongly differs on whether context is used
         for the matching through \texttt{regex = TRUE}.
  
\end{itemize}

By specifying more than one ordering, these orderings are used to break ties,
e.g. the default setting \texttt{ordering = c("size", "context", "reverse")}
will first order by size, and for those with the same size, it will order by
whether there is any context specified or not. For lines that are still tied
(i.e. the have the same size and both/neither have context) the order will be
reversed compared to the order as attested in the profile. Reversing order can
be useful because hand-written profiles tend to put general rules before
specific rules, which mostly should be applied in reverse order.

It is important to realize that different ordering of the rules does not have
so-called `feeding' and `bleeding' effects as known from finite-state rewrite
rules.\footnote{`Bleeding' is the effect that the application of a rule changes
the string, so as to prevent a next rule to apply. `Feeding' is the opposite: a
specific rule will only be applied becuase a previous rule changed the string
already. The interaction of rules with such `feeding' and `bleeding' effects is
extremely difficult to predict.} The graphemic parsing advocated here works
crucially different from rewrite rules in that there is nothing being rewritten:
each line in an orthography profile specifies a grapheme to be `captured' in the 
string. All lines in the profile are processed in a specifed order (as determined
by the option \texttt{ordering}). At the processing of a specific line, all 
matching graphemes in the data are marked as `captured', but not changed. 
Captured parts cannot be captured again, but they can still be used to match 
contexts of other lines in the profile. Only when all lines are processed the 
captured graphemes are separated (and possibly transliterated). In this way the 
result of the application of the `rules' is rather easy to predict.

To document the process of graphemic tokenization, it is strongly encouraged to

\section{Contextually specified graphemes}
\label{contextual-specification}

To refine a profile, it is also possible to add graphemes with contextual
specifications. An orthography profile can have columns called \texttt{Left} and
\texttt{Right} to specify the context in which the Grapheme is to be
separated.\footnote{The columns names \textttf{Left}, \textttf{Right} and
\textttf{Grapheme} are currently hard coded, so exactly these column names
should be used for these effects to take place. The position of the columns in
the profile is unimportant. So the column \textttf{Left} can occur anywhere.} We
are adding an extra line to the profile specifying that <s> is a grapheme when
it occurs after <mi>. These contextually specified graphemes are based on
regular expressions (so you can actually use regular expressions in the
description of the context). For these to be included in the graphemic parsing 
we have to specify the option \texttt{regex = TRUE}. This contextually specified 
grapheme should actually be handled first, so we could try \texttt{ordering = 
NULL}. However, we can also explicitly specifiy that rules with contextual 
information should be applied first by using \texttt{ordering = "context"}. That 
gives the right results for this toy example.

<<eval=FALSE, tidy=FALSE>>=
# add a contextual grapheme, and then use the edited profile to tokenize
tokenize( example
         , profile = "~/Desktop/profile_skeleton.txt"
         , regex = TRUE
         , ordering = "context"
        )$strings
@

<<echo=FALSE>>=
Left <- rep('', times = nrow(profile_skeleton))
profile_skeleton <- as.matrix(cbind(Left, profile_skeleton))
profile_skeleton <- rbind(c('mi','s','','',''), profile_skeleton)
tokenize(example, profile = profile_skeleton, regex = TRUE, order='context')$strings
@

<<results='asis', echo=FALSE>>=
print(xtable(profile_skeleton
        , caption = 'Orthography profile with contextual specification for <s>'
        , label = 'tab:profile_skeleton2'
        )
  , include.rownames = FALSE
  , booktabs = TRUE
  , size = 'scriptsize'
  , table.placement = 'htb'
  )
@

\section{Profile skeleton with columns for editing}
\label{profile-editing})

When it is expected that context might be important for a profile, then the 
profile skeleton can be written to a file with columns prepared for the 
contextual specifications by using the option \texttt{editing = TRUE}.

