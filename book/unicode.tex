%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%                                              %%%
%%%     Language Science Press Master File       %%%
%%%         follow the instructions below        %%%
%%%                                              %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Everything following a % is ignored
% Some lines start with %. Remove the % to include them

\documentclass[output=inprep,
%  long|short|inprep
%  ,blackandwhite
%  ,smallfont
%  ,draftmode  
		biblatex
		]{LSP/langsci}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%                                              %%%
%%%          additional packages                 %%%
%%%                                              %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% put all additional commands you need in the 
% following files. If you do not know what this might 
% mean, you can safely ignore this section
\usepackage{verbatim}
\input{localmetadata.tex}
\input{localpackages.tex}
\input{localhyphenation.tex}
\input{localcommands.tex}
\bibliography{localbibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%                                              %%%
%%%             Frontmatter                      %%%
%%%                                              %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
\maketitle
\frontmatter
% %% uncomment if you have preface and/or acknowledgements
% \include{chapters/preface}
% \include{chapters/acknowledgments}
% \include{chapters/abbreviations}
\tableofcontents
\mainmatter%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%                                              %%%
%%%             knitr settings                   %%%
%%%                                              %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Here are settings for knitr, which will be removed in the .tex file
% spacing of code-chunks is set with the "knitrout" environment in localcommands.tex



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%                                              %%%
%%%             Chapters                         %%%
%%%                                              %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\include{chapters/preface}
\include{chapters/introduction}
\include{chapters/pitfalls}
\include{chapters/ipa_background}
\include{chapters/ipa_meets_unicode}
\include{chapters/orthography_profiles}

\chapter{Implementation}
\label{implementation}

\section{Available implementations}

To illustrate the practical application of orthography profiles as defined in
Chapter \ref{orthography-profiles}, we have implemented two different versions
of the specifications presented there: one in Python and one in R. These two
implementations have rather different implementation histories, and we are not
100\% sure that they will in all situations give the same results. Also the
perfomance with larger datasets differs, and the code is not always as clean as
we would like it to be. In sum, the two implementations should be considered as
`proof of concept' and not as the final word on the practical application of the
specifications above. In our own experience, the current implementations are
sufficiently fast and stable to be useful for academic practice (e.g. checking
data consistency, or analyzing and transliterating small to medium sized data
sets), but they should probably better not be used for full-scale industry
applications.

This chapter will introduce the implementations, and give practical step-by-step 
guidelines for installing them and using them. Various simple, and sometimes 
somewhat abstract, examples will be discussed to show the different options 
available, and to illustrate the intended usage of orthography profiles. In the 
next chapter we will then discuss a few full-scale examples to illustrate the 
application and the usefulness of orthography profiles.

\subsection*{Installing the R implementation}

The R implementation is available in the package \texttt{qlcData}, which is 
directly available from the central R repository CRAN (Comprehensive R Archive 
Network). The R software environment itself has to be downloaded from its 
website.\footnote{\url{https://www.r-project.org}} After starting the included 
R program, the \texttt{qlcData} package for dealing with orthography profiles can be 
simply installed as follows:

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# download and install the qlcData software}
\hlkwd{install.packages}\hlstd{(}\hlstr{"qlcData"}\hlstd{)}
\hlcom{# load the software, so it can be used}
\hlkwd{library}\hlstd{(qlcData)}
\end{alltt}
\end{kframe}
\end{knitrout}

\noindent The version available through CRAN is the most recent stable version.
To obtain the most recent bug-fixes and experimental additions, please use the
development versions, which is available on
GitHub.\footnote{\url{http://github.com/cysouw/qlcData}} This development
version can be easily installed using the github-install helper software from the
\texttt{devtools} package.

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# download and install helper software}
\hlkwd{install.packages}\hlstd{(}\hlstr{"devtools"}\hlstd{)}
\hlcom{# install the qlcData package from GitHub}
\hlstd{devtools}\hlopt{::}\hlkwd{install_github}\hlstd{(}\hlstr{"cysouw/qlcData"}\hlstd{)}
\hlcom{# load the software, so it can be used}
\hlkwd{library}\hlstd{(qlcData)}
\end{alltt}
\end{kframe}
\end{knitrout}

\noindent Inside the \texttt{qlcData} package, there are two functions for
orthography processing, \texttt{write.profile} and \texttt{tokenize}. R includes
help files with illustrative examples, and also a so-called `vignette' with
explanations and examples.

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# view help files}
\hlkwd{help}\hlstd{(write.profile)}
\hlkwd{help}\hlstd{(tokenize)}
\hlcom{# view vignette with explanation and examples}
\hlkwd{vignette}\hlstd{(}\hlstr{"orthography_processing"}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

\noindent Basically, the idea is to use \texttt{write.profile} to produce a
basic orthography profile from some data and then \texttt{tokenize} to apply the
(possibly edited) profile on some data, as exemplified in the next section. This
can of course be performed though R, but additionally there are two more
interfaces to the R code supplied in the \texttt{qlcData} package: (i) bash
executables and (ii) `shiny' webapps.

The bash executables are little files providing an interface to the R code that
can be used in a shell on a UNIX-alike machine. The location of these
executables is a bit hidden away by the install procedure of R packages. The
location can be found by the following command in R. These executables can be 
used from here, or they can be linked and/or copied to any location as wanted.

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# show the path to the bash executables}
\hlkwd{file.path}\hlstd{(}\hlkwd{find.package}\hlstd{(}\hlstr{"qlcData"}\hlstd{),} \hlstr{"exec"}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

\noindent For example, a good way to use the executables in a terminal is to
make softlinks (using \texttt{ln}) from the executables to a directory in your
PATH, e.g. to \texttt{/usr/local/bin/}. The two executables are named
\texttt{tokenize} and \texttt{writeprofile}, and the links can be made directly 
by using Rscript to get the paths to the executables within the terminal.

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{alltt}
# get the paths to the R executables in bash
pathT=`Rscript -e 'cat(file.path(find.package("qlcData"), "exec", "tokenize"))'`
pathW=`Rscript -e 'cat(file.path(find.package("qlcData"), "exec", "writeprofile"))'`

# make softlinks to the R executables in /usr/local/bin
ln -is $pathT $pathW /usr/local/bin
\end{alltt}
\end{kframe}
\end{knitrout}

\noindent After this softlink it should be possible to access the \texttt{tokenize}
function from the shell. Try \texttt{tokenize --help} to test the functionality.

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{alltt}
tokenize --help
\end{alltt}

\begin{verbatim}
## USAGE: 
##   tokenize [-hlrd] [-s SEP -p REPL] [-t TRANS -m MISSING] [<STRINGS> <PROFILE>]
## 
## DESCRIPTION:
##   Using the function tokenize() from the R-package qlcData for tokenization and
##   transliteration of character strings based on an orthography profile. Details
##   http://www.rdocumentation.org/packages/qlcData/functions/tokenize.html
## 
##   STRINGS can be piped through. When no PROFILE is specified, default Unicode
##   tokenization is performed. Note that the R-code allows for even more options,
##   not all options are made available here for ease of use.
##   Errors are likewise (not yet) returned
## 
## OPTIONS:
##   -h, --help      Showing this help text
##   -l, --linear    Use linear method instead of default global method
##   -r, --regex     Use regex matching, including contexts in matching graphemes
##   -d, --NFD       Use NFD normalization instead of default NFC
##   -s SEP          Separator to be inserted after tokenization
##                     defaults to space [default: SPACE]
##   -p REPL         Replacement symbol in case separator occurs in the STRINGS
##                     defaults to nothing [default: NULL]
##   -t TRANS        Column in profile to use for transliteration [default: NULL]
##   -m MISSING      Character to be inserted for characters not in the PROFILE
##                     defaults to DOUBLE QUESTION MARK at U+2047 [default: ⁇]
## 
## EXAMPLES:
##   Try the difference between the following:
## 
##     tokenize áüî
##     tokenize --NFD -s '__' áüî
##   
##   Piping strings should work:
##   
##     ls | tokenize
\end{verbatim}
\end{kframe}
\end{knitrout}

To make the functionality even more accessible, we have prepared webapps with 
the \texttt{shiny} framework for the R functions. These webapps are available 
online at \url{TODO}. The webapps are also included inside the \texttt{qlcData} 
package and can be started with the following helper function:

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{launch_shiny}\hlstd{(}\hlstr{"tokenize"}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

TO DO

\subsection*{Installung the Python implementation}

TO DO

\section{Error reporting}
\label{error-reporting}

In the following example, it looks as if we have two identical strings,
\texttt{AABB}. However, this is just an surface-impression delivered by the
current font, which renders Latin and Cyrillic capitals identically. We can
identify this problem when we produce an orthography profile from the strings.
Using here the R implementation of orthography profiles, we first assign the two
strings to a variable \texttt{test}, and then produce an orthography profile
with the function \texttt{write.profile}. As it turns out, some of the letters 
are Cyrillic.

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{test} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"AABB"}\hlstd{,} \hlstr{"AАBВ"}\hlstd{)}
\hlkwd{write.profile}\hlstd{(test)}
\end{alltt}
\begin{verbatim}
##   Grapheme Frequency Codepoint                UnicodeName
## 1        A         3    U+0041     LATIN CAPITAL LETTER A
## 2        B         3    U+0042     LATIN CAPITAL LETTER B
## 3        А         1    U+0410  CYRILLIC CAPITAL LETTER A
## 4        В         1    U+0412 CYRILLIC CAPITAL LETTER VE
\end{verbatim}
\end{kframe}
\end{knitrout}

\noindent The working of error-message reporting can also nicely be illustrated
with this example. Suppose we made an orthography profile with just the two
Latin letters <A> and <B> as possible graphemes, then this profile would not be
sufficient to tokenize the strings. There are graphemes in the data that are not
in the profile, so the tokenization produces an error, which can be used to fix
the encoding. In the example below, we can see that the Cyrillic encoding is
found in the second string of the \texttt{test} input.

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{test} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"AABB"}\hlstd{,} \hlstr{"AАBВ"}\hlstd{)}
\hlkwd{tokenize}\hlstd{(test,} \hlkwc{profile} \hlstd{=} \hlkwd{c}\hlstd{(}\hlstr{"A"}\hlstd{,} \hlstr{"B"}\hlstd{))}
\end{alltt}


{\ttfamily\noindent\color{warningcolor}{\#\# Warning in tokenize(test, profile = c("{}A"{}, "{}B"{})): \\\#\# There were unknown characters found in the input data.\\\#\# Check output\$errors for a table with all problematic strings.}}\begin{verbatim}
## $strings
##   originals tokenized
## 1      AABB   A A B B
## 2      AАBВ   A ⁇ B ⁇
## 
## $profile
##   matched_rules Grapheme
## 1             3        B
## 2             3        A
## 
## $errors
##   originals  errors
## 2      AАBВ A ⁇ B ⁇
## 
## $missing
##   Grapheme Frequency Codepoint                UnicodeName
## 1        А         1    U+0410  CYRILLIC CAPITAL LETTER A
## 2        В         1    U+0412 CYRILLIC CAPITAL LETTER VE
\end{verbatim}
\end{kframe}
\end{knitrout}

\section{Different ways to write a profile}
\label{write-profile}

The function \texttt{write.profile} can be used to prepare a skeleton for an
orthography profile from some data. The preparation of an orthography profile
from some data might sound like a trivial problem, but actually there are
various different ways in which strings can be separated into graphemes by
\texttt{write.profile}. Consider the following example string. The default
settings of \texttt{write.profile} separates the string into Unicode graphemes
according to grapheme clusters (`user-perceived characters', see
Section~\ref{the-unicode-approach}).

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{example} \hlkwb{<-} \hlstr{"ÙÚÛÙÚÛ"}
\hlstd{profile_1} \hlkwb{<-} \hlkwd{write.profile}\hlstd{(example)}
\end{alltt}
\end{kframe}
\end{knitrout}

% latex table generated in R 3.3.0 by xtable 1.8-2 package
% Thu Jun 30 18:15:48 2016
\begin{table}[H]
\centering
\begingroup\scriptsize
\begin{tabular}{llll}
  \toprule
Gr. & Freq. & Codepoint & Unicode Name \\ 
  \midrule
Ú & 1 & U+00DA & LATIN CAPITAL LETTER U WITH ACUTE \\ 
  Ú & 1 & U+0055, U+0301 & LATIN CAPITAL LETTER U, COMBINING ACUTE ACCENT \\ 
  Ù & 1 & U+00D9 & LATIN CAPITAL LETTER U WITH GRAVE \\ 
  Ù & 1 & U+0055, U+0300 & LATIN CAPITAL LETTER U, COMBINING GRAVE ACCENT \\ 
  Û & 1 & U+00DB & LATIN CAPITAL LETTER U WITH CIRCUMFLEX \\ 
  Û & 1 & U+0055, U+0302 & LATIN CAPITAL LETTER U, COMBINING CIRCUMFLEX ACCENT \\ 
   \bottomrule
\end{tabular}
\endgroup
\caption{Profile 1 (default settings, splitting grapheme clusters)} 
\label{tab:profile1}
\end{table}


\noindent Some of these graphemes are single codepoints, others are combinations
of two codepoints. By specifying the splitting separator as empty, it is
possible to split the string into Unicode codepoints.

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{profile_2} \hlkwb{<-} \hlkwd{write.profile}\hlstd{(example,} \hlkwc{sep} \hlstd{=} \hlstr{""}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

% latex table generated in R 3.3.0 by xtable 1.8-2 package
% Thu Jun 30 18:15:48 2016
\begin{table}[H]
\centering
\begingroup\scriptsize
\begin{tabular}{llll}
  \toprule
Grapheme & Frequency & Codepoint & Unicode Name \\ 
  \midrule
́ & 1 & U+0301 & COMBINING ACUTE ACCENT \\ 
  ̀ & 1 & U+0300 & COMBINING GRAVE ACCENT \\ 
  ̂ & 1 & U+0302 & COMBINING CIRCUMFLEX ACCENT \\ 
  U & 3 & U+0055 & LATIN CAPITAL LETTER U \\ 
  Ú & 1 & U+00DA & LATIN CAPITAL LETTER U WITH ACUTE \\ 
  Ù & 1 & U+00D9 & LATIN CAPITAL LETTER U WITH GRAVE \\ 
  Û & 1 & U+00DB & LATIN CAPITAL LETTER U WITH CIRCUMFLEX \\ 
   \bottomrule
\end{tabular}
\endgroup
\caption{Profile 2 (splitting by codepoints)} 
\label{tab:profile2}
\end{table}


\noindent Some characters look identical, although they are encoded differently. 
Unicode offers different ways of normalization, which can be invoked here as 
well (see Section~\ref{pitfall-canonical-equivalence}).

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# after NFC normalization unicode codepoints have changed}
\hlstd{profile_3} \hlkwb{<-} \hlkwd{write.profile}\hlstd{(example,} \hlkwc{normalize} \hlstd{=} \hlstr{"NFC"}\hlstd{,} \hlkwc{sep} \hlstd{=} \hlstr{""}\hlstd{)}
\hlcom{# NFD normalization gives yet another structure of the codepoints}
\hlstd{profile_4} \hlkwb{<-} \hlkwd{write.profile}\hlstd{(example,} \hlkwc{normalize} \hlstd{=} \hlstr{"NFD"}\hlstd{,} \hlkwc{sep} \hlstd{=} \hlstr{""}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

% latex table generated in R 3.3.0 by xtable 1.8-2 package
% Thu Jun 30 18:15:48 2016
\begin{table}[H]
\centering
\begingroup\scriptsize
\begin{tabular}{llll}
  \toprule
Grapheme & Frequency & Codepoint & Unicode Name \\ 
  \midrule
Ú & 2 & U+00DA & LATIN CAPITAL LETTER U WITH ACUTE \\ 
  Ù & 2 & U+00D9 & LATIN CAPITAL LETTER U WITH GRAVE \\ 
  Û & 2 & U+00DB & LATIN CAPITAL LETTER U WITH CIRCUMFLEX \\ 
   \bottomrule
\end{tabular}
\endgroup
\caption{Profile 3 (splitting by NFC codepoints)} 
\label{tab:profile3}
\end{table}


% latex table generated in R 3.3.0 by xtable 1.8-2 package
% Thu Jun 30 18:15:48 2016
\begin{table}[H]
\centering
\begingroup\scriptsize
\begin{tabular}{llll}
  \toprule
Grapheme & Frequency & Codepoint & Unicode Name \\ 
  \midrule
́ & 2 & U+0301 & COMBINING ACUTE ACCENT \\ 
  ̀ & 2 & U+0300 & COMBINING GRAVE ACCENT \\ 
  ̂ & 2 & U+0302 & COMBINING CIRCUMFLEX ACCENT \\ 
  U & 6 & U+0055 & LATIN CAPITAL LETTER U \\ 
   \bottomrule
\end{tabular}
\endgroup
\caption{Profile 4 (splitting by NFD codepoints)} 
\label{tab:profile4}
\end{table}


\noindent It is important to realize that for Unicode grapheme definitions, NFC 
and NFD normalization are equivalent. This can be shown by normalizing the 
example in either NFC or NFD, but using default separation in 
\texttt{write.profile}. To be precise, default separation means setting \texttt{sep = NULL} 
but that has not be added explicitly.

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# note that NFC and NFD normalization are identical}
\hlcom{# for unicode grapheme definitions}
\hlstd{profile_5} \hlkwb{<-} \hlkwd{write.profile}\hlstd{(example,} \hlkwc{normalize} \hlstd{=} \hlstr{"NFD"}\hlstd{)}
\hlstd{profile_6} \hlkwb{<-} \hlkwd{write.profile}\hlstd{(example,} \hlkwc{normalize} \hlstd{=} \hlstr{"NFC"}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

% latex table generated in R 3.3.0 by xtable 1.8-2 package
% Thu Jun 30 18:15:48 2016
\begin{table}[H]
\centering
\begingroup\scriptsize
\begin{tabular}{llll}
  \toprule
Gr. & Freq. & Codepoint & Unicode Name \\ 
  \midrule
Ú & 2 & U+0055, U+0301 & LATIN CAPITAL LETTER U, COMBINING ACUTE ACCENT \\ 
  Ù & 2 & U+0055, U+0300 & LATIN CAPITAL LETTER U, COMBINING GRAVE ACCENT \\ 
  Û & 2 & U+0055, U+0302 & LATIN CAPITAL LETTER U, COMBINING CIRCUMFLEX ACCENT \\ 
   \bottomrule
\end{tabular}
\endgroup
\caption{Profile 5 (splitting by graphemes after NFD)} 
\label{tab:profile5}
\end{table}


% latex table generated in R 3.3.0 by xtable 1.8-2 package
% Thu Jun 30 18:15:49 2016
\begin{table}[H]
\centering
\begingroup\scriptsize
\begin{tabular}{llll}
  \toprule
Gr. & Freq. & Codepoint & Unicode Name \\ 
  \midrule
Ú & 2 & U+00DA & LATIN CAPITAL LETTER U WITH ACUTE \\ 
  Ù & 2 & U+00D9 & LATIN CAPITAL LETTER U WITH GRAVE \\ 
  Û & 2 & U+00DB & LATIN CAPITAL LETTER U WITH CIRCUMFLEX \\ 
   \bottomrule
\end{tabular}
\endgroup
\caption{Profile 6 (splitting by graphemes after NFC)} 
\label{tab:profile6}
\end{table}


\noindent Note that these different profiles can also be produced using the bash 
executable \texttt{writeprofile} (see \ref{implementations} for notes on how to 
install the bash executable). Exactly this example is also included in the help 
file of the executable.

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{alltt}
## show help
writeprofile --help
\end{alltt}

\begin{verbatim}
## USAGE: 
##   writeprofile [-hie] [-s SEP] [-n NORMALIZE] [<STRINGS>]
## 
## DESCRIPTION:
##   Using the function write.profile() from the R-package qlcData to prepare 
##   a skeleton for an orthography profile to be used with tokenize. Details:
##   http://www.rdocumentation.org/packages/qlcData/functions/readwrite_orthography
## 
##   STRINGS can be piped through. The results is a tab-delimited file for manual
##   editing. Note that the R-code allows for even more options, not all options 
##   are made available here for ease of use.
## 
## OPTIONS:
##   -h, --help      Showing this help text
##   -i, --info      Add columns with Unicode information to profile
##   -e, --editing   Add empty columns for easy editing of profile
##   -s SEP          Separator used to separate the strings
##                     Often useful is option UNI to split by unicode codepoints
##                     defaults to unicode character definition [default: NULL]
##   -n NORMALIZE    Normalization used to make profile. Possible options: NFD, NFC
##                     defaults to no normalization [default: NULL]
##                     
## EXAMPLES:
##   Note the differences between the following variants:
## 
##     example='ÙÚÛÙÚÛ'
##     echo -e $example
##     echo -e $example | writeprofile -i
##     echo -e $example | writeprofile -i -s UNI
##     echo -e $example | writeprofile -i -s UNI -n NFC
##     echo -e $example | writeprofile -i -s UNI -n NFD
\end{verbatim}
\end{kframe}
\end{knitrout}

\section{Using an orthography profile skeleton}
\label{profile-skeleton}

A common workflow to use these functions is to first make a skeleton for an
orthography profile and then edit this profile by hand. For example, Table
\ref{tab:profile_skeleton1} shows the profile skeleton after a few graphemes have
been added to the file. Note that in this example, the profile is written to the
desktop, and this file has to be edited manually. We simply add a few
multigraphs to the column \texttt{Grapheme} and leave the other columns empty.
these new graphemes are then included in the graphemic parsing.

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# a few words to be graphemically parsed}
\hlstd{example} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"mishmash"}\hlstd{,} \hlstr{"mishap"}\hlstd{,} \hlstr{"mischief"}\hlstd{,} \hlstr{"scheme"}\hlstd{)}
\hlcom{# write a profile skeleton to a file}
\hlkwd{write.profile}\hlstd{(example,} \hlkwc{file} \hlstd{=} \hlstr{"~/Desktop/profile_skeleton.txt"}\hlstd{)}
\hlcom{# edit the profile, and then use the edited profile to tokenize}
\hlkwd{tokenize}\hlstd{(example,} \hlkwc{profile} \hlstd{=} \hlstr{"~/Desktop/profile_skeleton.txt"}\hlstd{)}\hlopt{$}\hlstd{strings}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{verbatim}
##   originals    tokenized
## 1   shampoo  sh a m p oo
## 2    mishap   m i sh a p
## 3  mischief m i sch ie f
## 4    scheme    sch e m e
\end{verbatim}
\end{kframe}
\end{knitrout}

% latex table generated in R 3.3.0 by xtable 1.8-2 package
% Thu Jun 30 18:15:49 2016
\begin{table}[htb]
\centering
\begingroup\scriptsize
\begin{tabular}{llll}
  \toprule
Grapheme & Frequency & Codepoint & UnicodeName \\ 
  \midrule
sh &  &  &  \\ 
  ch &  &  &  \\ 
  sch &  &  &  \\ 
  ie &  &  &  \\ 
  oo &  &  &  \\ 
  a & 2 & U+0061 & LATIN SMALL LETTER A \\ 
  c & 2 & U+0063 & LATIN SMALL LETTER C \\ 
  e & 3 & U+0065 & LATIN SMALL LETTER E \\ 
  f & 1 & U+0066 & LATIN SMALL LETTER F \\ 
  h & 4 & U+0068 & LATIN SMALL LETTER H \\ 
  i & 3 & U+0069 & LATIN SMALL LETTER I \\ 
  m & 4 & U+006D & LATIN SMALL LETTER M \\ 
  o & 2 & U+006F & LATIN SMALL LETTER O \\ 
  p & 2 & U+0070 & LATIN SMALL LETTER P \\ 
  s & 4 & U+0073 & LATIN SMALL LETTER S \\ 
   \bottomrule
\end{tabular}
\endgroup
\caption{Manually edited profile skeleton} 
\label{tab:profile_skeleton1}
\end{table}


For quick solutions, it is also possible to leave out the Unicode information in
the profile skeleton by using the option \texttt{info = FALSE}. It is also
possible not to use a separate file at all, but process everything within R. In
simply situations this is often useful (see below), but in general we prefer to
handle everything through a separately saved orthography profile. This profile
often contains highly useful information that is nicely coded and saved inside
this one file, and can thus be easily distributed and shared. Doing the same as
above completely within R might look as follows:

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# make a profile, just select the column 'Grapheme'}
\hlstd{profile} \hlkwb{<-} \hlkwd{write.profile}\hlstd{(example)[,} \hlstr{"Grapheme"}\hlstd{]}
\hlcom{# extend the profile with multigraphs}
\hlstd{profile} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"sh"}\hlstd{,} \hlstr{"ch"}\hlstd{,} \hlstr{"sch"}\hlstd{,} \hlstr{"ie"}\hlstd{,} \hlstr{"oo"}\hlstd{, profile)}
\hlcom{# use the profile to tokenize}
\hlkwd{tokenize}\hlstd{(example, profile)}\hlopt{$}\hlstd{strings}
\end{alltt}
\begin{verbatim}
##   originals    tokenized
## 1   shampoo  sh a m p oo
## 2    mishap   m i sh a p
## 3  mischief m i sch ie f
## 4    scheme    sch e m e
\end{verbatim}
\end{kframe}
\end{knitrout}

To document a specific case of graphemic parsing, it is highly useful to save
all results of the tokenization to file by using the option \texttt{file.out},
for example as follows: 

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# save the results to various files}
\hlkwd{tokenize}\hlstd{( example}
         \hlstd{,} \hlkwc{profile} \hlstd{=} \hlstr{"~/Desktop/profile_skeleton.txt"}
         \hlstd{,} \hlkwc{file.out} \hlstd{=} \hlstr{"~/Desktop/result"}
        \hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

This will lead to the following four files being written:

\begin{itemize}
  
   \item \textsc{result_strings.tsv}:\\ A tab-separated file with the original
         and the tokenized/transliterated strings

   \item \textsc{result_profile.tsv}:\\ A tab-separated file with the
         graphemes with added frequencies of occurence in the data. The lines
         in the file is re-ordered according to the order that resulted from the
         ordering specifications.

   \item \textsc{result_errors.tsv}:\\ A tab-separated file with all original
         strings that contain unmatched parts. Unmatched parts are indicated
         with the character as specified with the option \texttt{missing}. By
         default the character \textsc{double question mark} < ⁇ > at
         \uni{2047} is used. When there are no errors, then this file is 
         absent.

    \item \textsc{result_missing.tsv}:\\ A tab-separated file with the graphemes
          that are missing from the original orthography profile, as indicated in
          the errors. When there are no errors, then this file is absent.
          
\end{itemize}

\section{Rule ordering}
\label{rule-ordering}

Not yet everything is correct in this graphemic parsing. The sequence <sh> in
`mishap' should not be a digraph, and likewise the sequence <sch> in `mischief'
should of course be separated into <s> and <ch>. One of the important issues to
get the graphemic parsing right is the order in which graphemes are parsed. For
example, currently the grapheme <sch> is parsed before the grapheme <ch>,
leading to <m\ i\ sch\ ie\ f> instead of the intended <m\ i\ s\ ch\ ie\ f>. The
reason that <sch> is parsed before <ch> is that by default longer graphemes are
parsed before shorter ones. Our experience is that in most cases this is
expected behaviour. You can change the ordering by specifying the option
\texttt{ordering}. Setting this option to \texttt{NULL} results in no
preferential ordering, i.e. the graphemes are parsed in the order of the
profile, from top to bottom. Now `mischief' is parsed correctly, but `scheme' is
wrong. So ordering is not the solution in this case.

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# do not reorder the profile}
\hlcom{# just apply the graphemes from top to bottom}
\hlkwd{tokenize}\hlstd{( example}
         \hlstd{,} \hlkwc{profile} \hlstd{=} \hlstr{"~/Desktop/profile_skeleton.txt"}
         \hlstd{,} \hlkwc{ordering} \hlstd{=} \hlkwa{NULL}
        \hlstd{)}\hlopt{$}\hlstd{strings}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{verbatim}
##   originals     tokenized
## 1   shampoo   sh a m p oo
## 2    mishap    m i sh a p
## 3  mischief m i s ch ie f
## 4    scheme    s ch e m e
\end{verbatim}
\end{kframe}
\end{knitrout}

There are various additional options for rule ordering implemented. Please check
the help description in R \texttt{help(tokenize)} for more details on the
possible rule ordering specificiations. In summary, there are four different 
ordering options, that can also be combined:

\begin{itemize}
  
   \item \textsc{size}:\\
         order the lines in the profile by the size of the
         grapheme, largest first. Size is measured by number of Unicode
         characters after normalization as specified in the option
         \texttt{normalize}. For example, <é> has a size of 1 with
         \texttt{normalize = "NFC"}, but a size of 2 with
         \texttt{normalize = "NFD"}.

   \item \textsc{context}:\\
         order the lines by whether they have any context
         specified (see next section). Lines with context coming first. Note
         that this only works when the option \texttt{regex = TRUE} is also chosen.

  \item \textsc{reverse}:\\
        order the lines from bottom to top.

  \item \textsc{frequency}:\\
         order the lines by the frequency with which they
         match in the specified strings before tokenization, least frequent
         coming first. This frequency of course depends crucially on the
         available strings, so it will lead to different orderings when applied
         to different data. Also note that this frequency is (necessarily)
         measured before graphemes are identified, so these ordering frequencies
         are not the same as the final frequencies shown in the output.
         Frequency of course also strongly differs on whether context is used
         for the matching through \texttt{regex = TRUE}.
  
\end{itemize}

By specifying more than one ordering, these orderings are used to break ties,
e.g. the default setting \texttt{ordering = c("size", "context", "reverse")}
will first order by size, and for those with the same size, it will order by
whether there is any context specified or not. For lines that are still tied
(i.e. the have the same size and both/neither have context) the order will be
reversed compared to the order as attested in the profile. Reversing order can
be useful because hand-written profiles tend to put general rules before
specific rules, which mostly should be applied in reverse order.

It is important to realize that different ordering of the rules does not have
so-called `feeding' and `bleeding' effects as known from finite-state rewrite
rules.\footnote{`Bleeding' is the effect that the application of a rule changes
the string, so as to prevent a next rule to apply. `Feeding' is the opposite: a
specific rule will only be applied becuase a previous rule changed the string
already. The interaction of rules with such `feeding' and `bleeding' effects is
extremely difficult to predict.} The graphemic parsing advocated here works
crucially different from rewrite rules in that there is nothing being rewritten:
each line in an orthography profile specifies a grapheme to be `captured' in the 
string. All lines in the profile are processed in a specifed order (as determined
by the option \texttt{ordering}). At the processing of a specific line, all 
matching graphemes in the data are marked as `captured', but not changed. 
Captured parts cannot be captured again, but they can still be used to match 
contexts of other lines in the profile. Only when all lines are processed the 
captured graphemes are separated (and possibly transliterated). In this way the 
result of the application of the `rules' is rather easy to predict.

To document the process of graphemic tokenization, it is strongly encouraged to

\section{Contextually specified graphemes}
\label{contextual-specification}

To refine a profile, it is also possible to add graphemes with contextual
specifications. An orthography profile can have columns called \texttt{Left} and
\texttt{Right} to specify the context in which the Grapheme is to be
separated.\footnote{The columns names \textttf{Left}, \textttf{Right} and
\textttf{Grapheme} are currently hard coded, so exactly these column names
should be used for these effects to take place. The position of the columns in
the profile is unimportant. So the column \textttf{Left} can occur anywhere.} We
are adding an extra line to the profile specifying that <s> is a grapheme when
it occurs after <mi>. These contextually specified graphemes are based on
regular expressions (so you can actually use regular expressions in the
description of the context). For these to be included in the graphemic parsing 
we have to specify the option \texttt{regex = TRUE}. This contextually specified 
grapheme should actually be handled first, so we could try \texttt{ordering = 
NULL}. However, we can also explicitly specifiy that rules with contextual 
information should be applied first by using \texttt{ordering = "context"}. That 
gives the right results for this toy example.

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# add a contextual grapheme, and then use the edited profile to tokenize}
\hlkwd{tokenize}\hlstd{( example}
         \hlstd{,} \hlkwc{profile} \hlstd{=} \hlstr{"~/Desktop/profile_skeleton.txt"}
         \hlstd{,} \hlkwc{regex} \hlstd{=} \hlnum{TRUE}
         \hlstd{,} \hlkwc{ordering} \hlstd{=} \hlstr{"context"}
        \hlstd{)}\hlopt{$}\hlstd{strings}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{verbatim}
##   originals     tokenized
## 1   shampoo   sh a m p oo
## 2    mishap   m i s h a p
## 3  mischief m i s ch ie f
## 4    scheme    s ch e m e
\end{verbatim}
\end{kframe}
\end{knitrout}

% latex table generated in R 3.3.0 by xtable 1.8-2 package
% Thu Jun 30 18:15:49 2016
\begin{table}[htb]
\centering
\begingroup\scriptsize
\begin{tabular}{lllll}
  \toprule
Left & Grapheme & Frequency & Codepoint & UnicodeName \\ 
  \midrule
mi & s &  &  &  \\ 
   & sh &  &  &  \\ 
   & ch &  &  &  \\ 
   & sch &  &  &  \\ 
   & ie &  &  &  \\ 
   & oo &  &  &  \\ 
   & a & 2 & U+0061 & LATIN SMALL LETTER A \\ 
   & c & 2 & U+0063 & LATIN SMALL LETTER C \\ 
   & e & 3 & U+0065 & LATIN SMALL LETTER E \\ 
   & f & 1 & U+0066 & LATIN SMALL LETTER F \\ 
   & h & 4 & U+0068 & LATIN SMALL LETTER H \\ 
   & i & 3 & U+0069 & LATIN SMALL LETTER I \\ 
   & m & 4 & U+006D & LATIN SMALL LETTER M \\ 
   & o & 2 & U+006F & LATIN SMALL LETTER O \\ 
   & p & 2 & U+0070 & LATIN SMALL LETTER P \\ 
   & s & 4 & U+0073 & LATIN SMALL LETTER S \\ 
   \bottomrule
\end{tabular}
\endgroup
\caption{Orthography profile with contextual specification for <s>} 
\label{tab:profile_skeleton2}
\end{table}


\section{Profile skeleton with columns for editing}
\label{profile-editing})

When it is expected that context might be important for a profile, then the 
profile skeleton can be written to a file with columns prepared for the 
contextual specifications by using the option \texttt{editing = TRUE}.


\include{chapters/use_cases}

\chapter{Testing code inclusion}


Trying to include code through knitr. 
Different languages can be easily 
included!

There is a bug for non-R engines: the fontsize settings and line spaceing are not correct. I have 
filed an issue, hopefully this will be fixed soon. 

For syntax colouring, install the package `highlight' and for nice tables 
install the package `xtable'.

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{alltt}
echo 'this is a simple bash example'
ls | wc
\end{alltt}

\begin{verbatim}
## this is a simple bash example
##       12      12     203
\end{verbatim}
\end{kframe}
\end{knitrout}

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{verbatim}
## note that bash chunks are executed as one, with all output at the end.
## You will have to make different chunks to get output separated.
## And add manual linebreaks!
\end{verbatim}
\end{kframe}
\end{knitrout}

Spacing of lines is fine. I think without background looks better

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# adding comments inside code? Probably not a good idea}
\hlstd{(example} \hlkwb{<-} \hlstr{"this is a simple R example"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] "this is a simple R example"
\end{verbatim}
\begin{alltt}

\hlstd{test} \hlkwb{<-} \hlnum{3} \hlopt{+} \hlnum{4}
\hlstd{test}
\end{alltt}
\begin{verbatim}
## [1] 7
\end{verbatim}
\end{kframe}
\end{knitrout}

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(qlcData)}
\hlstd{profile} \hlkwb{<-} \hlkwd{write.profile}\hlstd{(}\hlstr{"AΑА"}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

% latex table generated in R 3.3.0 by xtable 1.8-2 package
% Thu Jun 30 18:15:49 2016
\begin{table}[H]
\centering
\begingroup\small
\begin{tabular}{llll}
  \toprule
Grapheme & Frequency & Codepoint & UnicodeName \\ 
  \midrule
A & 1 & U+0041 & LATIN CAPITAL LETTER A \\ 
  Α & 1 & U+0391 & GREEK CAPITAL LETTER ALPHA \\ 
  А & 1 & U+0410 & CYRILLIC CAPITAL LETTER A \\ 
   \bottomrule
\end{tabular}
\endgroup
\caption{test} 
\label{bla}
\end{table}




You can refer to variables by using \textbackslash Sexpr{}. 
For example, the length of the example string is 26.
Crossreferencing also works, see Table~\ref{bla}.

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{alltt}
print('bla' + 'bla')
print(3+4)
\end{alltt}

\begin{verbatim}
## blabla
## 7
\end{verbatim}
\end{kframe}
\end{knitrout}

You have to add the option ``engine.path='python3'' to get Python 3.

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{alltt}
print('python3 needs an extra tag')
uni = 'aɽɮz'
print(uni)
\end{alltt}

\begin{verbatim}
## python3 needs an extra tag
## aɽɮz
\end{verbatim}
\end{kframe}
\end{knitrout}

Testing: does referencing to variables also work in Python? It does give an error, so no... !


% TODOS:

% Lots of characters within <> were lost in translation...
% some things to note in the translation to LaTeX:
% tildes in URLs broken -- turned into \textasciitilde{}
% <a> etc. seem to sometimes be lost
% section in some cases should be "chapter"


% other stuff to fix / proof read
% references
% sections, e.g. Section 4, Pitfall 4)
% URLs
% `', ``''
% reinsertion of a lot of the glyphs, etc.
% fix ligatures
% all glyphs need to be checked
% all <>s need to be checked and/or readded

\begin{comment}
===
Double quotation marks are generally used for distancing, in particular in the following situations:

1. when a passage from another work is cited in the text (e.g. According to Takahashi (2009: 33), “quotatives were never used in subordinate clauses in Old Japanese”); but block quotations do not have quotation marks;
2. when a technical term is mentioned that the author does not want to adopt, but wants to mention, e.g. This is sometimes called “pseudo-conservatism”, but I will not use this term here, as it could lead to confusion.

Single quotation marks are used exclusively for linguistic meanings, as in the following: Latin habere ‘have’ is not cognate with Old English hafian ‘have’.

===
so, we should normally use double quotation in most of our cases :-). In general though, I would like us to try and remove double quotations as much as possible. Mostly is thus signals uncertainty on our part :-).

\end{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%                                              %%%
%%%             Backmatter                       %%%
%%%                                              %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% There is normally no need to change the backmatter section
\input{backmatter.tex}
\end{document}

% you can create your book by running
% xelatex lsp-skeleton.tex
%
% you can also try a simple 
% make
% on the commandline
