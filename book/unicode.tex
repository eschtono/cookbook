%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%                                              %%%
%%%     Language Science Press Master File       %%%
%%%         follow the instructions below        %%%
%%%                                              %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Everything following a % is ignored
% Some lines start with %. Remove the % to include them

\documentclass[output=inprep,
%  long|short|inprep
%  ,blackandwhite
%  ,smallfont
%  ,draftmode  
		biblatex
		]{LSP/langsci}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%                                              %%%
%%%          additional packages                 %%%
%%%                                              %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% put all additional commands you need in the 
% following files. If you do not know what this might 
% mean, you can safely ignore this section
\usepackage{verbatim}
\input{localmetadata.tex}
\input{localpackages.tex}
\input{localhyphenation.tex}
\input{localcommands.tex}
\bibliography{localbibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%                                              %%%
%%%             Frontmatter                      %%%
%%%                                              %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
\maketitle
\frontmatter
% %% uncomment if you have preface and/or acknowledgements
% \include{chapters/preface}
% \include{chapters/acknowledgments}
% \include{chapters/abbreviations}
\tableofcontents
\mainmatter%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%                                              %%%
%%%             knitr settings                   %%%
%%%                                              %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Here are settings for knitr, which will be removed in the .tex file
% spacing of code-chunks is set with the "knitrout" environment in localcommands.tex



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%                                              %%%
%%%             Chapters                         %%%
%%%                                              %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\include{chapters/preface}
\include{chapters/introduction}
\include{chapters/pitfalls}
\include{chapters/ipa_background}
\include{chapters/ipa_meets_unicode}

%\include{chapters/orthography_profiles}

\chapter{Orthography profiles}
\label{orthography-profiles}

\section{Characterizing writing systems}
\label{characterizing-writing-systems}

At this point in the course of rapid ongoing developments, we are left with a
situation in which the Unicode Standard offers a highly detailed and flexible
approach to deal computationally with writing systems, but it has unfortunately
not influenced the linguistic practice very much. In many practical situations,
the Unicode Standard is far too complex for the day-to-day practice in
linguistics because it does not offer practical solutions for the down-to-earth
problems of many linguists. In this section, we propose some simple practical
guidelines and methods to improve on this situation.

Our central aims for linguistics, to be approached with a Unicode-based
solution, are: (i) to improve the consistency of the encoding of sources, (ii)
to transparently document knowledge about the writing system (including
transliteration), and (iii) to do all of that in a way that is easy and quick to
manage for many different sources with many different writing systems. The
central concept in our proposal is the \textsc{orthography profile}, a simple
tab-separated CSV text file, that characterizes and documents a writing system.
We also offer basic implementations in Python and R to assist with the
production of such files, and to apply orthography profiles for consistency
testing, grapheme tokenization and transliteration. Not only can orthography
profiles be helpful in the daily practice of linguistics, they also succinctly
document the orthographic details of a specific source, and, as such, might
fruitfully be published alongside sources (e.g.~in digital archives). Also, in
high-level linguistic analyzes in which the graphemic detail is of central
importance (e.g.~phonotactic or comparative-historical studies), orthography
profiles can transparently document the decisions that have been taken in the
interpretation of the orthography in the sources used.

Given these goals, Unicode locale descriptions (see Section~\ref{terminology})
might seem like the ideal orthography profiles. However, there are various
practical obstacles preventing the use of such locale descriptions in the daily
linguistic practice, namely: (i) the XML-structure is too verbose to easily and
quickly produce or correct manually, (ii) locale descriptions are designed for a
wide scope on information (like date formats or names of weekdays) most of which
is not applicable for documenting writing systems, and (iii) most crucially,
even if someone made the effort to produce a technically correct locale
description for a specific source at hand, then it is nigh impossible to deploy
the description. This is because a locale description has to be submitted to and
accepted by the Unicode Common Locale Data Repository. The repository is
(rightly so) not interested in descriptions that only apply to a limited set of
sources (e.g.~descriptions for only a single dictionary).

The major challenge then is developing an infrastructure to identify the
elements that are individual graphemes in a source, specifically for the
enormous variety of sources using some kind of alphabetic writing system.
Authors of source documents (e.g.~dictionaries, wordlists, corpora) use a
variety of writing systems that range from their own idiosyncratic
transcriptions to already well-established practical or longstanding
orthographies. Although the IPA is one practical choice as a sound-based
normalization for writing systems (which can act as an interlingual pivot to
attain interoperability across writing systems), graphemes in each writing
system must also be identified and standardized if interoperability across
different sources is to be achieved. In most cases, this amounts to more than
simply mapping a grapheme to an IPA segment because graphemes must first be
identified in context (e.g.~is the sequence one sound or two sounds or both?)
and strings must be tokenized, which may include taking orthographic rules into
account (e.g.~between vowels is /n/ and after a vowel but before a consonant is
a nasalized vowel /Ṽ/).

In our experience, data from each source must be
individually tokenized into graphemes so that its orthographic structure can be 
identified and its contents can be extracted. To extract data for analysis, a
source-by-source approach is required before an orthography profile can be
created. For example, almost each available lexicon on the world's languages is
idiosyncratic in its orthography and thus requires lexicon-specific approaches
to identify graphemes in the writing system and to map graphemes to phonemes, if
desired.

Our key proposal for the characterization of a writing system is to use a
grapheme tokenization as an inter-orthographic pivot. Basically, any source
document is tokenized by graphemes, and only then a mapping to IPA (or any other
orthographic transliteration) is performed. An \textsc{orthography profile} then is a
description of the units and rules that are needed to adequately model a
graphemic tokenization for a language variety as described in a particular
source document. An orthography profile summarizes the Unicode (tailored)
graphemes and orthographic rules used to write a language (the details of the
structure and assumptions of such a profile will be presented in the next
section).

% TODO: add tsoshi figure here?

As an example of graphemic tokenization, note the three different levels of
technological and linguistic elements that interact in the hypothetical lexical
form <tsʰṍ̰shi>:

\begin{enumerate}
	\def\labelenumi{\arabic{enumi}.} 
	\item code points (10 text elements): t s ʰ o \dia{0303} \dia{0330} \dia{0301} s h i 
	\item grapheme clusters (7 text elements): t s ʰ ṍ̰ s h i 
	\item tailored grapheme clusters (4 text elements): tsʰ ṍ̰ sh i 
\end{enumerate}

In (1), the string <tsʰṍ̰shi> has been tokenized into ten Unicode code points
(using NFD normalization), delimited here by space. Unicode tokenization is
required because sequences of code points can differ in their visual and logical
orders. For example, <õ̰> is ambiguous to whether it is the sequence of <o> +
<\dia{0303}> + <\dia{0330}> or <o> + <\dia{0330}> + <\dia{0303}>. Although these two variants are visually homoglyphs,
computationally they are different. Unicode normalization should be applied to
this string to reorder the code points into a canonical order, allowing the data
to be treated canonically equivalently for search and comparison. 

In (2), the
Unicode code points have been logically normalized and visually organized into
grapheme clusters, as specified by the Unicode Standard. The combining character
sequence <õ̰> is normalized and visually grouped together. Note that, the
\textsc{modifier letter small h} at \uni{02B0}, is not grouped with any 
other character. This is because it
belongs to Spacing Modifier Letters category in the Unicode Standard. These
characters are underspecified for the direction in which they modify a host
character. For example, can indicate either pre- or post-aspiration (whereas the
nasalization or creaky diacritic is defined in the Unicode Standard to apply to
a specified base character). 

Finally, to arrive at the graphemic tokenization in
(3), tailored grapheme clusters are needed (as for example specified in an
orthography profile). For example, this orthography profile would specify that
the sequence of characters, and form a single grapheme, and that and form a
grapheme. The orthography profile could also specify orthographic rules,
e.g.~when tokenization graphemes, in say English, the in the forms and should be
treated as distinct sequences depending on their contexts.

\section{Informal description}
\label{informal-description-of-orthography-profiles}

An orthography profile describes the Unicode code points, characters, graphemes
and orthographic rules in a writing system. An orthography profile is a
language-specific (and often even resource-specific) description of the units
and rules that are needed to adequately model a writing system. An important
assumption of our work is that we assume a resource is encoded in Unicode (or
has been converted to Unicode). Any data source that the Unicode Standard is
unable to capture, will also not be captured by an orthography profile.

Informally, an orthography profile specifies the graphemes (or, in Unicode
parlance, \textsc{tailored grapheme clusters}) that are expected to occur in any
data to be analyzed or checked for consistency. These graphemes are first
identified throughout the whole data (a step which we call
\textsc{tokenization}), and possibly simply returned as such, possibly including
error messages about any parts of the data that are not specified by the
orthography profile. Once the graphemes are identified, they might also be
changed into other graphemes (a step which we call \textsc{transliteration}).
When a grapheme has different possible transliterations, then these differences
should be separated by contextual specification, possibly down to listing
individual exceptional cases.

The logic of orthographic parsing ...

Note that to deal with ambiguous parsing cases, we can use the Unicode approach
using the zero width joiner. This is actually a non-joiner (the name is
confusing): the idea is to add this character into the text to identify cases in
which a sequence of characters is not supposed to be a complex grapheme (even
though the sequence is in the orthography profile)

In practice, we foresee a workflow in which orthography profiles are iteratively
refined, while at the same time inconsistencies and errors in the data to be
tokenized are corrected. In some more complex use-cases there might even be a
need for multiple different orthography profiles to be applied in sequence (see
Section~\ref{use-cases} on various exemplary use-cases). The result of any such
workflow will normally be a cleaned dataset and an explicit description of the
orthographic structure in the form of an orthography profile. Subsequently, the
orthography profiles can be easily distributed in scholarly channels alongside
the cleaned data, for example in supplementary material added to journal papers
or in electronic archives.

\section{Formal specification}
\label{formal-specification-of-orthography-profiles}

\subsection*{File Format}
The formal specifications of an orthography profile (or simply \textsc{profile}
for short) are the following:

\begin{enumerate}
	\def\labelenumi{A\arabic{enumi}.} 
	\item \textsc{A profile is a unicode UTF-8 encoded text file}, ideally
       using NFC, no-BOM, and LF (see Section~\ref{pitfall-file-formats}), that
       includes the information pertinent to the orthography. 
	\item \textsc{A profile is a tab-separated CSV file with an obligatory header
       line}. A minimal profile can have just a single column, in which case
       there will of course be no tabs, but the first line will still be the
       header. For all columns we assume the name in the header of the CSV file
       to be crucial. The actual ordering of the columns is unimportant. Because 
       tabs and newlines are field separators of the profile, these characters 
       will lead to problems when they occur in the original data.\footnote{The main
       reason to choose for tab-delimited, and not for quoted comma-separated files
       is that we want the profiles to be easily manipulated by hand. Tab-separation 
       is strongly preferred for this reason.}
	\item \textsc{Metadata are added in a separate UTF-8 text file} using a basic
       \textsc{tag: value} format. Metadata about the orthographic description
       given in the orthography profile includes, minimally, (i) author, (ii)
       date, (iii) title of the profile, (iv) a stable language identifier
       encoded in BCP 47/ISO 639-3 of the target language of the profile, and (v)
       bibliographic data for resource(s) that illustrate the orthography
       described in the profile. Further, (vi) the tokenization method and (vii)
       the unicode normalisation used should be documented here (see below).
    	\item \textsc{Separate lines with comments are not allowed}. Comments that
       belong to specific lines will have to be put in a separate column of
       the CSV file, e.g.~add a column called \textsc{comments}.            
\end{enumerate}

\noindent The content of a profile consists of lines, each describing a grapheme
of the orthography, using the following columns:

\begin{enumerate}
	\def\labelenumi{A\arabic{enumi}.} \setcounter{enumi}{4} 
	\item \textsc{A minimal profile consists of a single column} with a header
       called \texttt{Grapheme}, listing each of the different graphemes in a
       separate line. The name of this column is crucial for automatic 
       processing.
	\item \textsc{Optional columns can be used to specify the left and right
       context of the grapheme}, to be designated with the headers \texttt{Left}
       and \texttt{Right}, respectively. The same grapheme can occur multiple
       times with different contextual specifications, for example to
       distinguish different pronunciations depending on the context. 
	\item \textsc{The columns \texttt{Grapheme}, \texttt{Left} and \texttt{Right}
       can use regular expression metacharacters.} If regular expressions are
       used, then all literal usage of the special symbols, like full stops <.>
       or dollar signs <\$> (so-called \textsc{metacharacters}) have to be
       explicitly escaped by adding a backslash before them (i.e.~use
       <\textbackslash.> or <\textbackslash\$>). Note that any specification of
       context automatically expects regular expressions, so it is probably
       better to always escape all regular expression metacharacters when used
       literally in the orthography. The following symbols will need to be
       preceded by a backslash: {[} {]} ( ) \{ \} | ~+ * . - ! ? \^{} \$ and the
       backslash \textbackslash~itself. 
	\item \textsc{An optional column can be used to specify classes of graphemes},
       to be identified by the header \texttt{Class}. For example, this column
       can be used to define a class of vowels. Users can simply add ad-hoc
       identifiers in this column to indicate a group of graphemes, which can
       then be used in the description of the graphemes or the context. The
       identifiers should of course be chosen such that they do not conflate
       with any symbols used in the orthography themselves. Note that such
       classes only refer to the graphemes, not to the context. 
	\item \textsc{Columns describing transliterations for each graphemes can be
       added and named at will}. Often more than a single possible
       transliteration will be of interest. Any software application using these
       profiles should prompt the user to name any of these columns to select a
       specific transliteration. 
	\item \textsc{Any other columns can be added freely, but will mostly be ignored
       by any software application using the profiles}. As orthography profiles
       are also intended to be read and interpreted by humans, it is often
       highly useful to add extra information on the graphemes in further
       columns, like for example Unicode codepoints, Unicode names, frequency of
       occurrence, examples of occurrence, explanation of the contextual
       restrictions, or comments. 
 \end{enumerate}

\subsection*{Implementation}
For the automatic processing of the profiles, the following technical standards
will be expected:

\begin{enumerate}
	\def\labelenumi{B\arabic{enumi}.} 
	\item \textsc{Each line of a profile will be interpreted as a regular
       expression. }Software applications using profiles can also offer to
       interpret a profile in the literal sense to avoid the necessity for the
       user to escape regular expressions metacharacters in the profile.
       However, this only is possible when no contexts or classes are described,
       so this seems only useful in the most basic orthographies. 
	\item \textsc{The \textsc{class} column will be used to produce explicit
       \textsc{or} chains of regular expressions}, which will then be inserted
       in the \texttt{Grapheme}, \texttt{Left} and \texttt{Right} columns at
       the position indicated by the class-identifiers. For example, a class
       called \texttt{V} as a context specification might be replaced by a regular
       expression like:
       \texttt{(a\textbar{}e\textbar{}i\textbar{}o\textbar{}u\textbar{}ei\textbar{}au)}.
       Only the graphemes themselves are included here, not any contexts
       specified for the elements of the class. Note that in some cases the 
       ordering inside this regular expression might be crucial.
	\item \textsc{The \textsc{left} and \textsc{right} contexts will be included
       into the regular expressions by using lookbehind and lookahead}.
       Basically, the actual regular expression syntax of lookbehind and
       lookahead is simply hidden to the users by allowing them to only specify
       the contexts themselves. Internally, the contexts in the columns
       \texttt{Left} and \texttt{Right} are combined with the column
       \texttt{Grapheme} to form a complex regular expression like:\ 
       \texttt{(?\textless{}=Left)Grapheme(?=Right)}. 
	\item \textsc{The regular expressions will be applied in the order as specified
       in the profile, from top to bottom.} A software implementation can offer
       help in figuring out the optimal ordering of the regular expressions, but
       should then explicitly report on the order used.      
\end{enumerate}

\noindent The actual implementation of the profile on some text-string will function as
follows:

\begin{enumerate}
	\def\labelenumi{B\arabic{enumi}.} \setcounter{enumi}{4} 
	\item \textsc{All graphemes are matched in the text before they are tokenized
       or transliterated}. In this way, there is no necessity for the user to
       consider `feeding' and `bleeding' situations, in which the application of
       a rule either changes the text so another rule suddenly applies (feeding)
       or prevents another rule to apply (`bleeding'). 
	\item \textsc{The matching of the graphemes can occur either globally or
       linearly. }From a computer science perspective, the most natural way to
       match graphemes from a profile in some text is by walking linearly
       through the text-string from left to right, and at each position go
       through all graphemes in the profile to see which one matches, then go to
       the position at the end of the matched grapheme and start over. This is
       basically how a finite state transducer works, which is a
       well-established technique in computer science. However, from a
       linguistic point of view, our experience is that most linguists find it
       more natural to think from a global perspective. In this approach, the
       first grapheme in the profile is matched everywhere in the text-string
       first, before moving to the next grapheme in the profile. Theoretically,
       these approaches will lead to different results, though in practice of
       actual natural language orthographies they almost always lead to the same
       result. Still, we suggest that any software application using orthography
       profiles should offer both approaches (i.e. \textsc{global} or
       \textsc{linear}) to the user. The approach used should be documented in
       the metadata as \textsc{tokenization method}. 
	\item \textsc{The matching of the graphemes can occur either in NFC or NFD.} By
       default, both the profile and the text-string to be tokenized should be
       treated as NFC (see Section~\ref{pitfall-canonical-equivalence}). However, in some use-cases it turns out to
       be practical to treat both text and profile as NFD. This typically
       happens when very many different combinations of diacritics occur in the
       data. An NFD-profile can then be used to first check which individual
       diacritics are used, before turning to the more cumbersome inspection of
       all combinations. We suggest that any software application using
       orthography profiles should offer both approaches (i.e. \textsc{NFC} or
       \textsc{NFD}) to the user. The approach used should be documented in the
       metadata as \textsc{unicode normalization}. 
	\item \textsc{The text-string is always returned in tokenized form} by
       separating the matched graphemes by a user-specified symbols-string. Any
       transliteration will be returned on top of the tokenization. 
	\item \textsc{Leftover characters}, i.e.~\textsc{characters that are not
       matched by the profile, should be reported to the user as errors.}
       Typically, the unmatched character are replaced in the tokenization by a
       user-specified symbol-string.       
 \end{enumerate}

\subsection*{Software applications}

Any software application offering to use orthography profile:

\begin{enumerate}
	\def\labelenumi{\arabic{enumi}.} 
	\item \textsc{should offer user-options} to specify:
	\begin{enumerate}
		\def\labelenumii{C\arabic{enumii}.} 
		\item \textsc{the name of the column to be used for transliteration} (if any). 
		\item \textsc{the symbol-string to be inserted between graphemes.} Optionally,
        a warning might be given if the chosen string includes characters from
        the orthography itself. 
		\item \textsc{the symbol-string to be inserted for unmatched strings} in the
        tokenized and transliterated output. 
		\item \textsc{the tokenization method}, i.e.~whether the tokenization should
        proceed \textsc{global} or \textsc{linear} (see B6 above). 
		\item \textsc{unicode normalization}, i.e.~whether the text-string and profile
        should use \textsc{NFC} or \textsc{NFD}. 
    \end{enumerate}
	\item \textsc{might offer user-options}:
	\begin{enumerate}
		\def\labelenumii{C\arabic{enumii}.} \setcounter{enumii}{5} 
		\item \textsc{to assist in the ordering of the graphemes.} In our experience, it
        makes sense to apply larger graphemes before shorter graphemes, and to
        apply graphemes with context before graphemes without context. Further,
        frequently relevant rules might be applied after rarely relevant rules
        (though frequency is difficult to establish in practice, as it depends
        on the available data). Also, if this all fails to give any decisive
        ordering between rules, it seems useful to offer linguists the option to
        reverse the ordering from any manual specified ordering, because
        linguists tend to write the more general rule first, before turning to
        exceptions or special cases. 
		\item \textsc{to assist in dealing with upper and lower case characters.} It
        seems practical to offer some basic case matching, so characters like
        <a> and <A> are treated equally. This will be useful in many concrete
        cases, although the user should be warned that case matching does not
        function universally in the same way across orthographies. Ideally,
        users should prepare orthography profiles with all lowercase and
        uppercase variants explicitly mentioned, so by default no case matching
        should be performed. 
		\item \textsc{to treat the profile literal}, i.e.~to not interpret regular
        expression metacharacters. Matching graphemes literally often leads to
        strong speed increase, and would allow users to not needing to worry
        about escaping metacharacters. However, in our experience all actually
        interesting use-cases of orthography profiles include some contexts,
        which automatically prevents any literal interpretation.
    \end{enumerate}
	\item \textsc{should return the following information} to the user:
	\begin{enumerate}
		\def\labelenumii{C\arabic{enumii}.} \setcounter{enumii}{8} 
		\item \textsc{the original text-strings} to be processed in the specified
        unicode normalization, i.e.~in either NFC or NFD as specified by the
        user. 
		\item \textsc{the tokenized strings}, with additionally any transliterated
        strings, if transliteration is requested. 
		\item \textsc{a survey of all errors encountered}, ideally both in which
        text-strings any errors occurred and which characters in the
        text-strings lead to errors. 
		\item \textsc{a reordered profile}, when any automatic reordering is offered 
	\end{enumerate}
\end{enumerate}

\section{Available implementations}



\section{Examples}

In the following example, it looks as if we have two identical strings,
\texttt{AABB}. However, this is just an surface-impression delivered by the
current font, which renders Latin and Cyrillic capitals identically. We can
identify this problem when we produce an orthography profile from the strings.
Using here the R implementation of orthography profiles, we first assign the two
strings to a variable \texttt{test}, and then produce an orthography profile
with the function \texttt{write.profile}. As it turns out, some of the letters 
are Cyrillic.

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{test} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"AABB"}\hlstd{,} \hlstr{"AАBВ"}\hlstd{)}
\hlkwd{write.profile}\hlstd{(test)}
\end{alltt}
\begin{verbatim}
##   Grapheme Frequency Codepoint                UnicodeName
## 1        A         3    U+0041     LATIN CAPITAL LETTER A
## 2        B         3    U+0042     LATIN CAPITAL LETTER B
## 3        А         1    U+0410  CYRILLIC CAPITAL LETTER A
## 4        В         1    U+0412 CYRILLIC CAPITAL LETTER VE
\end{verbatim}
\end{kframe}
\end{knitrout}

\noindent The working of error-message reporting can also nicely be illustrated with this
example. Suppose we made an orthography profile with just the Latin <A> and <B> 
as possible graphemes, then this profile would not be sufficient to tokenize the 
strings. There are graphemes in the data that are not in the profile, so the 
tokenization produces an error, which can be used to fix the encoding. In the 
example below, we can see that the Cyrillic encoding is found in the second 
string of the \texttt{test} input.

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{test} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"AABB"}\hlstd{,} \hlstr{"AАBВ"}\hlstd{)}
\hlkwd{tokenize}\hlstd{(test,} \hlkwc{profile} \hlstd{=} \hlkwd{c}\hlstd{(}\hlstr{"A"}\hlstd{,} \hlstr{"B"}\hlstd{))}
\end{alltt}


{\ttfamily\noindent\color{warningcolor}{\#\# Warning in tokenize(test, profile = c("{}A"{}, "{}B"{})): \\\#\# There were unknown characters found in the input data.\\\#\# Check output\$errors for a table with all problematic strings.}}\begin{verbatim}
## $strings
##   originals tokenized
## 1      AABB   A A B B
## 2      AАBВ   A ⁇ B ⁇
## 
## $profile
##   matched_rules Grapheme
## 1             3        B
## 2             3        A
## 
## $errors
##   originals  errors
## 2      AАBВ A ⁇ B ⁇
## 
## $missing
##   Grapheme Frequency Codepoint                UnicodeName
## 1        А         1    U+0410  CYRILLIC CAPITAL LETTER A
## 2        В         1    U+0412 CYRILLIC CAPITAL LETTER VE
\end{verbatim}
\end{kframe}
\end{knitrout}



\include{chapters/use_cases}

\chapter{Testing code inclusion}


Trying to include code through knitr. 
Different languages can be easily 
included!

There is a bug for non-R engines: the fontsize settings and line spaceing are not correct. I have 
filed an issue, hopefully this will be fixed soon. 

For syntax colouring, install the package `highlight' and for nice tables 
install the package `xtable'.

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{alltt}
echo 'this is a simple bash example'
ls | wc
\end{alltt}

\begin{verbatim}
## this is a simple bash example
##       11      11     184
\end{verbatim}
\end{kframe}
\end{knitrout}

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{verbatim}
## note that bash chunks are executed as one, with all output at the end.
## You will have to make different chunks to get output separated.
## And add manual linebreaks!
\end{verbatim}
\end{kframe}
\end{knitrout}

Spacing of lines is fine. I think without background looks better

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# adding comments inside code? Probably not a good idea}
\hlstd{(example} \hlkwb{<-} \hlstr{"this is a simple R example"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] "this is a simple R example"
\end{verbatim}
\begin{alltt}
\hlstd{test} \hlkwb{<-} \hlnum{3} \hlopt{+} \hlnum{4}
\hlstd{test}
\end{alltt}
\begin{verbatim}
## [1] 7
\end{verbatim}
\end{kframe}
\end{knitrout}

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(qlcData)}
\hlstd{profile} \hlkwb{<-} \hlkwd{write.profile}\hlstd{(}\hlstr{"AΑА"}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

% latex table generated in R 3.2.2 by xtable 1.7-4 package
% Thu Dec 10 18:07:34 2015
\begin{table}[H]
\centering
{\small
\begin{tabular}{llll}
  \toprule
Grapheme & Frequency & Codepoint & UnicodeName \\ 
  \midrule
A & 1 & U+0041 & LATIN CAPITAL LETTER A \\ 
  Α & 1 & U+0391 & GREEK CAPITAL LETTER ALPHA \\ 
  А & 1 & U+0410 & CYRILLIC CAPITAL LETTER A \\ 
   \bottomrule
\end{tabular}
}
\caption{test} 
\label{bla}
\end{table}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in print(xtable(profile, caption = "{}test"{}, label = "{}bla"{}), include.rownames = FALSE, : could not find function "{}xtable"{}}}\end{kframe}



You can refer to variables by using \textbackslash Sexpr{}. 
For example, the length of the example string is 26.
Crossreferencing also works, see Table~\ref{bla}.

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{alltt}
print('bla' + 'bla')
print(3+4)
\end{alltt}

\begin{verbatim}
## blabla
## 7
\end{verbatim}
\end{kframe}
\end{knitrout}

You have to add the option ``engine.path='python3'' to get Python 3.

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{alltt}
print('python3 needs an extra tag')
uni = 'aɽɮz'
print(uni)
\end{alltt}

\begin{verbatim}
## python3 needs an extra tag
## aɽɮz
\end{verbatim}
\end{kframe}
\end{knitrout}

Testing: does referencing to variables also work in Python? It does give an error, so no... !


% TODOS:

% Lots of characters within <> were lost in translation...
% some things to note in the translation to LaTeX:
% tildes in URLs broken -- turned into \textasciitilde{}
% <a> etc. seem to sometimes be lost
% section in some cases should be "chapter"


% other stuff to fix / proof read
% references
% sections, e.g. Section 4, Pitfall 4)
% URLs
% `', ``''
% reinsertion of a lot of the glyphs, etc.
% fix ligatures
% all glyphs need to be checked
% all <>s need to be checked and/or readded

\begin{comment}
===
Double quotation marks are generally used for distancing, in particular in the following situations:

1. when a passage from another work is cited in the text (e.g. According to Takahashi (2009: 33), “quotatives were never used in subordinate clauses in Old Japanese”); but block quotations do not have quotation marks;
2. when a technical term is mentioned that the author does not want to adopt, but wants to mention, e.g. This is sometimes called “pseudo-conservatism”, but I will not use this term here, as it could lead to confusion.

Single quotation marks are used exclusively for linguistic meanings, as in the following: Latin habere ‘have’ is not cognate with Old English hafian ‘have’.

===
so, we should normally use double quotation in most of our cases :-). In general though, I would like us to try and remove double quotations as much as possible. Mostly is thus signals uncertainty on our part :-).

\end{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%                                              %%%
%%%             Backmatter                       %%%
%%%                                              %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% There is normally no need to change the backmatter section
\input{backmatter.tex}
\end{document}

% you can create your book by running
% xelatex lsp-skeleton.tex
%
% you can also try a simple 
% make
% on the commandline
